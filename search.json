[
  {
    "objectID": "posts/2023-04-12-la.html",
    "href": "posts/2023-04-12-la.html",
    "title": "Laplace Approximation",
    "section": "",
    "text": "try:\n  import jax\nexcept ModuleNotFoundError:\n  %pip install jaxlib jax\n  import jax\nimport jax.numpy as jnp\ntry:\n  import tensorflow_probability.substrates.jax as tfp\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom tqdm import trange\nimport logging\nlogger = logging.getLogger()\nclass CheckTypesFilter(logging.Filter):\n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\n\nSampling from the Bernouli distribution with \\(\\theta\\) = 0.7\n\nbernoulli_samples = tfp.distributions.Bernoulli(probs=0.7)\nsamples = bernoulli_samples.sample(sample_shape=100, seed=jax.random.PRNGKey(0))\nprint(samples)\nalpha = 3\nbeta = 5\nsamples.sum()\n\n[1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1]\n\n\nDeviceArray(69, dtype=int32)\n\n\n\n\nNegative log joint\n\ndef neg_logjoint(theta):\n    alpha = 3\n    beta = 5\n    dist_prior = tfp.distributions.Beta(alpha, beta)\n    dist_likelihood = tfp.distributions.Bernoulli(probs=theta)\n    return -(dist_prior.log_prob(theta) + dist_likelihood.log_prob(samples).sum())\n\n\n\nCalculating \\(\\theta_{map}\\) by minimising the negative log joint using gradient descent\n\ngradient = jax.value_and_grad(jax.jit(neg_logjoint))\nlr = 0.001\nepochs = 200\ntheta_map = 0.5\nlosses = []\nfor i in trange(epochs):\n    val, grad = gradient(theta_map)\n    theta_map -= lr * grad\n    losses.append(val)\nplt.plot(losses)\nsns.despine()\ntheta_map\n\n100%|██████████| 200/200 [00:02<00:00, 71.83it/s] \n\n\nDeviceArray(0.6698113, dtype=float32, weak_type=True)\n\n\n\n\n\n\n\nVerification of obtained \\(\\theta_{map}\\) value using the formula:\n\n\n\\(\\theta_{map} = \\frac{n_h+\\alpha-1}{n_h+n_t+\\alpha+\\beta-2}\\)\n\nnH = samples.sum().astype(\"float32\")\nnT = (samples.size - nH).astype(\"float32\")\ntheta_check = (nH + alpha - 1) / (nH + nT + alpha + beta - 2)\ntheta_check\n\nDeviceArray(0.6698113, dtype=float32)\n\n\n\n\nComputing Hessian and Covariance\n\nhessian = jax.hessian(neg_logjoint)(theta_map)\nhessian = jnp.reshape(hessian, (1, 1))\ncov = jnp.linalg.inv(hessian)\ncov\n\nDeviceArray([[0.00208645]], dtype=float32)\n\n\n\n\nPlots Comparing the distribution obtained using Laplace approximation with actual Beta Bernoulli posterior\n\nx = jnp.linspace(0, 1, 100)\nx = x.reshape(-1, 1)\nLaplace_Approx = tfp.distributions.MultivariateNormalFullCovariance(\n    loc=theta_map, covariance_matrix=cov\n)\nLaplace_Approx_pdf = Laplace_Approx.prob(x)\nplt.plot(x, Laplace_Approx_pdf, label=\"Laplace Approximation\")\n\nalpha = 3\nbeta = 5\ntrue_posterior = tfp.distributions.Beta(alpha + nH, beta + nT)\ntrue_posterior_pdf = true_posterior.prob(x)\nplt.plot(x, true_posterior_pdf, label=\"True Posterior\")\nplt.xlim(0, 1)\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f2181895a10>\n\n\n\n\n\n\n# Using log_prob\ntrue_posterior_pdf_log = true_posterior.log_prob(x)\nLaplace_Approx_pdf_log = Laplace_Approx.log_prob(x)\nplt.plot(x, Laplace_Approx_pdf_log, label=\"Laplace Approximation\")\nplt.plot(x, true_posterior_pdf_log, label=\"True Posterior\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f2180719250>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Laplace Approximation\n\n\n\n\n\n\n\nLaplace Approximation\n\n\nML\n\n\n\n\nImplementing Laplace Approximation in JAX\n\n\n\n\n\n\nMar 28, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\nNo matching items"
  }
]