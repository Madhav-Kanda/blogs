[
  {
    "objectID": "posts/2022-08-12-la.html",
    "href": "posts/2022-08-12-la.html",
    "title": "Laplace Approximation",
    "section": "",
    "text": "# Importing modules\ntry:\n  import jax                      # JAX is a library for differentiable programming\nexcept ModuleNotFoundError:\n  %pip install jaxlib jax\n  import jax\nimport jax.numpy as jnp           # JAX's numpy implementation\ntry:\n  import tensorflow_probability.substrates.jax as tfp     # TFP is a library for probabilistic programming\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom tqdm import trange\nimport logging\nlogger = logging.getLogger()\nclass CheckTypesFilter(logging.Filter):                   \n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\n\nSampling from the Bernouli distribution with \\(\\theta\\) = 0.7\n\nbernoulli_samples = tfp.distributions.Bernoulli(\n    probs=0.7\n)  # Create a Bernoulli distribution with p=0.7\nsamples = bernoulli_samples.sample(\n    sample_shape=100, seed=jax.random.PRNGKey(0)\n)  # Sample 100 samples from the distribution\nprint(samples)\n\nalpha = 3  # Set the parameter (alpha) of the Beta distribution\nbeta = 5  # Set the parameter (beta) of the Beta distribution\nsamples.sum()\n\n[1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1]\n\n\nDeviceArray(69, dtype=int32)\n\n\n\n\nNegative log joint\n\ndef neg_logjoint(theta):  # Define the negative log-joint distribution\n    alpha = 3\n    beta = 5\n    dist_prior = tfp.distributions.Beta(alpha, beta)\n    dist_likelihood = tfp.distributions.Bernoulli(probs=theta)\n    return -(dist_prior.log_prob(theta) + dist_likelihood.log_prob(samples).sum())\n\n\n\nCalculating \\(\\theta_{map}\\) by minimising the negative log joint using gradient descent\n\ngradient = jax.value_and_grad(\n    jax.jit(neg_logjoint)\n)  # Define the gradient of the negative log-joint distribution\nlr = 0.001  # Set the learning rate\nepochs = 200  # Set the number of epochs\ntheta_map = 0.5  # Set the initial value of theta\nlosses = []\nfor i in trange(epochs):  # Run the optimization loop\n    val, grad = gradient(theta_map)\n    theta_map -= lr * grad\n    losses.append(val)\nplt.plot(losses)\nsns.despine()\ntheta_map\n\n100%|██████████| 200/200 [00:02&lt;00:00, 71.83it/s] \n\n\nDeviceArray(0.6698113, dtype=float32, weak_type=True)\n\n\n\n\n\n\n\nVerification of obtained \\(\\theta_{map}\\) value using the formula:\n\n\n\\(\\theta_{map} = \\frac{n_h+\\alpha-1}{n_h+n_t+\\alpha+\\beta-2}\\)\n\nnH = samples.sum().astype(\"float32\")  # Compute the number of heads\nnT = (samples.size - nH).astype(\"float32\")  # Compute the number of tails\ntheta_check = (nH + alpha - 1) / (\n    nH + nT + alpha + beta - 2\n)  # Compute the posterior mean\ntheta_check\n\nDeviceArray(0.6698113, dtype=float32)\n\n\n\n\nComputing Hessian and Covariance\n\nhessian = jax.hessian(neg_logjoint)(\n    theta_map\n)  # Compute the Hessian of the negative log-joint distribution\nhessian = jnp.reshape(hessian, (1, 1))  # Reshape the Hessian to a 1x1 matrix\ncov = jnp.linalg.inv(hessian)  # Compute the covariance matrix\ncov\n\nDeviceArray([[0.00208645]], dtype=float32)\n\n\n\n\nPlots Comparing the distribution obtained using Laplace approximation with actual Beta Bernoulli posterior\n\n# Compute the Laplace approximation\nx = jnp.linspace(0, 1, 100)  # Create a grid of 100 points between 0 and 1\nx = x.reshape(-1, 1)  # Reshape the grid to a 100x1 matrix\nLaplace_Approx = tfp.distributions.MultivariateNormalFullCovariance(  # Create a multivariate normal distribution\n    loc=theta_map, covariance_matrix=cov\n)\nLaplace_Approx_pdf = Laplace_Approx.prob(\n    x\n)  # Compute the probability density function of the Laplace approximation\nplt.plot(x, Laplace_Approx_pdf, label=\"Laplace Approximation\")\n\n\n# Compute the true posterior distribution\nalpha = 3\nbeta = 5\ntrue_posterior = tfp.distributions.Beta(\n    alpha + nH, beta + nT\n)  # Create a Beta distribution\ntrue_posterior_pdf = true_posterior.prob(\n    x\n)  # Compute the probability density function of the true posterior\nplt.plot(x, true_posterior_pdf, label=\"True Posterior\")\nplt.xlim(0, 1)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2181895a10&gt;\n\n\n\n\n\n\n# Compute the log-probability density function of the Laplace approximation\ntrue_posterior_pdf_log = true_posterior.log_prob(x)\nLaplace_Approx_pdf_log = Laplace_Approx.log_prob(x)\nplt.plot(x, Laplace_Approx_pdf_log, label=\"Laplace Approximation\")\nplt.plot(x, true_posterior_pdf_log, label=\"True Posterior\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2180719250&gt;"
  },
  {
    "objectID": "posts/2023-10-18-vip.html",
    "href": "posts/2023-10-18-vip.html",
    "title": "Variationally Inferred Parameterization (VIP)",
    "section": "",
    "text": "Variationally Inferred Parameterization\n\nTutorial also hosted on Numpyro\nOccasionally, the Hamiltonian Monte Carlo (HMC) sampler encounters challenges in effectively sampling from the posterior distribution. One illustrative case is Neal’s funnel. In these situations, the conventional centered parameterization may prove inadequate, leading us to employ non-centered parameterization. However, there are instances where even non-centered parameterization may not suffice, necessitating the utilization of Variationally Inferred Parameterization to attain the desired centeredness within the range of 0 to 1.\nThe purpose of this tutorial is to implement Variationally Inferred Parameterization based on Automatic Reparameterization of Probabilistic Programs using LocScaleReparam in Numpyro.\n\n%pip -qq install numpyro\n%pip -qq install ucimlrepo\n\n\nimport jax\nimport numpyro\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nimport jax.numpy as jnp\nfrom numpyro.infer import MCMC, NUTS\nimport numpyro.distributions as dist\nfrom ucimlrepo import fetch_ucirepo\n\nrng_key = jax.random.PRNGKey(0)\nfrom numpyro.infer.reparam import LocScaleReparam\nfrom numpyro.infer import SVI, Trace_ELBO\nfrom numpyro.infer.autoguide import AutoDiagonalNormal\n\n\n\n1. Dataset\nWe will be using the German Credit Dataset for this illustration. The dataset consists of 1000 entries with 20 categorial symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes.\n\ndef load_german_credit():\n    statlog_german_credit_data = fetch_ucirepo(id=144)\n    X = statlog_german_credit_data.data.features\n    y = statlog_german_credit_data.data.targets\n    return X, y\n\n\nX, y = load_german_credit()\nX\n\n\n  \n    \n\n\n\n\n\n\nAttribute1\nAttribute2\nAttribute3\nAttribute4\nAttribute5\nAttribute6\nAttribute7\nAttribute8\nAttribute9\nAttribute10\nAttribute11\nAttribute12\nAttribute13\nAttribute14\nAttribute15\nAttribute16\nAttribute17\nAttribute18\nAttribute19\nAttribute20\n\n\n\n\n0\nA11\n6\nA34\nA43\n1169\nA65\nA75\n4\nA93\nA101\n4\nA121\n67\nA143\nA152\n2\nA173\n1\nA192\nA201\n\n\n1\nA12\n48\nA32\nA43\n5951\nA61\nA73\n2\nA92\nA101\n2\nA121\n22\nA143\nA152\n1\nA173\n1\nA191\nA201\n\n\n2\nA14\n12\nA34\nA46\n2096\nA61\nA74\n2\nA93\nA101\n3\nA121\n49\nA143\nA152\n1\nA172\n2\nA191\nA201\n\n\n3\nA11\n42\nA32\nA42\n7882\nA61\nA74\n2\nA93\nA103\n4\nA122\n45\nA143\nA153\n1\nA173\n2\nA191\nA201\n\n\n4\nA11\n24\nA33\nA40\n4870\nA61\nA73\n3\nA93\nA101\n4\nA124\n53\nA143\nA153\n2\nA173\n2\nA191\nA201\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nA14\n12\nA32\nA42\n1736\nA61\nA74\n3\nA92\nA101\n4\nA121\n31\nA143\nA152\n1\nA172\n1\nA191\nA201\n\n\n996\nA11\n30\nA32\nA41\n3857\nA61\nA73\n4\nA91\nA101\n4\nA122\n40\nA143\nA152\n1\nA174\n1\nA192\nA201\n\n\n997\nA14\n12\nA32\nA43\n804\nA61\nA75\n4\nA93\nA101\n4\nA123\n38\nA143\nA152\n1\nA173\n1\nA191\nA201\n\n\n998\nA11\n45\nA32\nA43\n1845\nA61\nA73\n4\nA93\nA101\n4\nA124\n23\nA143\nA153\n1\nA173\n1\nA192\nA201\n\n\n999\nA12\n45\nA34\nA41\n4576\nA62\nA71\n3\nA93\nA101\n4\nA123\n27\nA143\nA152\n1\nA173\n1\nA191\nA201\n\n\n\n\n\n1000 rows × 20 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nHere, X depicts 20 attributes and the values corresponding to these attributes for each person represented in the data entry and y is the output variable corresponding to these attributes\n\ndef data_transform(X, y):\n    def categorical_to_int(x):\n        d = {u: i for i, u in enumerate(np.unique(x))}\n        return np.array([d[i] for i in x])\n\n    categoricals = []\n    numericals = []\n    numericals.append(np.ones([len(y)]))\n    for column in X:\n        column = X[column]\n        if column.dtype == \"O\":\n            categoricals.append(categorical_to_int(column))\n        else:\n            numericals.append((column - column.mean()) / column.std())\n    numericals = np.array(numericals).T\n    status = np.array(y == 1, dtype=np.int32)\n    status = np.squeeze(status)\n\n    return jnp.array(numericals), jnp.array(categoricals), jnp.array(status)\n\nData transformation for feeding it into the Numpyro model\n\nnumericals, categoricals, status = data_transform(X, y)\n\n\nx_numeric = numericals.astype(jnp.float32)\nx_categorical = [jnp.eye(c.max() + 1)[c] for c in categoricals]\nall_x = jnp.concatenate([x_numeric] + x_categorical, axis=1)\nnum_features = all_x.shape[1]\ny = status[jnp.newaxis, Ellipsis]\n\n\n\n2. Model\nWe will be using a logistic regression model with hierarchical prior on coefficient scales\n\\[\\begin{aligned}\n\\log \\tau_0 & \\sim \\mathcal{N}(0,10) & \\log \\tau_i & \\sim \\mathcal{N}\\left(\\log \\tau_0, 1\\right) \\\\\n\\beta_i & \\sim \\mathcal{N}\\left(0, \\tau_i\\right) & y & \\sim \\operatorname{Bernoulli}\\left(\\sigma\\left(\\beta X^T\\right)\\right)\n\\end{aligned}\\]\n\ndef german_credit():\n    log_tau_zero = numpyro.sample(\"log_tau_zero\", dist.Normal(0, 10))\n    log_tau_i = numpyro.sample(\n        \"log_tau_i\", dist.Normal(log_tau_zero, jnp.ones(num_features))\n    )\n    beta = numpyro.sample(\n        \"beta\", dist.Normal(jnp.zeros(num_features), jnp.exp(log_tau_i))\n    )\n    numpyro.sample(\n        \"obs\",\n        dist.Bernoulli(logits=jnp.einsum(\"nd,md-&gt;mn\", all_x, beta[jnp.newaxis, :])),\n        obs=y,\n    )\n\n\nnuts_kernel = NUTS(german_credit)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=1000)\nmcmc.run(rng_key, extra_fields=(\"num_steps\",))\n\nsample: 100%|██████████| 2000/2000 [00:21&lt;00:00, 94.07it/s, 63 steps of size 6.31e-02. acc. prob=0.87]\n\n\n\nmcmc.print_summary()\n\n\n                    mean       std    median      5.0%     95.0%     n_eff     r_hat\n       beta[0]      0.13      0.38      0.05     -0.36      0.74    284.06      1.00\n       beta[1]     -0.34      0.12     -0.34     -0.52     -0.15    621.55      1.00\n       beta[2]     -0.27      0.13     -0.27     -0.45     -0.03    542.13      1.00\n       beta[3]     -0.30      0.10     -0.30     -0.44     -0.11    566.55      1.00\n       beta[4]     -0.00      0.07     -0.00     -0.12      0.11    782.35      1.00\n       beta[5]      0.12      0.09      0.11     -0.02      0.27    728.28      1.01\n       beta[6]     -0.08      0.08     -0.07     -0.22      0.05    822.89      1.00\n       beta[7]     -0.05      0.07     -0.04     -0.19      0.05    752.66      1.00\n       beta[8]     -0.42      0.32     -0.39     -0.87      0.05    198.00      1.00\n       beta[9]     -0.07      0.26     -0.02     -0.50      0.31    220.27      1.00\n      beta[10]      0.26      0.31      0.18     -0.15      0.78    404.97      1.00\n      beta[11]      1.23      0.34      1.25      0.68      1.79    227.34      1.01\n      beta[12]     -0.26      0.34     -0.17     -0.81      0.22    349.10      1.00\n      beta[13]     -0.30      0.34     -0.21     -0.86      0.13    387.72      1.00\n      beta[14]      0.07      0.20      0.04     -0.26      0.38    240.45      1.03\n      beta[15]      0.10      0.22      0.05     -0.18      0.50    287.41      1.02\n      beta[16]      0.76      0.30      0.76      0.22      1.24    364.73      1.03\n      beta[17]     -0.53      0.28     -0.55     -0.94     -0.05    269.95      1.00\n      beta[18]      0.70      0.42      0.70     -0.02      1.29    367.28      1.00\n      beta[19]      0.17      0.40      0.06     -0.43      0.77    333.54      1.00\n      beta[20]      0.03      0.19      0.01     -0.23      0.39    381.57      1.00\n      beta[21]      0.18      0.22      0.13     -0.14      0.53    335.48      1.00\n      beta[22]     -0.05      0.32     -0.01     -0.56      0.46    439.54      1.00\n      beta[23]     -0.10      0.30     -0.04     -0.63      0.30    508.20      1.00\n      beta[24]     -0.34      0.36     -0.25     -0.94      0.12    283.15      1.00\n      beta[25]      0.14      0.40      0.04     -0.46      0.71    433.69      1.00\n      beta[26]     -0.01      0.19     -0.00     -0.34      0.28    438.64      1.00\n      beta[27]     -0.36      0.27     -0.33     -0.78      0.04    377.33      1.01\n      beta[28]     -0.07      0.22     -0.03     -0.43      0.26    493.09      1.00\n      beta[29]      0.01      0.22      0.00     -0.32      0.34    448.21      1.00\n      beta[30]      0.35      0.43      0.22     -0.18      1.08    314.69      1.00\n      beta[31]      0.41      0.33      0.40     -0.10      0.90    402.62      1.00\n      beta[32]     -0.03      0.21     -0.01     -0.39      0.30    525.23      1.00\n      beta[33]     -0.12      0.18     -0.09     -0.41      0.16    334.94      1.00\n      beta[34]     -0.02      0.16     -0.01     -0.24      0.26    318.25      1.00\n      beta[35]      0.42      0.27      0.42     -0.04      0.81    455.99      1.00\n      beta[36]      0.05      0.17      0.03     -0.18      0.35    506.34      1.00\n      beta[37]     -0.12      0.25     -0.06     -0.57      0.21    470.11      1.00\n      beta[38]     -0.07      0.20     -0.04     -0.39      0.24    410.71      1.00\n      beta[39]      0.36      0.24      0.35     -0.04      0.71    359.55      1.00\n      beta[40]      0.05      0.20      0.02     -0.29      0.35    441.70      1.00\n      beta[41]     -0.00      0.21      0.00     -0.34      0.37    513.67      1.00\n      beta[42]     -0.13      0.27     -0.08     -0.59      0.23    402.64      1.00\n      beta[43]      0.55      0.46      0.49     -0.11      1.28    570.74      1.00\n      beta[44]      0.19      0.21      0.15     -0.14      0.50    379.76      1.00\n      beta[45]     -0.00      0.16      0.00     -0.25      0.26    352.19      1.00\n      beta[46]      0.01      0.16      0.01     -0.25      0.25    411.05      1.00\n      beta[47]     -0.16      0.24     -0.11     -0.55      0.18    455.59      1.00\n      beta[48]     -0.12      0.24     -0.07     -0.55      0.21    322.67      1.04\n      beta[49]     -0.04      0.23     -0.02     -0.45      0.30    437.47      1.02\n      beta[50]      0.38      0.28      0.37     -0.03      0.82    266.19      1.04\n      beta[51]     -0.14      0.22     -0.09     -0.52      0.16    406.31      1.00\n      beta[52]      0.19      0.23      0.14     -0.14      0.55    338.97      1.00\n      beta[53]      0.04      0.22      0.02     -0.23      0.43    438.03      1.00\n      beta[54]      0.05      0.24      0.02     -0.32      0.41    522.43      1.00\n      beta[55]      0.02      0.14      0.01     -0.22      0.23    562.00      1.00\n      beta[56]     -0.01      0.13     -0.01     -0.24      0.21    638.20      1.00\n      beta[57]      0.01      0.17      0.00     -0.25      0.34    590.99      1.00\n      beta[58]     -0.07      0.18     -0.04     -0.34      0.23    481.37      1.00\n      beta[59]      0.13      0.19      0.09     -0.12      0.47    507.56      1.00\n      beta[60]     -0.14      0.33     -0.06     -0.64      0.37    303.00      1.00\n      beta[61]      0.48      0.56      0.32     -0.18      1.41    438.86      1.00\n  log_tau_i[0]     -1.51      0.95     -1.52     -3.03      0.11    290.78      1.00\n  log_tau_i[1]     -1.07      0.67     -1.11     -2.12      0.03    641.04      1.00\n  log_tau_i[2]     -1.24      0.76     -1.26     -2.47      0.03    666.31      1.00\n  log_tau_i[3]     -1.16      0.65     -1.19     -2.20     -0.10    821.60      1.00\n  log_tau_i[4]     -2.11      0.88     -2.13     -3.50     -0.61    806.15      1.00\n  log_tau_i[5]     -1.71      0.86     -1.68     -3.28     -0.44    697.00      1.00\n  log_tau_i[6]     -1.88      0.84     -1.91     -3.30     -0.58    623.56      1.00\n  log_tau_i[7]     -1.99      0.90     -1.98     -3.51     -0.65    710.21      1.00\n  log_tau_i[8]     -1.00      0.86     -0.96     -2.23      0.52    445.30      1.00\n  log_tau_i[9]     -1.69      0.93     -1.63     -3.17     -0.14    326.33      1.00\n log_tau_i[10]     -1.41      0.95     -1.35     -2.93      0.19    441.60      1.01\n log_tau_i[11]     -0.11      0.57     -0.12     -0.97      0.80    539.60      1.00\n log_tau_i[12]     -1.36      0.96     -1.31     -3.16      0.01    336.11      1.00\n log_tau_i[13]     -1.30      0.95     -1.26     -2.85      0.28    335.04      1.00\n log_tau_i[14]     -1.72      0.89     -1.70     -3.05     -0.25    584.38      1.00\n log_tau_i[15]     -1.65      0.92     -1.63     -3.07     -0.10    345.77      1.03\n log_tau_i[16]     -0.51      0.65     -0.49     -1.42      0.59    676.64      1.00\n log_tau_i[17]     -0.84      0.76     -0.76     -2.09      0.34    303.14      1.00\n log_tau_i[18]     -0.69      0.82     -0.59     -2.03      0.61    359.35      1.00\n log_tau_i[19]     -1.45      0.99     -1.42     -2.97      0.25    397.18      1.00\n log_tau_i[20]     -1.75      0.94     -1.73     -3.39     -0.40    617.54      1.00\n log_tau_i[21]     -1.51      0.88     -1.49     -3.16     -0.27    488.52      1.00\n log_tau_i[22]     -1.56      0.93     -1.56     -3.06     -0.10    348.20      1.00\n log_tau_i[23]     -1.58      0.94     -1.57     -3.05      0.02    278.69      1.00\n log_tau_i[24]     -1.26      1.00     -1.12     -2.91      0.29    205.38      1.00\n log_tau_i[25]     -1.53      0.95     -1.56     -3.11      0.02    351.09      1.00\n log_tau_i[26]     -1.73      0.91     -1.74     -3.17     -0.22    492.18      1.00\n log_tau_i[27]     -1.15      0.89     -1.08     -2.66      0.17    485.34      1.00\n log_tau_i[28]     -1.69      0.92     -1.65     -3.15     -0.19    425.75      1.00\n log_tau_i[29]     -1.71      0.99     -1.71     -3.19      0.01    374.58      1.00\n log_tau_i[30]     -1.24      0.99     -1.20     -2.74      0.50    327.47      1.00\n log_tau_i[31]     -1.02      0.89     -0.91     -2.40      0.51    587.85      1.00\n log_tau_i[32]     -1.71      0.94     -1.70     -3.22     -0.11    511.74      1.00\n log_tau_i[33]     -1.69      0.90     -1.68     -3.13     -0.28    538.65      1.00\n log_tau_i[34]     -1.82      0.92     -1.81     -3.35     -0.35    423.01      1.00\n log_tau_i[35]     -1.06      0.82     -1.00     -2.30      0.34    470.50      1.00\n log_tau_i[36]     -1.79      0.87     -1.76     -3.15     -0.34    527.47      1.00\n log_tau_i[37]     -1.58      0.95     -1.54     -3.11      0.04    485.52      1.00\n log_tau_i[38]     -1.71      0.87     -1.65     -3.18     -0.34    482.67      1.00\n log_tau_i[39]     -1.12      0.85     -1.01     -2.44      0.33    337.59      1.00\n log_tau_i[40]     -1.76      0.96     -1.73     -3.58     -0.36    533.15      1.00\n log_tau_i[41]     -1.74      0.94     -1.70     -3.26     -0.22    500.91      1.00\n log_tau_i[42]     -1.57      0.95     -1.54     -3.04      0.01    499.44      1.00\n log_tau_i[43]     -0.87      0.93     -0.74     -2.28      0.58    445.98      1.00\n log_tau_i[44]     -1.52      0.89     -1.45     -2.95     -0.13    442.63      1.00\n log_tau_i[45]     -1.84      0.94     -1.79     -3.21     -0.09    673.31      1.00\n log_tau_i[46]     -1.82      0.85     -1.83     -3.26     -0.56    579.66      1.00\n log_tau_i[47]     -1.54      0.90     -1.51     -3.35     -0.30    428.50      1.00\n log_tau_i[48]     -1.62      0.89     -1.60     -3.00     -0.15    413.30      1.01\n log_tau_i[49]     -1.71      0.95     -1.68     -3.23     -0.13    514.04      1.00\n log_tau_i[50]     -1.12      0.92     -0.99     -2.67      0.38    206.76      1.03\n log_tau_i[51]     -1.61      0.92     -1.58     -3.07     -0.03    477.41      1.00\n log_tau_i[52]     -1.54      0.90     -1.49     -2.96     -0.09    459.83      1.00\n log_tau_i[53]     -1.74      0.92     -1.69     -3.13     -0.14    509.51      1.00\n log_tau_i[54]     -1.68      0.95     -1.67     -3.07      0.10    477.21      1.00\n log_tau_i[55]     -1.87      0.97     -1.88     -3.49     -0.35    514.38      1.00\n log_tau_i[56]     -1.87      0.96     -1.84     -3.23     -0.12    574.80      1.00\n log_tau_i[57]     -1.77      0.86     -1.72     -3.26     -0.36    646.10      1.00\n log_tau_i[58]     -1.78      0.92     -1.77     -3.18     -0.15    617.59      1.00\n log_tau_i[59]     -1.67      0.93     -1.61     -3.19     -0.21    510.74      1.00\n log_tau_i[60]     -1.50      0.99     -1.44     -3.09      0.08    386.86      1.00\n log_tau_i[61]     -1.09      1.06     -1.00     -2.79      0.52    421.27      1.00\n  log_tau_zero     -1.49      0.26     -1.49     -1.90     -1.05    169.88      1.00\n\nNumber of divergences: 37\n\n\nFrom mcmc.print_summary it is evident that there are 37 divergences. Thus, we will use Variationally Inferred Parameterization (VIP) to reduce these divergences\n\ndata = az.from_numpyro(mcmc)\naz.plot_trace(data, compact=True);\n\n\n\n\n\n\n3. Reparameterization\nWe introduce a parameterization parameters \\(\\lambda \\in [0,1]\\) for any variable \\(z\\), and transform:\n=&gt; \\(z\\) ~ \\(N (z | μ, σ)\\)\n=&gt; by defining \\(z\\) ~ \\(N(λμ, σ^λ)\\)\n=&gt; \\(z\\) = \\(μ + σ^{1-λ}(z - λμ)\\).\nThus, using the above transformation the joint density can be transformed as follows:\n\\[\\begin{aligned}\np(\\theta, \\hat{\\mu}, \\mathbf{y}) & =\\mathcal{N}(\\theta \\mid 0,1) \\times\n\\mathcal{N}\\left(\\mu \\mid \\theta, \\sigma_\\mu\\right) \\times \\mathcal{N}(\\mathbf{y} \\mid \\mu, \\sigma)\n\\end{aligned}\\]\n\\[\\begin{aligned}\np(\\theta, \\hat{\\mu}, \\mathbf{y}) & =\\mathcal{N}(\\theta \\mid 0,1) \\times \\mathcal{N}\\left(\\hat{\\mu} \\mid \\lambda \\theta, \\sigma_\\mu^\\lambda\\right) \\times \\mathcal{N}\\left(\\mathbf{y} \\mid \\theta+\\sigma_\\mu^{1-\\lambda}(\\hat{\\mu}-\\lambda \\theta), \\sigma\\right)\n\\end{aligned}\\]\n\ndef german_credit_reparam(beta_centeredness=None):\n    def model():\n        log_tau_zero = numpyro.sample(\"log_tau_zero\", dist.Normal(0, 10))\n        log_tau_i = numpyro.sample(\n            \"log_tau_i\", dist.Normal(log_tau_zero, jnp.ones(num_features))\n        )\n        with numpyro.handlers.reparam(\n            config={\"beta\": LocScaleReparam(beta_centeredness)}\n        ):\n            beta = numpyro.sample(\n                \"beta\", dist.Normal(jnp.zeros(num_features), jnp.exp(log_tau_i))\n            )\n        numpyro.sample(\n            \"obs\",\n            dist.Bernoulli(logits=jnp.einsum(\"nd,md-&gt;mn\", all_x, beta[jnp.newaxis, :])),\n            obs=y,\n        )\n\n    return model\n\nNow, using SVI we optimize \\(\\lambda\\).\n\nmodel = german_credit_reparam()\nguide = AutoDiagonalNormal(model)\nsvi = SVI(model, guide, numpyro.optim.Adam(3e-4), Trace_ELBO(10))\nsvi_results = svi.run(rng_key, 10000)\n\n100%|██████████| 10000/10000 [00:16&lt;00:00, 588.87it/s, init loss: 2165.2424, avg. loss [9501-10000]: 576.7846]\n\n\n\nreparam_model = german_credit_reparam(\n    beta_centeredness=svi_results.params[\"beta_centered\"]\n)\n\n\nnuts_kernel = NUTS(reparam_model)\nmcmc_reparam = MCMC(nuts_kernel, num_warmup=1000, num_samples=1000)\nmcmc_reparam.run(rng_key, extra_fields=(\"num_steps\",))\n\nsample: 100%|██████████| 2000/2000 [00:07&lt;00:00, 285.41it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\n\n\n\nmcmc_reparam.print_summary()\n\n\n                         mean       std    median      5.0%     95.0%     n_eff     r_hat\n beta_decentered[0]      0.12      0.40      0.06     -0.48      0.80    338.70      1.00\n beta_decentered[1]     -0.45      0.15     -0.45     -0.70     -0.21    791.23      1.00\n beta_decentered[2]     -0.38      0.17     -0.38     -0.65     -0.09    691.79      1.00\n beta_decentered[3]     -0.41      0.13     -0.41     -0.61     -0.19   1022.79      1.00\n beta_decentered[4]     -0.01      0.11     -0.01     -0.18      0.20   1176.84      1.00\n beta_decentered[5]      0.19      0.14      0.19     -0.04      0.41   1194.41      1.00\n beta_decentered[6]     -0.13      0.14     -0.13     -0.36      0.09   1227.24      1.00\n beta_decentered[7]     -0.07      0.12     -0.06     -0.24      0.14   1096.31      1.00\n beta_decentered[8]     -0.46      0.34     -0.46     -0.99      0.08    330.30      1.00\n beta_decentered[9]     -0.03      0.32     -0.02     -0.57      0.49    310.35      1.00\nbeta_decentered[10]      0.35      0.39      0.30     -0.26      1.00    426.11      1.00\nbeta_decentered[11]      1.29      0.31      1.30      0.81      1.82    433.16      1.00\nbeta_decentered[12]     -0.32      0.39     -0.25     -0.96      0.24    521.05      1.00\nbeta_decentered[13]     -0.38      0.40     -0.32     -1.00      0.24    410.05      1.00\nbeta_decentered[14]      0.08      0.28      0.06     -0.37      0.57    457.72      1.00\nbeta_decentered[15]      0.14      0.30      0.10     -0.28      0.66    612.31      1.00\nbeta_decentered[16]      0.85      0.31      0.86      0.41      1.45    432.14      1.00\nbeta_decentered[17]     -0.64      0.28     -0.65     -1.05     -0.14    523.15      1.00\nbeta_decentered[18]      0.78      0.42      0.78      0.07      1.46    545.52      1.00\nbeta_decentered[19]      0.15      0.39      0.08     -0.50      0.80    662.60      1.00\nbeta_decentered[20]      0.04      0.25      0.03     -0.39      0.40    445.85      1.00\nbeta_decentered[21]      0.24      0.27      0.21     -0.20      0.65    477.68      1.00\nbeta_decentered[22]     -0.03      0.38     -0.01     -0.64      0.60    984.59      1.00\nbeta_decentered[23]     -0.13      0.34     -0.08     -0.72      0.35    702.87      1.00\nbeta_decentered[24]     -0.41      0.39     -0.37     -1.08      0.13    603.13      1.00\nbeta_decentered[25]      0.19      0.47      0.09     -0.48      0.92    529.68      1.00\nbeta_decentered[26]      0.00      0.25      0.01     -0.47      0.35    690.54      1.00\nbeta_decentered[27]     -0.46      0.31     -0.46     -0.95      0.04    464.44      1.00\nbeta_decentered[28]     -0.09      0.30     -0.06     -0.56      0.41    464.65      1.00\nbeta_decentered[29]      0.02      0.30      0.01     -0.47      0.52    747.44      1.00\nbeta_decentered[30]      0.38      0.44      0.31     -0.30      1.05    717.12      1.00\nbeta_decentered[31]      0.47      0.36      0.47     -0.09      1.03    564.18      1.00\nbeta_decentered[32]     -0.03      0.26     -0.02     -0.44      0.44    572.03      1.00\nbeta_decentered[33]     -0.17      0.25     -0.15     -0.63      0.19    713.40      1.00\nbeta_decentered[34]     -0.02      0.21     -0.01     -0.39      0.32    620.45      1.01\nbeta_decentered[35]      0.53      0.31      0.55     -0.03      1.00    681.60      1.00\nbeta_decentered[36]      0.09      0.24      0.06     -0.27      0.49    610.06      1.00\nbeta_decentered[37]     -0.14      0.31     -0.10     -0.74      0.28    826.87      1.00\nbeta_decentered[38]     -0.12      0.25     -0.11     -0.53      0.30    493.49      1.00\nbeta_decentered[39]      0.44      0.28      0.44     -0.01      0.89    542.71      1.00\nbeta_decentered[40]      0.05      0.26      0.03     -0.39      0.45    709.78      1.00\nbeta_decentered[41]      0.02      0.30      0.01     -0.52      0.46    389.41      1.00\nbeta_decentered[42]     -0.15      0.33     -0.10     -0.74      0.32    607.05      1.00\nbeta_decentered[43]      0.66      0.47      0.65     -0.11      1.38    539.31      1.01\nbeta_decentered[44]      0.25      0.27      0.23     -0.19      0.66    686.63      1.00\nbeta_decentered[45]     -0.01      0.22     -0.00     -0.36      0.35    909.70      1.00\nbeta_decentered[46]      0.02      0.22      0.01     -0.33      0.39    741.33      1.00\nbeta_decentered[47]     -0.25      0.31     -0.21     -0.72      0.25    487.26      1.00\nbeta_decentered[48]     -0.18      0.30     -0.15     -0.67      0.30    400.57      1.00\nbeta_decentered[49]     -0.07      0.30     -0.05     -0.56      0.41    594.38      1.00\nbeta_decentered[50]      0.46      0.31      0.46     -0.09      0.88    384.33      1.00\nbeta_decentered[51]     -0.23      0.27     -0.19     -0.69      0.16    561.35      1.00\nbeta_decentered[52]      0.22      0.25      0.21     -0.19      0.60    456.41      1.00\nbeta_decentered[53]      0.07      0.29      0.04     -0.41      0.53    500.99      1.00\nbeta_decentered[54]      0.05      0.30      0.02     -0.49      0.51    896.08      1.00\nbeta_decentered[55]      0.01      0.23      0.01     -0.34      0.43   1033.13      1.00\nbeta_decentered[56]     -0.02      0.19     -0.01     -0.33      0.28    717.87      1.00\nbeta_decentered[57]      0.01      0.23      0.01     -0.33      0.41    684.61      1.00\nbeta_decentered[58]     -0.09      0.26     -0.08     -0.55      0.30    455.66      1.01\nbeta_decentered[59]      0.20      0.27      0.18     -0.20      0.63    413.57      1.01\nbeta_decentered[60]     -0.14      0.37     -0.09     -0.76      0.46    411.83      1.00\nbeta_decentered[61]      0.58      0.57      0.50     -0.23      1.50    495.86      1.00\n       log_tau_i[0]     -1.56      0.91     -1.53     -2.94     -0.01    480.69      1.00\n       log_tau_i[1]     -1.07      0.64     -1.08     -1.99      0.06    950.02      1.00\n       log_tau_i[2]     -1.25      0.74     -1.23     -2.36     -0.06    796.37      1.00\n       log_tau_i[3]     -1.15      0.65     -1.18     -2.25     -0.14    838.40      1.00\n       log_tau_i[4]     -2.09      0.90     -2.12     -3.51     -0.49   1120.31      1.00\n       log_tau_i[5]     -1.70      0.84     -1.69     -3.10     -0.35   1291.74      1.00\n       log_tau_i[6]     -1.87      0.92     -1.84     -3.67     -0.58   1066.38      1.00\n       log_tau_i[7]     -2.06      0.89     -2.07     -3.42     -0.42    641.48      1.00\n       log_tau_i[8]     -1.11      0.91     -1.03     -2.79      0.19    482.69      1.00\n       log_tau_i[9]     -1.65      0.91     -1.62     -3.05     -0.04    772.73      1.00\n      log_tau_i[10]     -1.31      0.94     -1.27     -2.77      0.24    578.34      1.00\n      log_tau_i[11]     -0.04      0.49     -0.08     -0.81      0.77    736.20      1.00\n      log_tau_i[12]     -1.37      0.98     -1.32     -2.86      0.31    623.14      1.00\n      log_tau_i[13]     -1.28      0.97     -1.21     -2.78      0.30    653.51      1.00\n      log_tau_i[14]     -1.70      0.95     -1.71     -3.20     -0.15    831.88      1.00\n      log_tau_i[15]     -1.67      0.97     -1.65     -3.15      0.04    726.23      1.00\n      log_tau_i[16]     -0.53      0.65     -0.49     -1.55      0.54    558.56      1.00\n      log_tau_i[17]     -0.81      0.68     -0.80     -1.91      0.27    655.36      1.00\n      log_tau_i[18]     -0.71      0.83     -0.60     -1.89      0.75    536.55      1.00\n      log_tau_i[19]     -1.55      0.98     -1.53     -3.17     -0.02    675.27      1.00\n      log_tau_i[20]     -1.81      0.90     -1.79     -3.36     -0.41    823.93      1.00\n      log_tau_i[21]     -1.53      0.93     -1.51     -3.03     -0.04    767.64      1.00\n      log_tau_i[22]     -1.60      0.99     -1.58     -3.25     -0.01    840.71      1.00\n      log_tau_i[23]     -1.64      0.97     -1.60     -3.15      0.10    615.62      1.00\n      log_tau_i[24]     -1.21      0.96     -1.11     -2.61      0.47    674.58      1.00\n      log_tau_i[25]     -1.54      1.04     -1.49     -3.39     -0.01    509.69      1.00\n      log_tau_i[26]     -1.77      0.95     -1.74     -3.27     -0.13    915.91      1.00\n      log_tau_i[27]     -1.14      0.89     -1.07     -2.53      0.36    569.10      1.01\n      log_tau_i[28]     -1.72      0.93     -1.66     -3.25     -0.20    911.70      1.01\n      log_tau_i[29]     -1.70      0.91     -1.71     -3.16     -0.14    648.29      1.00\n      log_tau_i[30]     -1.28      1.02     -1.28     -2.99      0.34    575.90      1.00\n      log_tau_i[31]     -1.10      0.86     -1.04     -2.39      0.45    730.21      1.00\n      log_tau_i[32]     -1.79      0.95     -1.82     -3.32     -0.19    667.95      1.00\n      log_tau_i[33]     -1.64      0.86     -1.62     -3.04     -0.32    948.64      1.00\n      log_tau_i[34]     -1.87      0.92     -1.88     -3.29     -0.25    909.49      1.00\n      log_tau_i[35]     -1.00      0.85     -0.95     -2.44      0.25    672.42      1.00\n      log_tau_i[36]     -1.76      0.92     -1.73     -3.19     -0.22    889.56      1.00\n      log_tau_i[37]     -1.63      1.00     -1.58     -3.11      0.12    973.29      1.00\n      log_tau_i[38]     -1.72      0.85     -1.70     -3.13     -0.30    837.73      1.00\n      log_tau_i[39]     -1.15      0.80     -1.13     -2.32      0.18    627.15      1.00\n      log_tau_i[40]     -1.76      0.93     -1.70     -3.36     -0.34    686.88      1.00\n      log_tau_i[41]     -1.75      0.96     -1.74     -3.20     -0.09    612.83      1.00\n      log_tau_i[42]     -1.62      0.91     -1.62     -3.21     -0.25    596.18      1.00\n      log_tau_i[43]     -0.79      0.93     -0.74     -2.20      0.83    560.52      1.01\n      log_tau_i[44]     -1.52      0.94     -1.48     -3.08      0.01    888.27      1.00\n      log_tau_i[45]     -1.83      0.90     -1.84     -3.27     -0.44   1122.53      1.00\n      log_tau_i[46]     -1.83      0.89     -1.78     -3.21     -0.31    990.03      1.00\n      log_tau_i[47]     -1.56      0.93     -1.48     -3.20     -0.21    736.01      1.00\n      log_tau_i[48]     -1.59      0.94     -1.54     -3.20     -0.01    588.77      1.00\n      log_tau_i[49]     -1.71      0.93     -1.68     -3.17     -0.17    813.94      1.00\n      log_tau_i[50]     -1.15      0.83     -1.11     -2.51      0.14    514.68      1.00\n      log_tau_i[51]     -1.54      0.89     -1.51     -2.97     -0.13    780.67      1.00\n      log_tau_i[52]     -1.59      0.93     -1.56     -3.21     -0.28    807.47      1.00\n      log_tau_i[53]     -1.74      0.92     -1.72     -3.10     -0.15    657.89      1.00\n      log_tau_i[54]     -1.74      0.93     -1.74     -3.09     -0.10    867.29      1.00\n      log_tau_i[55]     -1.89      0.94     -1.87     -3.34     -0.27   1132.90      1.00\n      log_tau_i[56]     -1.89      0.92     -1.89     -3.27     -0.27    980.72      1.00\n      log_tau_i[57]     -1.84      0.91     -1.84     -3.33     -0.38    813.32      1.00\n      log_tau_i[58]     -1.75      0.93     -1.74     -3.25     -0.34    583.52      1.00\n      log_tau_i[59]     -1.59      0.96     -1.53     -3.11     -0.06    754.39      1.01\n      log_tau_i[60]     -1.58      0.98     -1.52     -3.23     -0.08    561.05      1.00\n      log_tau_i[61]     -0.98      1.03     -0.85     -2.55      0.79    502.63      1.00\n       log_tau_zero     -1.49      0.26     -1.49     -1.91     -1.10    220.32      1.00\n\nNumber of divergences: 1\n\n\nThe number of divergences have significantly reduced from 37 to 1.\n\ndata = az.from_numpyro(mcmc_reparam)\naz.plot_trace(data, compact=True, figsize=(15, 25));\n\n\n\n\n\n\n4. References:\n\nhttps://arxiv.org/abs/1906.03028\nhttps://github.com/mgorinova/autoreparam/tree/master"
  },
  {
    "objectID": "posts/2023-01-15-cp.html",
    "href": "posts/2023-01-15-cp.html",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Guess the following images:\n\n\n\nimage.png\n\n\nPrediction set generated by conformal prediction for the images:\n\n\n\nimage.png\n\n\n\n\n\nUsing conformal prediction we aim to generate rigorous, finite sample confidence intervals for any model and any dataset. Unlike a point prediction from neural network, here we will get a confidence interval in which desired output is guaranteed to be.\n\n\n\n\nBegin with a fitted predicted model which we call \\(\\hat{f}\\).\nCreate a predicted set (set of possible labels) for this model using a small amount of calibration data."
  },
  {
    "objectID": "posts/2023-01-15-cp.html#why-use-conformal-prediction",
    "href": "posts/2023-01-15-cp.html#why-use-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Guess the following images:\n\n\n\nimage.png\n\n\nPrediction set generated by conformal prediction for the images:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#aim",
    "href": "posts/2023-01-15-cp.html#aim",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Using conformal prediction we aim to generate rigorous, finite sample confidence intervals for any model and any dataset. Unlike a point prediction from neural network, here we will get a confidence interval in which desired output is guaranteed to be."
  },
  {
    "objectID": "posts/2023-01-15-cp.html#outline",
    "href": "posts/2023-01-15-cp.html#outline",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Begin with a fitted predicted model which we call \\(\\hat{f}\\).\nCreate a predicted set (set of possible labels) for this model using a small amount of calibration data."
  },
  {
    "objectID": "posts/2023-01-15-cp.html#given",
    "href": "posts/2023-01-15-cp.html#given",
    "title": "Conformal Prediction",
    "section": "Given",
    "text": "Given\n\nA calibration dataset \\(\\{(x_i,y_i)\\}_{i=1}^n\\) (This is the dataset that the model does not see during training).\nA \\(model\\) \\(\\hat{\\pi}(x) = P[Y=y|X=x]\\)\nA \\(new\\) \\(data\\) \\(point\\) \\(x_{n+1}\\) to test the model"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#goal",
    "href": "posts/2023-01-15-cp.html#goal",
    "title": "Conformal Prediction",
    "section": "Goal",
    "text": "Goal\nPredict a set \\(\\tau(X_{test})\\) for the data point \\(X_{test}\\) that is a subset of the label space \\(i.e.\\) predict a set, \\(\\tau(X_{test}) \\subseteq y\\). This set should contain the true class \\(Y_{test}\\) and should be valid in the following sense:\n$ 1 - P[Y_{test} (X_{test})] - + $\nhere \\(\\alpha\\) is a user chosen rate in \\(\\in [0,1]\\), \\(y\\) is the set of all labels & \\(n\\) is the number of points in calibration dataset. The above mentioned property is called Marginal Coverage."
  },
  {
    "objectID": "posts/2023-01-15-cp.html#objective-for-the-sets",
    "href": "posts/2023-01-15-cp.html#objective-for-the-sets",
    "title": "Conformal Prediction",
    "section": "Objective for the sets",
    "text": "Objective for the sets\n\nExact coverage\nSmall size\nSize of the set should indicate the difficulty of the examples \\(i.e.\\) Adaptivity"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#general-method-for-conformal-prediction",
    "href": "posts/2023-01-15-cp.html#general-method-for-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "General Method for Conformal Prediction",
    "text": "General Method for Conformal Prediction\n\nIdentify a heuristic notion of uncertainity\nDefine a score function \\(S(x,y)\\) based on the values in step 1. In general large values of \\(S\\) corresponds to a bad fit between \\(x\\) \\(\\&\\) \\(y\\)\nCompute \\(\\hat{q} : \\frac{\\lceil{(n+1)(1-\\alpha)}\\rceil}{n}\\) quantile of \\(S(x,y)\\) on calibration dataset\nTo obtain the prediction set: \\(\\tau(x) = \\{y:S(x,y) \\le \\hat{q} \\}\\)"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#now-lets-implement-conformal-prediction",
    "href": "posts/2023-01-15-cp.html#now-lets-implement-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "Now Lets’ implement conformal Prediction",
    "text": "Now Lets’ implement conformal Prediction\nA simpler version of the conformal prediction\n\nCompute \\(\\hat{q} : \\alpha\\) quantile of \\(S(x,y)\\) on calibration dataset where \\(S(x,y)\\) is the score function correspoding to the true label\nTo obtain the prediction set: \\(\\tau(x) = \\{y:S(x,y) \\ge \\hat{q} \\}\\)\n\n\ncalib_target_results = np.array(calib_target_results)\ncalib_prediction_results = np.array(calib_prediction_results)\ntest_prediction_results = np.array(test_prediction_results)\ntest_target_results = np.array(test_target_results)\n\ncalib_prediction_results.shape, calib_target_results.shape, test_prediction_results.shape, test_target_results.shape\n\n((5000, 10), (5000,), (5000, 10), (5000,))\n\n\n\ncalib_df = pd.DataFrame(calib_prediction_results)\ncalib_df[\"Max\"] = calib_df.max(axis=1)\ncalib_df[\"Max_idx\"] = calib_df.idxmax(axis=1)\ncalib_df[\"True_idx\"] = calib_target_results\ncalib_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n0\n7.672552e-08\n5.311417e-07\n1.562821e-04\n3.211690e-09\n4.822823e-04\n2.262360e-06\n9.993560e-01\n3.220139e-11\n1.609646e-06\n9.680530e-07\n0.999356\n6\n6\n\n\n1\n7.041307e-08\n1.779545e-07\n6.658971e-06\n4.084817e-07\n6.008969e-08\n1.122653e-08\n1.417746e-04\n1.310595e-09\n9.998498e-01\n9.002325e-07\n0.999850\n8\n8\n\n\n2\n1.891271e-06\n7.539936e-07\n8.252732e-06\n7.875642e-05\n6.665982e-01\n2.489255e-06\n7.079141e-04\n4.619782e-05\n3.568919e-02\n2.968663e-01\n0.666598\n4\n5\n\n\n3\n3.600329e-08\n4.196039e-09\n4.928238e-06\n3.145105e-09\n9.909703e-01\n3.594812e-08\n2.783901e-05\n7.394720e-06\n8.526304e-07\n8.988610e-03\n0.990970\n4\n4\n\n\n4\n8.007726e-06\n8.686094e-05\n1.262506e-04\n9.905047e-01\n2.181237e-07\n3.404921e-07\n8.625836e-06\n2.125504e-06\n9.200612e-03\n6.224329e-05\n0.990505\n3\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n3.017699e-06\n8.860756e-05\n3.417503e-04\n9.827271e-01\n5.802370e-05\n1.595727e-06\n2.064353e-04\n2.161075e-06\n1.435820e-02\n2.213240e-03\n0.982727\n3\n3\n\n\n4996\n5.440750e-13\n9.999996e-01\n7.675386e-08\n5.033297e-11\n2.031701e-11\n4.493560e-11\n7.547337e-10\n3.736634e-08\n2.838881e-07\n3.246134e-12\n1.000000\n1\n1\n\n\n4997\n5.372684e-09\n3.440873e-09\n1.484947e-07\n9.999758e-01\n2.131953e-12\n4.374669e-11\n2.115596e-12\n2.734103e-06\n2.123476e-05\n9.234377e-08\n0.999976\n3\n3\n\n\n4998\n1.966202e-07\n6.421658e-07\n5.014269e-05\n2.109797e-09\n2.172950e-05\n5.110368e-07\n9.999149e-01\n3.196222e-10\n3.617991e-06\n8.205562e-06\n0.999915\n6\n4\n\n\n4999\n2.777072e-14\n1.000000e+00\n1.977435e-08\n3.481474e-11\n2.748897e-12\n9.301366e-12\n5.802548e-11\n3.565957e-08\n2.786610e-08\n1.873916e-12\n1.000000\n1\n1\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\ntest_df = pd.DataFrame(test_prediction_results)\ntest_df[\"Max\"] = test_df.max(axis=1)\ntest_df[\"Max_idx\"] = test_df.idxmax(axis=1)\ntest_df[\"True_idx\"] = test_target_results\ntest_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n0\n1.507169e-09\n6.793355e-10\n9.392306e-10\n3.272889e-09\n4.279739e-09\n1.000421e-10\n7.060916e-13\n9.973305e-01\n3.673427e-07\n2.669133e-03\n0.997331\n7\n7\n\n\n1\n1.528662e-08\n4.265159e-12\n3.008223e-05\n9.999698e-01\n1.717640e-15\n2.861857e-14\n1.901743e-12\n4.749006e-13\n8.433129e-08\n6.817440e-14\n0.999970\n3\n3\n\n\n2\n1.440134e-10\n3.157558e-06\n2.407116e-05\n5.108404e-11\n5.610714e-07\n1.101829e-08\n9.999588e-01\n4.209422e-15\n1.334558e-05\n3.933881e-10\n0.999959\n6\n6\n\n\n3\n6.468001e-12\n9.999986e-01\n3.110969e-07\n3.365797e-09\n3.275069e-11\n3.477734e-10\n1.458348e-08\n3.999051e-08\n1.144535e-06\n1.596769e-10\n0.999999\n1\n1\n\n\n4\n8.815193e-05\n6.341890e-03\n8.312734e-04\n1.951014e-04\n1.125594e-02\n2.372148e-04\n7.425601e-03\n1.311864e-03\n1.279543e-02\n9.595175e-01\n0.959518\n9\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n2.418953e-08\n7.203251e-08\n1.530548e-07\n2.897828e-06\n5.851566e-07\n6.657368e-09\n3.703725e-08\n6.012499e-05\n9.920814e-01\n7.854681e-03\n0.992081\n8\n8\n\n\n4996\n4.415281e-09\n1.504091e-08\n2.217878e-06\n9.999076e-01\n1.170547e-09\n1.767450e-10\n1.133458e-08\n1.103373e-09\n8.993938e-05\n2.646218e-07\n0.999908\n3\n3\n\n\n4997\n1.863301e-18\n3.640452e-17\n5.301770e-13\n5.253428e-15\n9.999909e-01\n7.949940e-15\n1.362663e-11\n6.954227e-13\n6.574446e-12\n9.051804e-06\n0.999991\n4\n4\n\n\n4998\n6.859559e-17\n7.214033e-16\n3.279532e-11\n1.938939e-14\n9.999964e-01\n1.438514e-13\n1.612270e-10\n2.956339e-12\n7.959585e-12\n3.579523e-06\n0.999996\n4\n4\n\n\n4999\n2.643393e-01\n7.046102e-07\n1.252546e-02\n5.733218e-01\n1.109647e-07\n3.844558e-07\n2.375773e-02\n1.729390e-07\n1.260542e-01\n1.263923e-07\n0.573322\n3\n5\n\n\n\n\n5000 rows × 13 columns"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#intuitive-understanding",
    "href": "posts/2023-01-15-cp.html#intuitive-understanding",
    "title": "Conformal Prediction",
    "section": "Intuitive understanding",
    "text": "Intuitive understanding\n\nstds_cal = np.std(calib_prediction_results, axis=1)\nmin_std_indices_cal = np.argsort(stds_cal)\n\nSorting based on std deviation in the rows\n\ncalib_df_std = calib_df.loc[min_std_indices_cal]\ncalib_df_std\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n4792\n2.204627e-01\n1.263805e-02\n1.064651e-01\n2.497595e-02\n3.831481e-02\n1.915999e-02\n2.428759e-01\n4.884591e-02\n2.138946e-01\n7.236703e-02\n0.242876\n6\n5\n\n\n3635\n3.720535e-03\n3.984575e-02\n6.415164e-02\n8.764555e-02\n3.574266e-01\n1.044143e-02\n2.420991e-02\n2.146130e-01\n4.631169e-02\n1.516339e-01\n0.357427\n4\n6\n\n\n1115\n1.880663e-02\n2.226060e-01\n8.889224e-02\n6.196426e-02\n1.060972e-02\n5.382521e-03\n2.736638e-01\n7.305532e-03\n2.858033e-01\n2.496605e-02\n0.285803\n8\n5\n\n\n1031\n1.033852e-01\n5.177050e-02\n7.211524e-02\n1.307259e-01\n8.214290e-03\n5.626875e-03\n2.568235e-01\n5.283331e-03\n3.498454e-01\n1.620973e-02\n0.349845\n8\n5\n\n\n1778\n7.243351e-02\n5.959544e-05\n1.995622e-01\n1.425000e-05\n2.422987e-01\n4.558027e-04\n1.450675e-01\n8.497129e-03\n4.330557e-04\n3.311783e-01\n0.331178\n9\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2909\n1.735541e-15\n2.001958e-12\n8.669379e-16\n1.821178e-15\n2.716592e-15\n2.300831e-16\n2.615356e-20\n1.000000e+00\n8.539057e-13\n3.396687e-09\n1.000000\n7\n7\n\n\n2914\n1.000000e+00\n5.918221e-19\n1.986233e-10\n1.237661e-14\n4.863426e-19\n3.770102e-16\n1.504733e-11\n7.523226e-09\n1.627343e-10\n1.057123e-13\n1.000000\n0\n0\n\n\n2920\n1.139901e-11\n6.290391e-12\n4.389559e-08\n1.000000e+00\n4.736745e-17\n5.172185e-15\n1.662083e-15\n2.492923e-11\n5.069354e-10\n1.366577e-12\n1.000000\n3\n3\n\n\n2847\n2.374335e-11\n5.410563e-10\n1.000000e+00\n6.474658e-10\n3.318972e-12\n1.285993e-12\n1.194038e-08\n1.259493e-15\n1.686373e-10\n9.267352e-15\n1.000000\n2\n2\n\n\n4999\n2.777072e-14\n1.000000e+00\n1.977435e-08\n3.481474e-11\n2.748897e-12\n9.301366e-12\n5.802548e-11\n3.565957e-08\n2.786610e-08\n1.873916e-12\n1.000000\n1\n1\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\ny1 = calib_prediction_results[min_std_indices_cal[0]]\ny2 = calib_prediction_results[min_std_indices_cal[1]]\ny3 = calib_prediction_results[min_std_indices_cal[2]]\ny4 = calib_prediction_results[4500]\n\nx = np.arange(10)\n\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\n\n# Add bar plots to the subplots\nbars1 = axs[0, 0].bar(x, y1, color=\"#ff7f0e\", width=0.6)\naxs[0, 0].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[0]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars2 = axs[0, 1].bar(x, y2, color=\"#2ca02c\", width=0.6)\naxs[0, 1].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[1]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars3 = axs[1, 0].bar(x, y3, color=\"#1f77b4\", width=0.6)\naxs[1, 0].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[2]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars4 = axs[1, 1].bar(x, y4, color=\"#d62728\", width=0.6)\naxs[1, 1].set_title(\n    f\"True Label: {calib_target_results[4500]}\", fontsize=12, fontweight=\"bold\"\n)\n\n# Add labels and title to the figure\nfig.suptitle(\"Model's output on calibration dataset\", fontsize=14, fontweight=\"bold\")\n\nfor ax in axs.flat:\n    ax.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\n# Fine-tune the subplot layout\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nbars1[calib_target_results[min_std_indices_cal[0]]].set_color(\"#9467bd\")\nbars2[calib_target_results[min_std_indices_cal[1]]].set_color(\"#9467bd\")\nbars3[calib_target_results[min_std_indices_cal[2]]].set_color(\"#9467bd\")\nbars4[calib_target_results[4500]].set_color(\"#9467bd\")\n\n\n\n\n\ncalib_true = calib_prediction_results[\n    np.arange(calib_prediction_results.shape[0]), calib_target_results\n]\n\nFrom the above plot it is evident that most of the softmax outputs corresponding to the true label are either close to 1 or close to 0. Thus, once we find out the value corresponding to the threshold of the 0 peak in the plot. Any quantile value just above this will quickly go near the next peak as there is no distribution mass for the rest of the softmax outputs.\n\n## Quantile value that we use to predict the prediction set\nqhat_intuit = np.quantile(calib_true, 0.15)  ## taking 15% quantile\nqhat_intuit\n\n0.17026243805885327\n\n\n_This leads to the fact that 85% of examples have their true class softmax score above \\(\\hat{q}_{intuit}\\)_\nSorting the test dataset according to std\n\nstds_test = np.std(test_prediction_results, axis=1)\nmin_std_indices_test = np.argsort(stds_test)\n\n\ntest_df_std = test_df.loc[min_std_indices_test]\ntest_df_std\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n1646\n9.070086e-06\n7.102170e-02\n1.074573e-01\n2.019677e-01\n2.011996e-01\n1.028181e-03\n1.132987e-03\n1.039726e-01\n1.606203e-01\n1.515905e-01\n0.201968\n3\n8\n\n\n1184\n4.843292e-08\n1.841913e-01\n5.504493e-02\n1.019774e-01\n1.562013e-01\n2.759257e-04\n9.354739e-04\n1.538243e-01\n1.096576e-02\n3.365835e-01\n0.336583\n9\n7\n\n\n3654\n4.213768e-04\n7.209036e-02\n1.887893e-02\n1.189776e-02\n3.334404e-01\n2.851049e-03\n1.309165e-01\n8.140079e-04\n2.165484e-01\n2.121412e-01\n0.333440\n4\n5\n\n\n3019\n2.256842e-01\n2.321515e-07\n2.911193e-01\n2.409384e-01\n1.869016e-11\n2.536230e-09\n5.548395e-02\n2.447040e-10\n1.867739e-01\n2.661302e-11\n0.291119\n2\n5\n\n\n4896\n1.610583e-01\n4.679085e-05\n8.821968e-04\n1.386619e-01\n1.308930e-03\n8.392150e-05\n3.836229e-02\n5.723350e-02\n2.713310e-01\n3.310311e-01\n0.331031\n9\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3128\n1.000000e+00\n4.840718e-18\n1.905646e-08\n2.940498e-11\n1.603996e-18\n5.580763e-16\n4.068423e-11\n1.949603e-11\n2.676126e-09\n3.716259e-15\n1.000000\n0\n0\n\n\n4507\n1.798757e-18\n1.045387e-13\n5.538239e-17\n2.234547e-16\n7.610493e-18\n6.503045e-19\n4.574509e-24\n1.000000e+00\n1.466630e-14\n1.054316e-12\n1.000000\n7\n7\n\n\n3130\n5.435448e-18\n5.444775e-11\n1.383103e-15\n2.973889e-15\n2.213195e-15\n1.832753e-16\n7.384352e-22\n1.000000e+00\n3.005146e-12\n3.994342e-10\n1.000000\n7\n7\n\n\n3142\n1.000000e+00\n2.979347e-17\n1.458575e-08\n1.863164e-11\n1.070295e-18\n1.046285e-15\n7.635514e-11\n2.087787e-12\n5.457018e-09\n3.388109e-15\n1.000000\n0\n0\n\n\n3060\n1.986817e-17\n5.229983e-14\n1.823637e-17\n3.066719e-17\n1.413327e-16\n2.220211e-18\n7.849721e-23\n1.000000e+00\n4.188006e-14\n2.416875e-10\n1.000000\n7\n7\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\n## Forming prediction sets\n\ny1 = test_prediction_results[min_std_indices_test[1]]\ny2 = test_prediction_results[min_std_indices_test[2]]\ny3 = test_prediction_results[min_std_indices_test[3]]\ny4 = test_prediction_results[4500]\n\ntest_array_indices = [\n    min_std_indices_test[1],\n    min_std_indices_test[2],\n    min_std_indices_test[3],\n    4500,\n]\n\nx = np.arange(10)\n\n# Create a new figure with 3 subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Add bar plots to the subplots\nbars1 = axs[0, 0].bar(x, y1, color=\"#ff7f0e\", width=0.6)\naxs[0, 0].set_title(\n    f\"True Label: {test_target_results[test_array_indices[0]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars2 = axs[0, 1].bar(x, y2, color=\"#2ca02c\", width=0.6)\naxs[0, 1].set_title(\n    f\"True Label: {test_target_results[test_array_indices[1]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars3 = axs[1, 0].bar(x, y3, color=\"#1f77b4\", width=0.6)\naxs[1, 0].set_title(\n    f\"True Label: {test_target_results[test_array_indices[2]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars4 = axs[1, 1].bar(x, y4, color=\"#d62728\", width=0.6)\naxs[1, 1].set_title(\n    f\"True Label: {test_target_results[test_array_indices[3]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\n\n# Add labels and title to the figure\nfig.suptitle(\"Model's output on test dataset\", fontsize=14, fontweight=\"bold\")\n\nfor ax in axs.flat:\n    ax.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axs.flatten():\n    ax.axhline(y=qhat_intuit, color=\"black\", linewidth=2)\n\n# Fine-tune the subplot layout\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n\nbars1_idx = y1 &gt; qhat_intuit\nbars2_idx = y2 &gt; qhat_intuit\nbars3_idx = y3 &gt; qhat_intuit\nbars4_idx = y4 &gt; qhat_intuit\n\nfor i in range(10):\n    if bars1_idx[i]:\n        bars1[i].set_color(\"#8c564b\")\n    if bars2_idx[i]:\n        bars2[i].set_color(\"#8c564b\")\n    if bars3_idx[i]:\n        bars3[i].set_color(\"#8c564b\")\n    if bars4_idx[i]:\n        bars4[i].set_color(\"#8c564b\")\n\nbars1[test_target_results[test_array_indices[0]]].set_color(\"#9467bd\")\nbars2[test_target_results[test_array_indices[1]]].set_color(\"#9467bd\")\nbars3[test_target_results[test_array_indices[2]]].set_color(\"#9467bd\")\nbars4[test_target_results[test_array_indices[3]]].set_color(\"#9467bd\")\n\n\n\n\n_All the bars above the \\(\\hat{q}_{intuit}\\) are the part of the prediction set for the corresponding test dataset_\n\n# Use numpy indexing to get the softmax scores for each image corresponding to their true labels\ntest_true = test_prediction_results[\n    np.arange(test_prediction_results.shape[0]), test_target_results\n]"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#implementing-conformal-prediction-using-the-general-method",
    "href": "posts/2023-01-15-cp.html#implementing-conformal-prediction-using-the-general-method",
    "title": "Conformal Prediction",
    "section": "Implementing conformal prediction using the General Method",
    "text": "Implementing conformal prediction using the General Method\n1. Here the heuristic notion of uncertainity is softmax output\n\n# Problem setup\nn = 5000  # number of calibration points\nalpha = 0.15  # 1-alpha is the desired coverage\n\n\ncal_scores = 1 - calib_true\n\n2. Defining the score function as: \\(S(x,y)\\) = \\(1- softmax(x_i,y_i)\\)\n\nq_level = np.ceil((n + 1) * (1 - alpha)) / n  ## alpha = 0.1, n = 1000, q_level = 0.901\nqhat = np.quantile(\n    cal_scores, q_level\n)  ## value for which 90% of the scores are less than it\nqhat\n\n0.8337927011847496\n\n\n3. Calculating \\(\\hat{q}\\)\n4. Creating the prediction set\n$ C(x) = {y:S(x,y) }$\n\nfig = plt.figure()\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(test_images[test_array_indices[i]][0], cmap=\"gray\", interpolation=\"none\")\n    test_scores = 1 - test_prediction_results[test_array_indices[i]]\n    prediction_set = test_scores &lt; qhat\n    indices = []\n    for index, val in enumerate(prediction_set):\n        if val:\n            indices.append(index)\n\n    true_label = test_target_results[test_array_indices]\n\n    plt.title(f\"Set: {indices} True:{true_label[i]}\")\n    plt.xticks([])\n    plt.yticks([])"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#insights-and-summary",
    "href": "posts/2023-01-15-cp.html#insights-and-summary",
    "title": "Conformal Prediction",
    "section": "Insights and Summary:",
    "text": "Insights and Summary:\n\nGiven an image \\(x\\) and label \\(j\\). Softmax measures \\(P(Y = j | X = x)\\). However, we have no guarantee that the softmax outputs are any good; they maybe arbitrarily overfit or otherwise untrustworthy. Thus, we use the holdout set to adjust for their deficiencies.\nIn the above example the holdout set contained 5000 examples that the model never saw during training which gives us an honest appraisal of its performance.\nHere, the conformal score was 1 - softmax output of the true class. Then we took $ = 1 - $ quantile of the scores.\nUsing Step 3 at the test time, we got the softmax outputs of a new image \\(X_{test}\\) and collected all classes with outputs above $ 1 − $ into a prediction set \\(C(X_{test})\\)\n\n\nImplementation using Imagenet\n\nif not os.path.exists(\"../data\"):\n    os.system(\"gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz\")\n    os.system(\"tar -xf ../data.tar.gz -C ../\")\n    os.system(\"rm ../data.tar.gz\")\nif not os.path.exists(\"../data/imagenet/human_readable_labels.json\"):\n    !wget -nv -O ../data/imagenet/human_readable_labels.json -L https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\ndata = np.load(\"../data/imagenet/imagenet-resnet152.npz\")\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nsmx = data[\"smx\"]\nlabels = data[\"labels\"].astype(int)\n\n\n# Problem setup\nn = 1000  # number of calibration points\nalpha = 0.1  # 1-alpha is the desired coverage\n\n\nidx = np.array([1] * n + [0] * (smx.shape[0] - n)) &gt; 0\nnp.random.shuffle(idx)\ncal_smx, val_smx = smx[idx, :], smx[~idx, :]\ncal_labels, val_labels = labels[idx], labels[~idx]\n\n\nimagenet_calib_df = pd.DataFrame(cal_smx)\nimagenet_calib_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n\n\n\n\n0\n9.646587e-01\n1.350361e-05\n2.151330e-07\n1.699551e-06\n2.384544e-06\n1.646308e-06\n1.394906e-07\n2.117511e-08\n3.057390e-09\n4.086660e-10\n...\n5.279725e-09\n9.462966e-08\n1.185813e-08\n5.307772e-10\n2.161666e-07\n1.007043e-08\n9.514928e-08\n8.144019e-07\n1.339111e-07\n6.878381e-09\n\n\n1\n9.992527e-01\n1.005275e-06\n5.030975e-08\n2.312540e-08\n6.919812e-07\n5.068674e-08\n5.945228e-08\n2.580266e-09\n1.059923e-09\n5.929557e-11\n...\n3.178681e-10\n3.120479e-09\n2.160190e-09\n6.229624e-10\n3.004631e-08\n2.982520e-10\n3.827619e-08\n2.310420e-07\n9.114003e-08\n6.513726e-10\n\n\n2\n9.998410e-01\n2.081634e-08\n2.163244e-09\n1.033369e-08\n9.947884e-09\n4.689700e-09\n4.500399e-09\n4.603104e-11\n2.665861e-11\n4.032333e-12\n...\n1.170430e-10\n1.740400e-10\n1.001514e-10\n2.484425e-11\n6.860166e-10\n5.098253e-11\n9.393597e-10\n3.404014e-08\n1.460277e-09\n8.657306e-13\n\n\n3\n9.996231e-01\n6.980400e-06\n7.547856e-08\n1.445374e-07\n5.570853e-07\n1.413495e-06\n1.172659e-07\n4.219434e-09\n9.644072e-10\n4.150972e-11\n...\n1.467458e-09\n1.727905e-08\n4.188708e-08\n8.764998e-10\n3.017675e-08\n1.152834e-09\n2.212167e-08\n5.312061e-07\n7.742039e-09\n6.035842e-10\n\n\n4\n3.740840e-07\n9.997242e-01\n6.791072e-10\n4.707819e-09\n3.942747e-09\n3.235905e-07\n1.922253e-08\n7.563041e-09\n4.848560e-09\n1.836324e-11\n...\n1.428742e-09\n2.168828e-09\n7.591582e-10\n7.432400e-11\n8.145293e-10\n6.436701e-10\n6.601004e-10\n2.608228e-10\n1.372821e-09\n1.686885e-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n1.338609e-13\n2.481204e-13\n2.981873e-12\n2.992094e-12\n8.025680e-13\n1.196738e-12\n1.058158e-12\n1.031028e-14\n8.366420e-14\n1.834110e-16\n...\n2.835526e-10\n1.185883e-09\n5.707088e-11\n1.053065e-10\n6.115803e-09\n1.000000e+00\n6.104551e-11\n3.662891e-10\n2.519912e-13\n1.617798e-11\n\n\n996\n1.461727e-05\n2.962031e-05\n2.201413e-07\n3.813796e-08\n1.196295e-07\n3.246882e-07\n2.723306e-06\n2.824044e-06\n9.639041e-06\n2.289236e-05\n...\n2.566843e-02\n1.156482e-01\n1.163806e-03\n6.741311e-03\n1.174134e-03\n1.015575e-03\n4.271199e-01\n3.027194e-01\n3.415058e-04\n8.422194e-07\n\n\n997\n3.484188e-06\n1.047919e-07\n7.475886e-08\n3.465406e-07\n1.347377e-06\n4.767327e-06\n4.182576e-08\n4.709841e-08\n1.508235e-08\n1.065061e-08\n...\n2.632773e-06\n2.174731e-05\n3.774206e-04\n1.449335e-04\n8.616778e-01\n8.140442e-05\n1.180090e-04\n1.335993e-01\n7.427732e-06\n3.561391e-08\n\n\n998\n7.082336e-04\n2.196637e-05\n1.516250e-05\n7.714512e-04\n1.190577e-01\n5.289520e-02\n3.330159e-04\n1.690781e-07\n1.402206e-06\n4.881958e-08\n...\n1.218608e-05\n2.880947e-03\n5.116140e-04\n1.090989e-01\n8.638866e-03\n3.532250e-02\n1.301925e-02\n5.380661e-01\n1.777594e-05\n4.036425e-07\n\n\n999\n4.129702e-14\n2.889617e-12\n2.798768e-13\n4.931771e-13\n2.598153e-12\n9.916586e-14\n3.006582e-13\n8.608723e-12\n2.060572e-12\n5.938183e-13\n...\n1.010096e-11\n1.221625e-11\n2.120370e-11\n2.549839e-13\n5.645862e-10\n5.007978e-11\n3.019627e-10\n1.544346e-11\n1.280130e-11\n9.915479e-01\n\n\n\n\n1000 rows × 1000 columns"
  },
  {
    "objectID": "posts/2023-01-15-cp.html#adaptive-prediction-sets",
    "href": "posts/2023-01-15-cp.html#adaptive-prediction-sets",
    "title": "Conformal Prediction",
    "section": "Adaptive Prediction Sets",
    "text": "Adaptive Prediction Sets\nIn comparison to the earlier method:\n\nThis method will have a larger predictive set size.\nMuch more adaptive (\\(i.e.\\) Larger set size for hard examples and small set size for easy examples).\nThe earlier method used the softmax value corresponding to only the true class of the output.\n\n\nImplementation\n\nif not os.path.exists(\"../data\"):\n    os.system(\"gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz\")\n    os.system(\"tar -xf ../data.tar.gz -C ../\")\n    os.system(\"rm ../data.tar.gz\")\nif not os.path.exists(\"../data/imagenet/human_readable_labels.json\"):\n    !wget -nv -O ../data/imagenet/human_readable_labels.json -L https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\ndata = np.load(\"../data/imagenet/imagenet-resnet152.npz\")\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nsmx = data[\"smx\"]\nlabels = data[\"labels\"].astype(int)\n\n\n# Problem setup\nn = 1000  # number of calibration points\nalpha = 0.1  # 1-alpha is the desired coverage\n\n\nidx = np.array([1] * n + [0] * (smx.shape[0] - n)) &gt; 0\nnp.random.shuffle(idx)\ncal_smx, val_smx = smx[idx, :], smx[~idx, :]\ncal_labels, val_labels = labels[idx], labels[~idx]\n\n\ncal_pi = cal_smx.argsort(1)[\n    :, ::-1\n]  ## sorting the cal_smx in descending order and storing the indices in cal_pi\ncal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(\n    axis=1\n)  ##  take the elements of 'cal_smx' corresponding to the indices in 'cal_pi' and take the cumulative sum along each row\ncal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n    range(n), cal_labels\n]  ##  take the elements of 'cal_srt' corresponding to the indices of the sorted 'cal_pi' and select the score corresponding to the 'cal_labels'\n\n\nexample = np.array(\n    [\n        [3, 4, 2, 1, 6],\n        [5, 4, 6, 7, 3],\n        [9, 5, 4, 3, 7],\n        [5, 4, 3, 7, 8],\n        [0, 3, 2, 1, 6],\n    ]\n)\nexample_labels = np.array([2, 3, 1, 4, 2])\n\nexample_pi = example.argsort(1)[:, ::-1]\nprint(example_pi)\n\nexample_srt = np.take_along_axis(example, example_pi, axis=1).cumsum(axis=1)\nprint(example_srt)\n\nexample_scores = np.take_along_axis(example_srt, example_pi.argsort(axis=1), axis=1)\nprint(example_scores)\n\nexample_scores[range(5), example_labels]\n\n[[4 1 0 2 3]\n [3 2 0 1 4]\n [0 4 1 2 3]\n [4 3 0 1 2]\n [4 1 2 3 0]]\n[[ 6 10 13 15 16]\n [ 7 13 18 22 25]\n [ 9 16 21 25 28]\n [ 8 15 20 24 27]\n [ 6  9 11 12 12]]\n[[13 10 15 16  6]\n [18 22 13  7 25]\n [ 9 21 25 28 16]\n [20 24 27 15  8]\n [12  9 11 12  6]]\n\n\narray([15,  7, 21,  8, 11])\n\n\n\n# example = np.array([[3,4,2,1,6],[5,4,6,7,3],[9,5,4,3,7],[5,4,3,7,8],[0,3,2,1,6]])\n# example_pi = example.sort()\n# print(example_pi)\n\n1. The softmax output corresponding to an image is sorted in decreasing order. Then we consider the \\(E_i\\) as the total mass of the softmax function for a particular label until we reach the true label.\n\n# Get the score quantile\nqhat = np.quantile(cal_scores, np.ceil((n + 1) * (1 - alpha)) / n)\nqhat\n\n0.9998794758319854\n\n\n2. Calculating the quantile value \\(\\hat{q}\\)\n\n# Deploy (output=list of length n, each element is tensor of classes)\nval_pi = val_smx.argsort(1)[:, ::-1]\nval_srt = np.take_along_axis(val_smx, val_pi, axis=1).cumsum(axis=1)\nprediction_sets = np.take_along_axis(val_srt &lt;= qhat, val_pi.argsort(axis=1), axis=1)\n\n\neg = np.array([7, 5, 4, 3, 2, 1, 4, 3, 6, 7])\neg_pi = eg.argsort()[::-1]\neg_srt = np.take_along_axis(eg, eg_pi, axis=0).cumsum()\np_set = np.take_along_axis(eg_srt &lt;= 0.8, eg_pi.argsort(), axis=0)\n\n\nimg = smx[1][0:10]\nimg_pi = img.argsort()[::-1]\nimg_srt = np.take_along_axis(img, img_pi, axis=0).cumsum()\nprediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\nprint(img_pi, img, img_srt, prediction_set)\n\n[0 1 4 3 5 2 6 7 8 9] [9.64658678e-01 1.35036071e-05 2.15132957e-07 1.69955081e-06\n 2.38454436e-06 1.64630808e-06 1.39490609e-07 2.11751061e-08\n 3.05739034e-09 4.08666018e-10] [0.96465868 0.96467218 0.96467457 0.96467627 0.96467791 0.96467813\n 0.96467827 0.96467829 0.96467829 0.96467829] [ True  True  True  True  True  True  True  True  True  True]\n\n\n\nwith open(\"../data/imagenet/human_readable_labels.json\") as f:\n    label_strings = np.array(json.load(f))\n\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nfor i in range(10):\n    rand_path = np.random.choice(example_paths)\n    img = imread(\"../data/imagenet/examples/\" + rand_path)\n    img_index = int(rand_path.split(\".\")[0])\n    img_pi = smx[img_index].argsort()[::-1]\n    img_srt = np.take_along_axis(smx[img_index], img_pi, axis=0).cumsum()\n    prediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\n    plt.figure()\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.show()\n    print(f\"The prediction set is: {list(label_strings[prediction_set])}\")\n\n\n\n\nThe prediction set is: ['prairie grouse', 'partridge', 'Afghan Hound', 'Otterhound', 'Bedlington Terrier', 'Kerry Blue Terrier', 'Giant Schnauzer', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'Irish Setter', 'Gordon Setter', 'Brittany', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Australian Kelpie', 'Komondor', 'Newfoundland', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'hyena', 'leopard', 'cheetah', 'brown bear', 'American black bear', 'mongoose', 'wild boar', 'bison', 'ram', 'llama', 'weasel', 'mink', 'guenon', 'baboon', 'honeycomb', 'jeep', 'wig', 'acorn', 'gyromitra']\nThe prediction set is: ['Otterhound', 'Border Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Miniature Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Australian Silky Terrier', 'German Shepherd Dog']\nThe prediction set is: ['goldfish', 'tiger shark', 'cock', 'house finch', 'agama', 'triceratops', 'ring-necked snake', 'sea snake', 'southern black widow', 'centipede', 'black grouse', 'prairie grouse', 'grey parrot', 'macaw', 'lorikeet', 'hornbill', 'hummingbird', 'toucan', 'flatworm', 'nematode', 'sea slug', 'fiddler crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'crane (bird)', 'oystercatcher', 'Bloodhound', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Labrador Retriever', 'English Setter', 'Gordon Setter', 'Brittany', 'Cocker Spaniels', 'Sussex Spaniel', 'Rottweiler', 'Greater Swiss Mountain Dog', 'Dalmatian', 'cougar', 'tiger', 'polar bear', 'mongoose', 'ladybug', 'stick insect', 'cockroach', 'leafhopper', 'damselfly', 'red admiral', 'gossamer-winged butterfly', 'sea cucumber', 'hamster', 'beaver', 'guinea pig', 'pig', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'skunk', 'badger', 'macaque', 'marmoset', 'red panda', 'eel', 'coho salmon', 'rock beauty', 'clownfish', 'sturgeon', 'garfish', 'lionfish', 'aircraft carrier', 'airliner', 'airship', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'waste container', 'assault rifle', 'backpack', 'balance beam', 'balloon', 'Band-Aid', 'baluster', 'barber chair', 'barbershop', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'swimming cap', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'beer bottle', 'binoculars', 'birdhouse', 'boathouse', 'bolo tie', 'bottle cap', 'breakwater', 'broom', 'buckle', 'bulletproof vest', 'high-speed train', 'taxicab', 'cannon', 'canoe', 'can opener', 'car mirror', 'carousel', 'tool kit', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'catamaran', 'CD player', 'mobile phone', 'chain', 'chain-link fence', 'chainsaw', 'movie theater', 'coffeemaker', 'computer keyboard', 'container ship', 'convertible', 'cowboy hat', 'crane (machine)', 'crash helmet', 'crate', 'crutch', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'digital clock', 'digital watch', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'drilling rig', 'dumbbell', 'Dutch oven', 'electric fan', 'electric locomotive', 'envelope', 'feather boa', 'fireboat', 'fire engine', 'football helmet', 'forklift', 'freight car', 'frying pan', 'garbage truck', 'gas mask', 'gas pump', 'go-kart', 'golf ball', 'golf cart', 'grille', 'grocery store', 'guillotine', 'barrette', 'half-track', 'hammer', 'hand-held computer', 'hard disk drive', 'harvester', 'hatchet', 'holster', 'honeycomb', 'hook', 'horizontal bar', 'horse-drawn vehicle', \"jack-o'-lantern\", 'jeep', 'jigsaw puzzle', 'pulled rickshaw', 'joystick', 'knot', 'lab coat', 'ladle', 'laptop computer', 'lawn mower', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'speaker', 'sawmill', 'magnetic compass', 'mailbox', 'manhole cover', 'match', 'maze', 'medicine chest', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'minivan', 'missile', 'mobile home', 'Model T', 'modem', 'monitor', 'moped', 'mortar', 'scooter', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'neck brace', 'odometer', 'oil filter', 'oscilloscope', 'bullock cart', 'oxygen mask', 'packet', 'paddle wheel', 'padlock', 'paintbrush', 'parachute', 'parking meter', 'passenger car', 'payphone', 'pencil case', 'pencil sharpener', 'Petri dish', 'photocopier', 'picket fence', 'pickup truck', 'pier', 'pill bottle', 'ping-pong ball', 'hand plane', 'plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'soda bottle', 'power drill', 'printer', 'projectile', 'projector', 'hockey puck', 'punching bag', 'race car', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rotisserie', 'eraser', 'ruler', 'running shoe', 'safe', 'sandal', 'weighing scale', 'school bus', 'scoreboard', 'CRT screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shopping cart', 'shovel', 'ski', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'solar thermal collector', 'space bar', 'space heater', 'space shuttle', 'motorboat', 'sports car', 'spotlight', 'steam locomotive', 'through arch bridge', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'suspension bridge', 'mop', 'swing', 'switch', 'syringe', 'tank', 'tape player', 'television', 'threshing machine', 'tile roof', 'toaster', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'tray', 'trimaran', 'trolleybus', 'tub', 'turnstile', 'typewriter keyboard', 'viaduct', 'wall clock', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water tower', 'whistle', 'wing', 'shipwreck', 'yurt', 'website', 'traffic sign', 'traffic light', 'dust jacket', 'ice pop', 'hot dog', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'hay', 'meatloaf', 'burrito', 'alp', 'bubble', 'cliff', 'coral reef', 'volcano', 'baseball player', 'scuba diver', 'rapeseed', 'corn', 'coral fungus', 'agaric', 'stinkhorn mushroom', 'ear']\nThe prediction set is: ['alligator lizard', 'trilobite', 'scorpion', 'tick', 'centipede', 'conch', 'snail', 'chiton', 'crayfish', 'hermit crab', 'isopod', 'ground beetle', 'weevil', 'cockroach', 'cicada', 'sea cucumber', 'armadillo', 'corn']\nThe prediction set is: ['tench', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'cock', 'hen', 'house finch', 'junco', 'indigo bunting', 'vulture', 'spotted salamander', 'loggerhead sea turtle', 'leatherback sea turtle', 'green iguana', 'desert grassland whiptail lizard', 'frilled-necked lizard', 'Gila monster', 'European green lizard', 'triceratops', 'eastern hog-nosed snake', 'kingsnake', 'vine snake', 'night snake', 'boa constrictor', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'tick', 'centipede', 'black grouse', 'ruffed grouse', 'prairie grouse', 'partridge', 'grey parrot', 'macaw', 'sulphur-crested cockatoo', 'coucal', 'hornbill', 'toucan', 'tusker', 'echidna', 'wombat', 'jellyfish', 'sea anemone', 'flatworm', 'conch', 'slug', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'red king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'American coot', 'dunlin', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Italian Greyhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'Gordon Setter', 'Brittany', 'Clumber Spaniel', 'English Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Bouvier des Flandres', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland', 'Pyrenean Mountain Dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Griffon Bruxellois', 'Pembroke Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog', 'Alaskan tundra wolf', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'leopard', 'American black bear', 'ground beetle', 'rhinoceros beetle', 'grasshopper', 'cricket', 'stick insect', 'cockroach', 'mantis', 'ringlet', 'monarch butterfly', 'starfish', 'sea urchin', 'cottontail rabbit', 'Angora rabbit', 'hamster', 'porcupine', 'beaver', 'common sorrel', 'zebra', 'hippopotamus', 'ox', 'water buffalo', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'armadillo', 'gibbon', 'Asian elephant', 'African bush elephant', 'snoek', 'eel', 'coho salmon', 'rock beauty', 'sturgeon', 'garfish', 'lionfish', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airship', 'altar', 'ambulance', 'analog clock', 'apron', 'waste container', 'assault rifle', 'backpack', 'bakery', 'balloon', 'Band-Aid', 'banjo', 'baluster', 'barbell', 'barber chair', 'barbershop', 'barometer', 'barrel', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military cap', 'beer bottle', 'beer glass', 'bell-cot', 'bib', 'tandem bicycle', 'bikini', 'ring binder', 'binoculars', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bow', 'bow tie', 'brass', 'bra', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'can opener', 'cardigan', 'car mirror', 'carousel', 'tool kit', 'carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'CD player', 'cello', 'mobile phone', 'chain', 'chain mail', 'chainsaw', 'chest', 'chiffonier', 'chime', 'china cabinet', 'Christmas stocking', 'church', 'movie theater', 'cleaver', 'cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'coil', 'combination lock', 'computer keyboard', 'confectionery store', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'crane (machine)', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fireboat', 'fire screen sheet', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'gas mask', 'gas pump', 'goblet', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'grille', 'grocery store', 'guillotine', 'barrette', 'hair spray', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'harmonica', 'harp', 'hatchet', 'holster', 'home theater', 'hook', 'hoop skirt', 'horizontal bar', 'hourglass', 'iPod', 'clothes iron', \"jack-o'-lantern\", 'jeans', 'T-shirt', 'pulled rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lens cap', 'library', 'limousine', 'ocean liner', 'lipstick', 'slip-on shoe', 'lotion', 'speaker', 'sawmill', 'magnetic compass', 'mail bag', 'mailbox', 'tights', 'tank suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'maze', 'medicine chest', 'megalith', 'microphone', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mobile home', 'Model T', 'modem', 'monastery', 'monitor', 'square academic cap', 'mosque', 'mosquito net', 'scooter', 'mountain bike', 'tent', 'mousetrap', 'moving van', 'muzzle', 'nail', 'neck brace', 'necklace', 'nipple', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'overskirt', 'oxygen mask', 'packet', 'paddle', 'padlock', 'paintbrush', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'passenger car', 'patio', 'payphone', 'pedestal', 'pencil case', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'picket fence', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pirate ship', 'pitcher', 'planetarium', 'plastic bag', 'plate rack', 'plunger', 'Polaroid camera', 'pole', 'poncho', 'billiard table', 'soda bottle', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'projector', 'punching bag', 'purse', 'quill', 'quilt', 'race car', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'rugby ball', 'ruler', 'running shoe', 'safe', 'safety pin', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'schooner', 'CRT screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe store', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski mask', 'sleeping bag', 'sliding door', 'slot machine', 'snorkel', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'space bar', 'space heater', 'space shuttle', 'spatula', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'through arch bridge', 'steel drum', 'stethoscope', 'scarf', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swimsuit', 'swing', 'switch', 'syringe', 'table lamp', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'front curtain', 'thimble', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'toy store', 'semi-trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'trombone', 'tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vase', 'vault', 'velvet', 'vending machine', 'vestment', 'violin', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'wing', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'yawl', 'yurt', 'website', 'comic book', 'crossword', 'traffic sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'consomme', 'hot pot', 'ice pop', 'pretzel', 'hot dog', 'cabbage', 'zucchini', 'butternut squash', 'cucumber', 'mushroom', 'orange', 'pineapple', 'banana', 'jackfruit', 'custard apple', 'chocolate syrup', 'dough', 'pizza', 'pot pie', 'burrito', 'red wine', 'espresso', 'cup', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'shoal', 'seashore', 'valley', 'volcano', 'baseball player', 'bridegroom', 'scuba diver', \"yellow lady's slipper\", 'corn', 'coral fungus', 'agaric', 'gyromitra', 'earth star', 'hen-of-the-woods', 'bolete', 'toilet paper']\nThe prediction set is: []\nThe prediction set is: ['tennis ball']\nThe prediction set is: []\nThe prediction set is: ['jay', 'tick', 'grey parrot', 'macaw', 'slug', 'common gallinule', 'Pekingese', 'Shih Tzu', 'Papillon', 'West Highland White Terrier', 'Shetland Sheepdog', 'collie', 'rhinoceros beetle', 'dragonfly', 'damselfly', 'sea urchin', 'accordion', 'analog clock', 'backpack', 'ballpoint pen', 'Band-Aid', 'bassoon', 'beaker', 'bib', 'ring binder', 'bolo tie', 'bookcase', 'bookstore', 'bow', 'bow tie', 'broom', 'bucket', 'buckle', 'cauldron', 'candle', 'can opener', 'tool kit', 'carton', 'cassette', 'cassette player', 'CD player', 'mobile phone', 'chain', 'chime', 'Christmas stocking', 'cloak', 'coffee mug', 'coil', 'computer keyboard', 'croquet ball', 'crutch', 'desk', 'digital clock', 'digital watch', 'dishcloth', 'drum', 'drumstick', 'electric guitar', 'envelope', 'face powder', 'feather boa', 'filing cabinet', 'flute', 'fountain pen', 'grand piano', 'barrette', 'hair spray', 'hammer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'hook', 'iPod', 'jeans', 'jigsaw puzzle', 'knot', 'lab coat', 'ladle', 'laptop computer', 'lens cap', 'paper knife', 'library', 'lighter', 'lipstick', 'lotion', 'loupe', 'mail bag', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'medicine chest', 'microphone', 'mitten', 'modem', 'square academic cap', 'computer mouse', 'nail', 'necklace', 'nipple', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'packet', 'paintbrush', 'pan flute', 'paper towel', 'pencil case', 'pencil sharpener', 'perfume', 'plectrum', 'pinwheel', 'plunger', 'pole', 'pot', 'printer', 'purse', 'quill', 'racket', 'radio', 'reel', 'remote control', 'revolver', 'rifle', 'eraser', 'ruler', 'safety pin', 'scabbard', 'screw', 'screwdriver', 'shovel', 'ski', 'slide rule', 'snorkel', 'sock', 'sombrero', 'soup bowl', 'spatula', 'spindle', 'steel drum', 'stethoscope', 'scarf', 'strainer', 'sunglass', 'sunglasses', 'sunscreen', 'mop', 'switch', 'syringe', 'tape player', 'tennis ball', 'thimble', 'torch', 'tray', 'tripod', 'umbrella', 'velvet', 'violin', 'wall clock', 'wallet', 'water bottle', 'whistle', 'wig', 'wooden spoon', 'wool', 'comic book', 'crossword', 'dust jacket', 'ice pop', 'toilet paper']\nThe prediction set is: ['grey parrot', 'macaw', 'lorikeet']"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Variationally Inferred Parameterization (VIP)\n\n\n\n\n\n\n\nBayesian Inference\n\n\nReparameterization\n\n\n\n\nTutorial for performing Variationally Inferred Parameterization (VIP) for Bayesian inference in Numpyro\n\n\n\n\n\n\nOct 18, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nLearning Loss for Active Learning\n\n\n\n\n\n\n\nActive Learning\n\n\nCNN\n\n\n\n\nExplaining and Implementing Learning Loss for Active Learning.\n\n\n\n\n\n\nOct 15, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nWindowed Non-Linear Reparamaterization\n\n\n\n\n\n\n\nMCMC\n\n\nBayesian\n\n\n\n\nExplaining and implementing Windowed Non-Linear Reparamaterization (WNR) for Bayesian inference.\n\n\n\n\n\n\nJul 21, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nConformal Prediction\n\n\n\n\n\n\n\nConformal Prediction\n\n\nML\n\n\n\n\nExplaining Conformal Prediction\n\n\n\n\n\n\nJan 15, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nLaplace Approximation\n\n\n\n\n\n\n\nLaplace Approximation\n\n\nML\n\n\n\n\nImplementing Laplace Approximation in JAX\n\n\n\n\n\n\nAug 12, 2022\n\n\nMadhav Kanda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html",
    "href": "posts/2023-07-21-wnr.html",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "",
    "text": "Expressing the model in terms of new variables defined by a bijective transformation of the original variables.\nFor example: One such reparameterization technique is Non-centering.\n\nConsider: z ~ \\(N(\\mu, \\sigma)\\) (centered parameterization)\nIntroducing z’ ~ \\(N(0,1)\\)\nNow, z = \\(\\mu + \\sigma z'\\) (Non-centered parameterization)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#what-is-reparameterisation",
    "href": "posts/2023-07-21-wnr.html#what-is-reparameterisation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "",
    "text": "Expressing the model in terms of new variables defined by a bijective transformation of the original variables.\nFor example: One such reparameterization technique is Non-centering.\n\nConsider: z ~ \\(N(\\mu, \\sigma)\\) (centered parameterization)\nIntroducing z’ ~ \\(N(0,1)\\)\nNow, z = \\(\\mu + \\sigma z'\\) (Non-centered parameterization)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#why-reparameterisation",
    "href": "posts/2023-07-21-wnr.html#why-reparameterisation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Why Reparameterisation?",
    "text": "Why Reparameterisation?\n\nAt times MCMC algorithms have trouble sampling from distributions.\nOne such example is Neal’s funnel in which due to strong non-linear dependence between latent variables.\nNon-centering the model removes this dependence, converting the funnel into a spherical Gaussian distribution.\nNealsFunnel(z, x)\n\n\\(z ∼ N (0, 3)\\) \\(\\quad\\)\n\\(x ∼ N (0, \\exp(z/2))\\)\n\n\n\n\n\nThus, centering is not the solution to all sampling related problems.\n\n## Slide not to be shown (imports)\nimport jax\nimport optuna\nimport numpyro\nimport jax.numpy as jnp\nfrom numpyro.infer import MCMC, NUTS\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\nfrom jax.scipy.optimize import minimize\n\nrng_key = jax.random.PRNGKey(0) \nfrom numpyro.infer.reparam import LocScaleReparam\nfrom numpyro.handlers import reparam\n\n\n## Slide not to be shown (Some functions)\ndef inference(model):\n    nuts_kernel = NUTS(model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    mcmc.run(rng_key, extra_fields=('potential_energy',))\n    return mcmc.get_samples()\n\ndef scatterplt(samples):\n    try:\n        plt.scatter(samples['theta_decentered'], samples['mu'])\n        plt.xlabel('theta_decentered')\n    except:\n        plt.scatter(samples['theta'], samples['mu'])\n        plt.xlabel('theta')\n    plt.ylabel('mu')\n    plt.show()\n\n\nLets’ see an example to find out why!\n\n\nsamples_normal = jax.random.normal(rng_key, shape=(10000,)) + 3  ## samples from a standard normal\n\ndef model():\n    mu = numpyro.sample('mu', dist.Normal(5, 1))                  ## mu ~ N(5,1)\n    theta = numpyro.sample('theta', dist.Normal(mu, 2))           ## theta ~ N(mu,2)\n    numpyro.sample('obs', dist.Normal(theta, 1), obs=samples_normal)     ## P(y|theta) ~ N(theta,1)\n\n\nncp_model = reparam(model, config={'theta': LocScaleReparam(0)})\nsamples_ncp = inference(ncp_model)\nscatterplt(samples_ncp)\n\nsample: 100%|██████████| 1500/1500 [00:05&lt;00:00, 285.49it/s, 427 steps of size 1.29e-02. acc. prob=0.91]\n\n\n\n\n\n\nncp_model = reparam(model, config={'theta': LocScaleReparam(1)})\nsamples_ncp = inference(ncp_model)\nscatterplt(samples_ncp)\n\nsample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 918.19it/s, 7 steps of size 7.40e-01. acc. prob=0.93] \n\n\n\n\n\n\nFrom the above experiment it is evident that the we don’t get the best results in non-centered parameterisation rather the best centeredness varies from model to model based on parameters."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#centeredness-vs-non-centeredness",
    "href": "posts/2023-07-21-wnr.html#centeredness-vs-non-centeredness",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Centeredness vs Non-centeredness",
    "text": "Centeredness vs Non-centeredness\n\nThe best parameterization for a given model may lie somewhere between centered and non centered representation.\nExisting solutions:\n\nVariationally Inferred Parameterization[1]\n\nNeuTra-lizing Bad Geometry in HMC[2]"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#problems-with-existing-solutions",
    "href": "posts/2023-07-21-wnr.html#problems-with-existing-solutions",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Problems with existing solutions",
    "text": "Problems with existing solutions\n\nRequires separate pre-processing steps apart from the regular warmup and sampling steps which increases the computation cost.\nNeed to tune the hyperparameters for the existing solutions to get good results"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#proposed-solution",
    "href": "posts/2023-07-21-wnr.html#proposed-solution",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Proposed Solution",
    "text": "Proposed Solution\n\nFinding the optimal centeredness during the succesive windows of warmup.\nLoss function for finding centeredness should be such that it takes the parameterized distribution as close as possible to Normal distribution."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#warmup-phase",
    "href": "posts/2023-07-21-wnr.html#warmup-phase",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Warmup Phase",
    "text": "Warmup Phase\n\nUsed for adaptation of inverse mass matrix (\\(M^{-1}\\)) and time step size (\\(\\Delta t\\)).\nConsist of three stages: \n\nInitial buffer (I): Time step adaptation (\\(\\Delta t\\))\nWindow buffer (II): Both Time step (\\(\\Delta t\\)) & Inverse mass matrix adaptation (\\(M^{-1}\\))\nTerm buffer (III): Final Time step adaptation (\\(\\Delta t\\))"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#modified-warmup-phase",
    "href": "posts/2023-07-21-wnr.html#modified-warmup-phase",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Modified Warmup Phase",
    "text": "Modified Warmup Phase\n\nUsed for adaptation of inverse mass matrix (\\(M^{-1}\\)), time step size (\\(\\Delta t\\)) and centeredness (\\(c\\)).\nInitial buffer and Term buffer remains the same.\nUsing the samples obtained after each window buffer, optimize the centeredness (\\(c\\)) so as to reduce the distance between the present reparameterized distribution and an independent normal distribution.\nFor each succesive window, reparameterize the model based on the optimal centeredness obtained and repeat the step for finding optimal centeredness."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#loss-function",
    "href": "posts/2023-07-21-wnr.html#loss-function",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Loss Function",
    "text": "Loss Function\n\n\\(\\Theta \\in R^{SXN}\\) \\(\\to\\) matrix of draws\n\\(\\Phi_{(c)}\\) \\(\\to\\) bijective family of reparameterisation (maps \\(P\\) to \\(P_{c}\\) and \\(\\Theta\\) to \\(\\Theta_{c})\\)\n\\(\\log p_{c}\\) = \\(\\log p\\ (inv\\ \\Phi_{(c)})\\) + \\(\\log |\\nabla_{\\theta} \\Phi_{(c)}|\\)\n\\(Q_{c}\\) = \\(N(mean(\\Theta_{c}), diag var(\\Theta_{c}))\\)\n\\(\\Phi_{(c^{*})} =\\ argmin_{\\Phi_{(c)}}KL(P_{(c)}||Q_{(c)})\\)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#experimentation",
    "href": "posts/2023-07-21-wnr.html#experimentation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Experimentation",
    "text": "Experimentation\n\nInitial experiments done on Eight School ’s model.\n\n\nJ = 8\ny = jnp.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\nsigma = jnp.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n\n\ndef eight_schools(J, sigma, y=None):\n    mu = numpyro.sample('mu', dist.Normal(2, 5))                 # mu ~ N(2,5)\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))              # tau ~ HalfCauchy(5)\n    with numpyro.plate('J', J):\n        theta = numpyro.sample('theta', dist.Normal(mu, tau))    # theta_i ~ N(mu,tau)\n        numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)  # P(y|theta_i) ~ N(theta_i, sigma)\n\n\nOn post warmup samples\n\n\n# Sampling\nnuts_kernel = NUTS(eight_schools)\nwarmup = MCMC(nuts_kernel, num_warmup=1000, num_samples=0)    # Collecting all warmup samples           \nwarmup.warmup(rng_key, J, sigma, y=y, collect_warmup=True)\nsamples = warmup.get_samples()\n\nwarmup: 100%|██████████| 1000/1000 [00:01&lt;00:00, 650.18it/s, 31 steps of size 1.67e-01. acc. prob=0.79]\n\n\n\nUsing the following continuous reparameterization family (from Variationally Inferred Parameterization (VIP)) :\n\nFor each \\(z_{i}\\) ~ \\(N(z_{i}|\\mu_{i}, \\sigma_{i})\\)\nDefine \\(z'_{i}\\) ~ $N({i} {i}, ^{_{i}}) $\nNow, \\(z_{i}\\) = \\(\\mu_{i} + \\sigma_{i}^{1-\\lambda_{i}}(z'_{i}-\\lambda_{i}\\mu_{i})\\)\n\n\n\n# For reparameterizing the samples and for creating the mulivariate normal distribution\ndef reparameterize_samples_dist(samples, c):\n    param_samples = samples['theta'].T\n    param_mean = samples['mu']\n    param_std = samples['tau']\n\n    new_param_samples = param_mean * c + (param_samples - param_mean) * param_std ** (c - 1)\n    \n    theta_mu = jnp.mean(new_param_samples, axis=1)\n    theta_std = jnp.std(new_param_samples, axis=1)\n    \n    mvn = dist.MultivariateNormal(loc=theta_mu, covariance_matrix=jnp.diag(theta_std**2))\n    \n    return new_param_samples, mvn, theta_mu, theta_std\n\n\n# For Computing the jacobian matrix for transformation\ndef log_jacobian(samples, c):\n    sigma = samples['tau']\n    logJ = jnp.sum(jnp.log(sigma) * (1 - c) * samples['theta'].shape[1])\n    logJ /= len(sigma)\n    return logJ\n\n\n# Function for computing KL divergence\ndef kl_value(centeredness):\n    centeredness = centeredness[0]\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, centeredness)\n    jacobian_log = log_jacobian(samples, centeredness)\n    kl = -mvn.log_prob(reparam_sample.T).mean() + jacobian_log    \n    return kl\n\n\nOptimization\n\nJax based BFGS Optimization\n\n\n# jax based BFGS optimization\nc_initial = jnp.array([0.1])\nres = minimize(kl_value, c_initial, method='BFGS')\n\nbest_c_jax = res.x[0]\nprint(best_c_jax)\n\n0.039444983\n\n\n\nOptuna based optimization\n\n\ndef kl_value_optuna(centeredness):\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, centeredness)\n    jacobian_log = log_jacobian(samples, centeredness)\n    kl = -mvn.log_prob(reparam_sample.T).mean() + jacobian_log    \n    return kl\n\n\n# Defining the function that needs to be minimized\ndef objective(trial):\n    x = trial.suggest_float('x', 0, 1) \n    loss = kl_value_optuna(x)\n    return loss\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nbest_trial = study.best_trial\nbest_c_optuna = best_trial.params['x']\n\n[I 2023-07-21 12:30:17,152] A new study created in memory with name: no-name-ce6ea8e4-5557-4576-b6d3-9b66b3f4e014\n[I 2023-07-21 12:30:17,522] Trial 0 finished with value: 21.478862762451172 and parameters: {'x': 0.17584669682579634}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,528] Trial 1 finished with value: 24.09613037109375 and parameters: {'x': 0.7817069280810928}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,533] Trial 2 finished with value: 22.280963897705078 and parameters: {'x': 0.30662176874135927}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,544] Trial 3 finished with value: 24.22972297668457 and parameters: {'x': 0.8316834716343527}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,549] Trial 4 finished with value: 24.571487426757812 and parameters: {'x': 0.9536417862630372}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,558] Trial 5 finished with value: 23.32807159423828 and parameters: {'x': 0.5278171991648825}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,565] Trial 6 finished with value: 23.81903839111328 and parameters: {'x': 0.6804841397939312}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,573] Trial 7 finished with value: 23.242023468017578 and parameters: {'x': 0.5051001539715525}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,578] Trial 8 finished with value: 23.161338806152344 and parameters: {'x': 0.4847693487670296}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,584] Trial 9 finished with value: 24.19658851623535 and parameters: {'x': 0.8193228786412999}. Best is trial 0 with value: 21.478862762451172.\n[I 2023-07-21 12:30:17,598] Trial 10 finished with value: 20.99654197692871 and parameters: {'x': 0.01012783576019044}. Best is trial 10 with value: 20.99654197692871.\n[I 2023-07-21 12:30:17,609] Trial 11 finished with value: 20.97226333618164 and parameters: {'x': 0.021482897698871527}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,621] Trial 12 finished with value: 21.007823944091797 and parameters: {'x': 0.006225750391182828}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,633] Trial 13 finished with value: 20.98029899597168 and parameters: {'x': 0.017028345744416362}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,643] Trial 14 finished with value: 21.32019805908203 and parameters: {'x': 0.14837261040270638}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,656] Trial 15 finished with value: 21.357559204101562 and parameters: {'x': 0.15505667225509345}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,664] Trial 16 finished with value: 22.35342788696289 and parameters: {'x': 0.3191617380266179}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,676] Trial 17 finished with value: 20.983047485351562 and parameters: {'x': 0.01570799167586904}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,690] Trial 18 finished with value: 22.05672264099121 and parameters: {'x': 0.2691589238654932}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,699] Trial 19 finished with value: 21.073299407958984 and parameters: {'x': 0.09619890391216168}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,709] Trial 20 finished with value: 21.652862548828125 and parameters: {'x': 0.20427583423078496}. Best is trial 11 with value: 20.97226333618164.\n[I 2023-07-21 12:30:17,720] Trial 21 finished with value: 20.96302032470703 and parameters: {'x': 0.050305196738679905}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,735] Trial 22 finished with value: 21.06765365600586 and parameters: {'x': 0.09466542797673394}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,762] Trial 23 finished with value: 21.016345977783203 and parameters: {'x': 0.07878693559027633}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,773] Trial 24 finished with value: 21.854490280151367 and parameters: {'x': 0.23652560404399758}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,793] Trial 25 finished with value: 20.981300354003906 and parameters: {'x': 0.06372276076878293}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,812] Trial 26 finished with value: 21.158065795898438 and parameters: {'x': 0.1165211765395495}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,823] Trial 27 finished with value: 21.760175704956055 and parameters: {'x': 0.22146025668389857}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,839] Trial 28 finished with value: 20.983776092529297 and parameters: {'x': 0.015370600866575199}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,848] Trial 29 finished with value: 21.641178131103516 and parameters: {'x': 0.20239517028292664}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,861] Trial 30 finished with value: 21.31525421142578 and parameters: {'x': 0.1474752271207078}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,883] Trial 31 finished with value: 20.98286247253418 and parameters: {'x': 0.06455904892080892}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,898] Trial 32 finished with value: 20.981739044189453 and parameters: {'x': 0.0639595469926666}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,910] Trial 33 finished with value: 21.024593353271484 and parameters: {'x': 0.0012143679706077826}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,924] Trial 34 finished with value: 21.2662353515625 and parameters: {'x': 0.13837976610354358}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,940] Trial 35 finished with value: 20.981460571289062 and parameters: {'x': 0.0638092573044853}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,949] Trial 36 finished with value: 22.237255096435547 and parameters: {'x': 0.2991726535422863}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,968] Trial 37 finished with value: 21.532812118530273 and parameters: {'x': 0.184783432933084}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,978] Trial 38 finished with value: 20.97808074951172 and parameters: {'x': 0.06190912736734475}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:17,991] Trial 39 finished with value: 21.184711456298828 and parameters: {'x': 0.12220730824639862}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:18,006] Trial 40 finished with value: 22.582069396972656 and parameters: {'x': 0.3606091195943603}. Best is trial 21 with value: 20.96302032470703.\n[I 2023-07-21 12:30:18,015] Trial 41 finished with value: 20.962871551513672 and parameters: {'x': 0.050133846093693396}. Best is trial 41 with value: 20.962871551513672.\n[I 2023-07-21 12:30:18,030] Trial 42 finished with value: 20.960521697998047 and parameters: {'x': 0.04691414895707719}. Best is trial 42 with value: 20.960521697998047.\n[I 2023-07-21 12:30:18,040] Trial 43 finished with value: 21.464412689208984 and parameters: {'x': 0.17342687997114256}. Best is trial 42 with value: 20.960521697998047.\n[I 2023-07-21 12:30:18,048] Trial 44 finished with value: 21.101139068603516 and parameters: {'x': 0.10335950307395814}. Best is trial 42 with value: 20.960521697998047.\n[I 2023-07-21 12:30:18,060] Trial 45 finished with value: 20.973033905029297 and parameters: {'x': 0.05876309223154809}. Best is trial 42 with value: 20.960521697998047.\n[I 2023-07-21 12:30:18,073] Trial 46 finished with value: 20.958948135375977 and parameters: {'x': 0.043590468170786406}. Best is trial 46 with value: 20.958948135375977.\n[I 2023-07-21 12:30:18,081] Trial 47 finished with value: 21.227359771728516 and parameters: {'x': 0.1308621815383041}. Best is trial 46 with value: 20.958948135375977.\n[I 2023-07-21 12:30:18,095] Trial 48 finished with value: 20.95848846435547 and parameters: {'x': 0.037008621383143876}. Best is trial 48 with value: 20.95848846435547.\n[I 2023-07-21 12:30:18,108] Trial 49 finished with value: 21.464881896972656 and parameters: {'x': 0.17350566862746925}. Best is trial 48 with value: 20.95848846435547.\n[I 2023-07-21 12:30:18,124] Trial 50 finished with value: 20.9591064453125 and parameters: {'x': 0.04403677732959216}. Best is trial 48 with value: 20.95848846435547.\n[I 2023-07-21 12:30:18,139] Trial 51 finished with value: 20.958484649658203 and parameters: {'x': 0.037033242388128024}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,150] Trial 52 finished with value: 21.0146484375 and parameters: {'x': 0.004091265726784574}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,166] Trial 53 finished with value: 21.19355010986328 and parameters: {'x': 0.12404204653819792}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,178] Trial 54 finished with value: 20.958499908447266 and parameters: {'x': 0.03695412586533163}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,192] Trial 55 finished with value: 21.081878662109375 and parameters: {'x': 0.09847162565512366}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,208] Trial 56 finished with value: 20.95962142944336 and parameters: {'x': 0.03372279503215306}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,217] Trial 57 finished with value: 21.075090408325195 and parameters: {'x': 0.09667862167771879}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,230] Trial 58 finished with value: 21.01943016052246 and parameters: {'x': 0.002676824165289346}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,243] Trial 59 finished with value: 21.409378051757812 and parameters: {'x': 0.16408245708056968}. Best is trial 51 with value: 20.958484649658203.\n[I 2023-07-21 12:30:18,257] Trial 60 finished with value: 20.958477020263672 and parameters: {'x': 0.04184774669269055}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,273] Trial 61 finished with value: 20.958759307861328 and parameters: {'x': 0.04299274311023122}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,286] Trial 62 finished with value: 20.963279724121094 and parameters: {'x': 0.028588220146174667}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,308] Trial 63 finished with value: 21.21792984008789 and parameters: {'x': 0.12898901933473095}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,322] Trial 64 finished with value: 21.052663803100586 and parameters: {'x': 0.09043107808448948}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,339] Trial 65 finished with value: 20.961326599121094 and parameters: {'x': 0.030925393866438965}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,349] Trial 66 finished with value: 21.047931671142578 and parameters: {'x': 0.08903778837552029}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,366] Trial 67 finished with value: 20.959081649780273 and parameters: {'x': 0.03496986332824197}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,382] Trial 68 finished with value: 21.02678680419922 and parameters: {'x': 0.08239338012971459}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,393] Trial 69 finished with value: 20.958797454833984 and parameters: {'x': 0.03579992403933894}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,405] Trial 70 finished with value: 21.020240783691406 and parameters: {'x': 0.0024429664456278935}. Best is trial 60 with value: 20.958477020263672.\n[I 2023-07-21 12:30:18,423] Trial 71 finished with value: 20.958251953125 and parameters: {'x': 0.03889943726397521}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,433] Trial 72 finished with value: 21.33806610107422 and parameters: {'x': 0.15159023058486498}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,448] Trial 73 finished with value: 21.003616333007812 and parameters: {'x': 0.07397970858308872}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,462] Trial 74 finished with value: 20.959957122802734 and parameters: {'x': 0.033072453153977874}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,480] Trial 75 finished with value: 21.172382354736328 and parameters: {'x': 0.1196073051785179}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,492] Trial 76 finished with value: 21.009960174560547 and parameters: {'x': 0.07644060332194952}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,500] Trial 77 finished with value: 21.13602066040039 and parameters: {'x': 0.11161294201348658}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,512] Trial 78 finished with value: 20.95917510986328 and parameters: {'x': 0.034730924034632855}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,524] Trial 79 finished with value: 21.633689880371094 and parameters: {'x': 0.20118860863334553}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,535] Trial 80 finished with value: 21.027835845947266 and parameters: {'x': 0.000326094284544165}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,545] Trial 81 finished with value: 20.958572387695312 and parameters: {'x': 0.03662093145763981}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,559] Trial 82 finished with value: 20.968891143798828 and parameters: {'x': 0.05577215986849743}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,574] Trial 83 finished with value: 20.997417449951172 and parameters: {'x': 0.07142156091329199}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,582] Trial 84 finished with value: 20.967121124267578 and parameters: {'x': 0.025091432153655814}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,596] Trial 85 finished with value: 21.060935974121094 and parameters: {'x': 0.09279958012809214}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,611] Trial 86 finished with value: 21.30889892578125 and parameters: {'x': 0.14631639736301744}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,625] Trial 87 finished with value: 20.963882446289062 and parameters: {'x': 0.027963839404734812}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,691] Trial 88 finished with value: 20.964874267578125 and parameters: {'x': 0.05227439563750636}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,714] Trial 89 finished with value: 21.124919891357422 and parameters: {'x': 0.10905847650264305}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,728] Trial 90 finished with value: 20.96737289428711 and parameters: {'x': 0.05453974509843049}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,742] Trial 91 finished with value: 20.96955680847168 and parameters: {'x': 0.023275832726661432}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,797] Trial 92 finished with value: 21.005821228027344 and parameters: {'x': 0.07485153652705367}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,829] Trial 93 finished with value: 20.958648681640625 and parameters: {'x': 0.04258364384159399}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,843] Trial 94 finished with value: 21.148571014404297 and parameters: {'x': 0.11443265557035082}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,853] Trial 95 finished with value: 20.959461212158203 and parameters: {'x': 0.04489944712956695}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,864] Trial 96 finished with value: 21.022884368896484 and parameters: {'x': 0.001691175673073697}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,875] Trial 97 finished with value: 21.254417419433594 and parameters: {'x': 0.13612660260735093}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,892] Trial 98 finished with value: 21.00398063659668 and parameters: {'x': 0.07412590881408465}. Best is trial 71 with value: 20.958251953125.\n[I 2023-07-21 12:30:18,905] Trial 99 finished with value: 20.972087860107422 and parameters: {'x': 0.02159164323540737}. Best is trial 71 with value: 20.958251953125.\n\n\n\ndf_study = study.trials_dataframe()\nplt.plot(df_study['params_x'], df_study['value'], 'o', alpha=0.5)\nplt.xlabel(\"Centeredness\", fontsize = 12)\nplt.ylabel(\"Loss\", fontsize = 12)\nplt.show()\n\n\n\n\n\nEvident from above that Optuna keeps decreasing the search space after each iteration to get close to the best centeredness value.\n\n\n# Function for creating the scatter plot\ndef scatterplt_2(samples, c):\n    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    axes = axes.flatten()\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, c)\n    reparam_sample = reparam_sample if reparam else samples['theta'].T\n    axes[0].scatter(reparam_sample[0],jnp.log(samples['tau']))\n    axes[0].set_xlabel('theta0_reparam')\n    axes[0].set_ylabel('log_std')\n\n    axes[1].scatter(samples['theta'].T[0],jnp.log(samples['tau']))\n    axes[1].set_xlabel('theta0')\n\n\nResults obtained from initial experiments\n\n\nscatterplt_2(samples, c = best_c_jax)     ## Using the best centeredness computed"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#implementation",
    "href": "posts/2023-07-21-wnr.html#implementation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Implementation",
    "text": "Implementation\n\nModified BlackJax sampler code to incorporate the proposed algorithm.\nInference time up from 2 seconds to 20 seconds.\nTo evaluate the results used a model whose true centeredness already known:\n\n\\(\\mu\\) ~ \\(N(0,1)\\)\n\\(\\log \\sigma\\) ~ \\(N(0,1)\\)\n\\(x\\) ~ \\(N((1-c_{i}) \\mu, \\sigma^{(1-c_{i})})\\)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#results",
    "href": "posts/2023-07-21-wnr.html#results",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Results",
    "text": "Results\n\n\n\n\nLets’ compare the sampling efficieny\n\n\\[\\begin{array}{c} \\hline\nParameter & ESS/ \\nabla (vip) & ESS/ \\nabla (our) \\\\ \\hline\n\\mu & 9.27*10^{-4} & 5.54*10^{-4} \\\\\n\\tau & 4.22*10^{-5} & \\textbf{$5.07*10^{-4}$} \\\\\n\\theta & 9.73*10^{-4} & \\textbf{$1.66*10^{-3}$} \\\\ \\hline\n\\end{array}\\]\n\nOur ESS/ \\(\\nabla\\) results better then the ones produced by existing methods."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#scalability",
    "href": "posts/2023-07-21-wnr.html#scalability",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Scalability",
    "text": "Scalability"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#future-work",
    "href": "posts/2023-07-21-wnr.html#future-work",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Future Work",
    "text": "Future Work\n\nImplementing a stochastic gradient based approach for finding the optimal centeredness\nTrying different parameterization families."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#references",
    "href": "posts/2023-07-21-wnr.html#references",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "References",
    "text": "References\n[1] M.I.Gorinova,D.Moore,andM.D.Hoffman,“Automaticreparameterisation of probabilistic programs,” 2019.\n[2] M.Hoffman,P.Sountsov,etal.,“Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport,” 2019.\n[3] https://mc-stan.org/docs/reference-manual/hmc-algorithm-parameters.html\n[4] Gabrié, Marylou, Grant M. Rotskoff, and Eric Vanden-Eijnden. 2021. “Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods.” arXiv. https://doi.org/10.48550/arXiv.2107.08001.\n[5] Hoﬀman, Matthew D, and Andrew Gelman. n.d. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo,” 31.\n[6] https://num.pyro.ai/en/0.7.2/examples/funnel.html"
  },
  {
    "objectID": "posts/2023-10-15-llal.html",
    "href": "posts/2023-10-15-llal.html",
    "title": "Learning Loss for Active Learning",
    "section": "",
    "text": "Experimental result of semi-supervised learning suggests that higher portion of annotated data ensures better performance.\nBut, annotation is expensive and time-consuming. Thus, limiting the budget.\nActive learning is a solution to this problem as it selects data points that are most informative and asks for annotation of these data points only."
  },
  {
    "objectID": "posts/2023-10-15-llal.html#why-active-learning",
    "href": "posts/2023-10-15-llal.html#why-active-learning",
    "title": "Learning Loss for Active Learning",
    "section": "",
    "text": "Experimental result of semi-supervised learning suggests that higher portion of annotated data ensures better performance.\nBut, annotation is expensive and time-consuming. Thus, limiting the budget.\nActive learning is a solution to this problem as it selects data points that are most informative and asks for annotation of these data points only."
  },
  {
    "objectID": "posts/2023-10-15-llal.html#what-is-active-learning",
    "href": "posts/2023-10-15-llal.html#what-is-active-learning",
    "title": "Learning Loss for Active Learning",
    "section": "What is Active Learning?",
    "text": "What is Active Learning?\n\nModel actively selects data points that the model is uncertain about. Using the selected data points, the model is retrained.\nEg: In a classification task, the model selects data points that are close to the decision boundary. Later, the model is retrained using the selected data points.\nLets’ demonstrate this using CIFAR-10 example.\n\n\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# Torchvison\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\n\nimport os\n\nos.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices\"  ## setting the environment\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  ## using GPU core 1\n\n\nModel Architecture\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out1 = self.layer1(out)\n        out2 = self.layer2(out1)\n        out3 = self.layer3(out2)\n        out4 = self.layer4(out3)\n        out = F.avg_pool2d(out4, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out, [out1, out2, out3, out4]\n\n\ndef ResNet18(num_classes = 10):\n    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n\n\n\nData Transformation (CIFAR-10)\n\ntrain_transform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomCrop(size=32, padding=4),\n    T.ToTensor(),\n    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n])\n\ntest_transform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n])\n\n\ncifar10_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)\ncifar10_unlabeled   = CIFAR10('../cifar10', train=True, download=True, transform=test_transform)\ncifar10_test  = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nclass SubsetSequentialSampler(torch.utils.data.Sampler):\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in range(len(self.indices)))\n    \n    def __len__(self):\n        return len(self.indices)\n\n\nVisualising the dataset\n\n# plot some sample images from the dataset\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(10, 6))\nfor i, ax in enumerate(axes.flat):\n    img, label = cifar10_train[i]\n    img = np.clip(img.permute(1, 2, 0), 0, 1) # clip the input image for the valid range for imshow\n    ax.imshow(img)\n    ax.set_title(f\"Label: {label}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# cifar_10_labels = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer',\n# 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n\n\n\n\n\n\nTraining & Testing function\n\ndef train(model, criterion, optimizer, dataloaders, num_epochs, scheduler):\n    model.train()\n    train_accuracy = []\n    for epoch in range(num_epochs):\n        scheduler.step()\n\n        train_total = 0\n        train_correct = 0\n\n        for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n            inputs = data[0].cuda()\n            labels = data[1].cuda()\n            optimizer.zero_grad()\n\n            scores, features = model(inputs)\n            target_loss = criterion(scores, labels)\n            _, preds = torch.max(scores.data, 1)\n            train_total += labels.size(0)\n            train_correct += (preds == labels).sum().item()\n\n            loss = torch.sum(target_loss) / target_loss.size(0)\n            loss.backward()\n            optimizer.step()\n\n        train_acc = 100* train_correct / train_total\n        train_accuracy.append(train_acc)\n        \n        # Evaluating test loss\n        total = 0\n        correct = 0\n        model.eval()\n        for data in dataloaders['test']:\n            inputs = data[0].cuda()\n            labels = data[1].cuda()\n\n            scores, features = model(inputs)\n            _, preds = torch.max(scores.data, 1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n\n        test_acc = 100* correct / total\n        \n    return train_accuracy, test_acc\n\n\n\nUsing Entropy as the acquisition function for active learning\n\ndef entropy(model, unlabeled_loader):\n    model.eval()\n    entropies = []\n    with torch.no_grad():\n        for data in unlabeled_loader:\n            inputs = data[0].cuda()\n            scores, _ = model(inputs)\n            entropy = -torch.sum(F.softmax(scores, dim=1) * F.log_softmax(scores, dim=1), dim=1)\n            entropies.append(entropy.cpu().numpy())\n    return np.concatenate(entropies)\n\n\n## CONFIG\n\nNUM_TRAIN = 50000\nBATCH = 128\nADDENDUM = 1000\nSUBSET = 10000\nMOMENTUM = 0.9\nWDECAY = 5e-4\nLR = 0.01\nCYCLES = 5\nMILESTONES = [160]\nEPOCH = 40\n\n\n\nActive Learning Loop\n\nindices = list(range(NUM_TRAIN))\nrandom.shuffle(indices)\nlabeled_set = indices[:ADDENDUM]\nunlabeled_set = indices[ADDENDUM:]\n\ntest_accuracy_list = []\ntrain_accuracy_list = []\n\ntrain_loader = DataLoader(cifar10_train, batch_size=BATCH, sampler=SubsetRandomSampler(labeled_set), pin_memory=True)\ntest_loader  = DataLoader(cifar10_test, batch_size=BATCH)\ndataloaders  = {'train': train_loader, 'test': test_loader}\n\n# Model\nresnet18    = ResNet18(num_classes=10).cuda()\ntorch.backends.cudnn.benchmark = False\n\n# Active learning cycles\nfor cycle in range(CYCLES):\n    criterion = nn.CrossEntropyLoss(reduction='none')\n    optimizer = optim.SGD(resnet18.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WDECAY)\n    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES)\n\n    # Training and test\n    train_acc, test_acc = train(resnet18, criterion, optimizer, dataloaders, EPOCH, scheduler)\n    test_accuracy_list.append(test_acc)\n    train_accuracy_list.append(train_acc)\n    print('Cycle {}/{} || Label set size {}: Test Accuracy {} '.format(cycle+1, CYCLES, len(labeled_set), test_acc))\n\n    random.shuffle(unlabeled_set)\n    subset = unlabeled_set[:SUBSET]\n\n    # Create unlabeled dataloader for the unlabeled subset\n    unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH, sampler=SubsetSequentialSampler(subset), pin_memory=True)\n\n    # Measure uncertainty of each data points in the subset\n    uncertainty = entropy(resnet18, unlabeled_loader)\n\n    # Index in ascending order\n    arg = np.argsort(uncertainty)\n    \n    # Update the labeled dataset and the unlabeled dataset, respectively\n    labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n    unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n\n    # Create a new dataloader for the updated labeled dataset\n    dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH, sampler=SubsetRandomSampler(labeled_set), \n                                        pin_memory=True)\n\n/home/project_3/anaconda3/envs/gan/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n                                                                                                                                                                                                                                         \n\n\nCycle 1/5 || Label set size 1000: Test Accuracy 39.01 \nCycle 2/5 || Label set size 2000: Test Accuracy 51.99 \nCycle 3/5 || Label set size 3000: Test Accuracy 57.39 \nCycle 4/5 || Label set size 4000: Test Accuracy 62.73 \nCycle 5/5 || Label set size 5000: Test Accuracy 65.35 \n\n\n\nactive_learning_loop = [1, 2, 3, 4, 5]  # Active learning loop numbers \n\nplt.plot(active_learning_loop, test_accuracy_list, marker='o', linestyle='-')\n\nplt.xlabel('Active Learning Loop')\nplt.ylabel('Test Accuracy')\nplt.title('Test Accuracy Over Active Learning Loop')\nplt.show()"
  },
  {
    "objectID": "posts/2023-10-15-llal.html#learning-to-loss-active-learning",
    "href": "posts/2023-10-15-llal.html#learning-to-loss-active-learning",
    "title": "Learning Loss for Active Learning",
    "section": "Learning to Loss Active Learning",
    "text": "Learning to Loss Active Learning\n\nIn this active learning a loss prediction module is attached to the target network to predict the loss.\nThis module is then used to suggest the data that the target module is likely to produce a wrong prediction on.\n\n\nLoss for the loss prediction network\n\\[\\begin{array}{r}\nL_{\\text {loss }}\\left(\\hat{l p}, l^p\\right)=\n\\max \\left(0,-\\mathbb{1}\\left(l_i, l_j\\right) \\cdot\\left(\\hat{l_i}-\\hat{l_j}\\right)+\\xi\\right)\n\\text { s.t. } \\mathbb{1}\\left(l_i, l_j\\right)= \\begin{cases}+1, & \\text { if } l_i&gt;l_j \\\\\n-1, & \\text { otherwise }\\end{cases}\n\\end{array}\\]\nWe learn the loss prediction module by considering the difference between a pair of loss predictions, which completely make the loss prediction module discard the overall scale changes\n\ndef LossPredLoss(input, target, margin=1.0):\n    assert len(input) % 2 == 0, 'the batch size is not even.'\n    assert input.shape == input.flip(0).shape\n    \n    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n    target = (target - target.flip(0))[:len(target)//2]\n    target = target.detach()\n\n    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n    loss = torch.sum(torch.clamp(margin - one * input, min=0))\n    loss = loss / input.size(0) # Note that the size of input is already halved\n\n    return loss\n\n\n\nLossNet\n\nLossNet takes in as input the output from the intermediate layer of the target network to predict the loss.\n\n\n\nimage.png\n\n\n\n\nclass LossNet(nn.Module):\n    def __init__(self, feature_sizes=[32, 16, 8, 4], num_channels=[64, 128, 256, 512], interm_dim=128):\n        super(LossNet, self).__init__()\n        \n        self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n        self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n        self.GAP3 = nn.AvgPool2d(feature_sizes[2])\n        self.GAP4 = nn.AvgPool2d(feature_sizes[3])\n\n        self.FC1 = nn.Linear(num_channels[0], interm_dim)\n        self.FC2 = nn.Linear(num_channels[1], interm_dim)\n        self.FC3 = nn.Linear(num_channels[2], interm_dim)\n        self.FC4 = nn.Linear(num_channels[3], interm_dim)\n\n        self.linear = nn.Linear(4 * interm_dim, 1)\n    \n    def forward(self, features):\n        out1 = self.GAP1(features[0])\n        out1 = out1.view(out1.size(0), -1)\n        out1 = F.relu(self.FC1(out1))\n\n        out2 = self.GAP2(features[1])\n        out2 = out2.view(out2.size(0), -1)\n        out2 = F.relu(self.FC2(out2))\n\n        out3 = self.GAP3(features[2])\n        out3 = out3.view(out3.size(0), -1)\n        out3 = F.relu(self.FC3(out3))\n\n        out4 = self.GAP4(features[3])\n        out4 = out4.view(out4.size(0), -1)\n        out4 = F.relu(self.FC4(out4))\n\n        out = self.linear(torch.cat((out1, out2, out3, out4), 1))\n        return out\n\n\n# Function for evaluating the test accuracy\ndef test(models, dataloaders, mode='val'):\n    assert mode == 'val' or mode == 'test'\n    models['backbone'].eval()\n    models['module'].eval()\n\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for (inputs, labels) in dataloaders[mode]:\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n\n            scores, _ = models['backbone'](inputs)\n            _, preds = torch.max(scores.data, 1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n    \n    return 100 * correct / total\n\n\n# Training function for both the target model and the loss prediction module\ndef train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss):\n    for epoch in range(num_epochs):\n        schedulers['backbone'].step()\n        schedulers['module'].step()\n\n        models['backbone'].train()\n        models['module'].train()\n\n        for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n            inputs = data[0].cuda()\n            labels = data[1].cuda()\n\n            optimizers['backbone'].zero_grad()\n            optimizers['module'].zero_grad()\n\n            scores, features = models['backbone'](inputs)\n            target_loss = criterion(scores, labels)\n\n            # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n            if epoch &gt; epoch_loss:\n                features[0] = features[0].detach()\n                features[1] = features[1].detach()\n                features[2] = features[2].detach()\n                features[3] = features[3].detach()\n            pred_loss = models['module'](features)\n            pred_loss = pred_loss.view(pred_loss.size(0))\n\n            # Combined loss function\n            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n            m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n            loss            = m_backbone_loss + WEIGHT * m_module_loss\n\n            loss.backward()\n            optimizers['backbone'].step()\n            optimizers['module'].step()\n\n\n# Selecting the acquisition points based on the predictions of the loss prediction module\ndef get_uncertainty(models, unlabeled_loader):\n    models['backbone'].eval()\n    models['module'].eval()\n    uncertainty = torch.tensor([]).cuda()\n\n    with torch.no_grad():\n        for (inputs, labels) in unlabeled_loader:\n            inputs = inputs.cuda()\n            scores, features = models['backbone'](inputs)\n            pred_loss = models['module'](features) \n            pred_loss = pred_loss.view(pred_loss.size(0))\n\n            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n    return uncertainty.cpu()\n\n\nEPOCHL = 120       # After EPOCHL, stop the gradient from the loss prediction module propagated to the target model.\nMARGIN = 1.0 \nWEIGHT = 1.0\n\n\nindices = list(range(NUM_TRAIN))\nrandom.shuffle(indices)\nlabeled_set = indices[:ADDENDUM]\nunlabeled_set = indices[ADDENDUM:]\n\ntest_accuracy_lal = []\n\ntrain_loader = DataLoader(cifar10_train, batch_size=BATCH, sampler=SubsetRandomSampler(labeled_set), pin_memory=True)\ntest_loader  = DataLoader(cifar10_test, batch_size=BATCH)\ndataloaders  = {'train': train_loader, 'test': test_loader}\n\n# Model\nresnet18    = ResNet18(num_classes=10).cuda()\nloss_module = LossNet().cuda()\nmodels      = {'backbone': resnet18, 'module': loss_module}\ntorch.backends.cudnn.benchmark = False\n\n# Active learning cycles\nfor cycle in range(CYCLES):\n    # Loss, criterion and scheduler (re)initialization\n    criterion      = nn.CrossEntropyLoss(reduction='none')\n    optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n                            momentum=MOMENTUM, weight_decay=WDECAY)\n    optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n                            momentum=MOMENTUM, weight_decay=WDECAY)\n    sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n    sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=MILESTONES)\n\n    optimizers = {'backbone': optim_backbone, 'module': optim_module}\n    schedulers = {'backbone': sched_backbone, 'module': sched_module}\n\n    # Training and test\n    train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL)\n    acc = test(models, dataloaders, mode='test')\n    test_accuracy_lal.append(acc)\n    print('Cycle {}/{} || Label set size {}: Test acc {}'.format(cycle+1, CYCLES, len(labeled_set), acc))\n\n    # Randomly sample 10000 unlabeled data points\n    random.shuffle(unlabeled_set)\n    subset = unlabeled_set[:SUBSET]\n\n    # Create unlabeled dataloader for the unlabeled subset\n    unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH, sampler=SubsetSequentialSampler(subset), pin_memory=True)\n\n    # Measure uncertainty of each data points in the subset\n    uncertainty = get_uncertainty(models, unlabeled_loader)\n\n    # Index in ascending order\n    arg = np.argsort(uncertainty)\n    \n    # Update the labeled dataset and the unlabeled dataset, respectively\n    labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n    unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n\n    # Create a new dataloader for the updated labeled dataset\n    dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH, sampler=SubsetRandomSampler(labeled_set), pin_memory=True)\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]                                                                                                                                                                                                                                         \n\n\nCycle 1/5 || Label set size 1000: Test acc 43.6\nCycle 2/5 || Label set size 2000: Test acc 50.23\nCycle 3/5 || Label set size 3000: Test acc 57.39\nCycle 4/5 || Label set size 4000: Test acc 63.17\nCycle 5/5 || Label set size 5000: Test acc 69.3\n\n\n\nactive_learning_loop = [1, 2, 3, 4, 5]  # Active learning loop numbers \nplt.plot(active_learning_loop, test_accuracy_lal, marker='o', linestyle='-')\n\nplt.xlabel('Active Learning Loop')\nplt.ylabel('Test Accuracy')\nplt.title('Test Accuracy Over Active Learning Loop')\nplt.show()"
  },
  {
    "objectID": "posts/2023-10-15-llal.html#references",
    "href": "posts/2023-10-15-llal.html#references",
    "title": "Learning Loss for Active Learning",
    "section": "References:",
    "text": "References:\n\nhttps://arxiv.org/pdf/1905.03677.pdf\nhttps://github.com/Mephisto405/Learning-Loss-for-Active-Learning"
  }
]