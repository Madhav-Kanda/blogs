[
  {
    "objectID": "posts/2023-07-21-wnr.html",
    "href": "posts/2023-07-21-wnr.html",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "",
    "text": "Windowed Non-Linear Reparameterisation for probabilistic models"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#what-is-reparameterisation",
    "href": "posts/2023-07-21-wnr.html#what-is-reparameterisation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "What is Reparameterisation?",
    "text": "What is Reparameterisation?\n\nExpressing the model in terms of new variables defined by a bijective transformation of the original variables.\nFor example: One such reparameterization technique is Non-centering.\n\nConsider: z ~ \\(N(\\mu, \\sigma)\\) (centered parameterization)\nIntroducing z’ ~ \\(N(0,1)\\)\nNow, z = \\(\\mu + \\sigma z'\\) (Non-centered parameterization)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#why-reparameterisation",
    "href": "posts/2023-07-21-wnr.html#why-reparameterisation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Why Reparameterisation?",
    "text": "Why Reparameterisation?\n\nAt times MCMC algorithms have trouble sampling from distributions.\nOne such example is Neal’s funnel in which due to strong non-linear dependence between latent variables.\nNon-centering the model removes this dependence, converting the funnel into a spherical Gaussian distribution.\nNealsFunnel(z, x)\n\n\\(z ∼ N (0, 3)\\) \\(\\quad\\)\n\\(x ∼ N (0, \\exp(z/2))\\)\n\n\n\n\n\n\nIs non centering the solution to all sampling related problem?\nNo!\n\n\n## Slide not to be shown (imports)\nimport jax\nimport optuna\nimport numpyro\nimport jax.numpy as jnp\nfrom numpyro.infer import MCMC, NUTS\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\nfrom jax.scipy.optimize import minimize\n\nrng_key = jax.random.PRNGKey(0) \nfrom numpyro.infer.reparam import LocScaleReparam\nfrom numpyro.handlers import reparam\n\n\n## Slide not to be shown (Some functions)\ndef inference(model):\n    nuts_kernel = NUTS(model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    mcmc.run(rng_key, extra_fields=('potential_energy',))\n    return mcmc.get_samples()\n\ndef scatterplt(samples):\n    try:\n        plt.scatter(samples['theta_decentered'], samples['mu'])\n        plt.xlabel('theta_decentered')\n    except:\n        plt.scatter(samples['theta'], samples['mu'])\n        plt.xlabel('theta')\n    plt.ylabel('mu')\n    plt.show()\n\n\nLets’ see an example to find out why!\n\n\nsamples_normal = jax.random.normal(rng_key, shape=(10000,)) + 3  ## samples from a standard normal\n\ndef model():\n    mu = numpyro.sample('mu', dist.Normal(5, 1))                  ## mu ~ N(5,1)\n    theta = numpyro.sample('theta', dist.Normal(mu, 2))           ## theta ~ N(mu,2)\n    numpyro.sample('obs', dist.Normal(theta, 1), obs=samples_normal)     ## P(y|theta) ~ N(theta,1)\n\n\nncp_model = reparam(model, config={'theta': LocScaleReparam(0)})\nsamples_ncp = inference(ncp_model)\nscatterplt(samples_ncp)\n\nsample: 100%|██████████████████████████████████████████████████| 1500/1500 [00:05&lt;00:00, 292.12it/s, 427 steps of size 1.29e-02. acc. prob=0.91]\n\n\n\n\n\n\nncp_model = reparam(model, config={'theta': LocScaleReparam(1)})\nsamples_ncp = inference(ncp_model)\nscatterplt(samples_ncp)\n\nsample: 100%|████████████████████████████████████████████████████| 1500/1500 [00:01&lt;00:00, 945.30it/s, 7 steps of size 7.40e-01. acc. prob=0.93]\n\n\n\n\n\n\nFrom the above experiment it is evident that the we don’t get the best results in non-centered parameterisation rather the best centeredness varies from model to model based on parameters."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#centeredness-vs-non-centeredness",
    "href": "posts/2023-07-21-wnr.html#centeredness-vs-non-centeredness",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Centeredness vs Non-centeredness",
    "text": "Centeredness vs Non-centeredness\n\nThe best parameterization for a given model may lie somewhere between centered and non centered representation.\nExisting solutions:\n\nVariationally Inferred Parameterization[1]\n\nNeuTra-lizing Bad Geometry in HMC[2]"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#problems-with-existing-solutions",
    "href": "posts/2023-07-21-wnr.html#problems-with-existing-solutions",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Problems with existing solutions",
    "text": "Problems with existing solutions\n\nRequires separate pre-processing steps apart from the regular warmup and sampling steps which increases the computation cost.\nNeed to tune the hyperparameters for the existing solutions to get good results"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#proposed-solution",
    "href": "posts/2023-07-21-wnr.html#proposed-solution",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Proposed Solution",
    "text": "Proposed Solution\n\nFinding the optimal centeredness during the succesive windows of warmup.\nLoss function for finding centeredness should be such that it takes the parameterized distribution as close as possible to Normal distribution."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#warmup-phase",
    "href": "posts/2023-07-21-wnr.html#warmup-phase",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Warmup Phase",
    "text": "Warmup Phase\n\nUsed for adaptation of inverse mass matrix (\\(M^{-1}\\)) and time step size (\\(\\Delta t\\)).\nConsist of three stages: \n\nInitial buffer (I): Time step adaptation (\\(\\Delta t\\))\nWindow buffer (II): Both Time step (\\(\\Delta t\\)) & Inverse mass matrix adaptation (\\(M^{-1}\\))\nTerm buffer (III): Final Time step adaptation (\\(\\Delta t\\))"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#modified-warmup-phase",
    "href": "posts/2023-07-21-wnr.html#modified-warmup-phase",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Modified Warmup Phase",
    "text": "Modified Warmup Phase\n\nUsed for adaptation of inverse mass matrix (\\(M^{-1}\\)), time step size (\\(\\Delta t\\)) and centeredness (\\(c\\)).\nInitial buffer and Term buffer remains the same.\nUsing the samples obtained after each window buffer, optimize the centeredness (\\(c\\)) so as to reduce the distance between the present reparameterized distribution and an independent normal distribution.\nFor each succesive window, reparameterize the model based on the optimal centeredness obtained and repeat the step for finding optimal centeredness."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#loss-function",
    "href": "posts/2023-07-21-wnr.html#loss-function",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Loss Function",
    "text": "Loss Function\n\n\\(\\Theta^{SXN}\\) \\(\\to\\) matrix of draws\n\\(\\Phi^{(c)}\\) \\(\\to\\) bijective family of reparameterisation (maps \\(P\\) to \\(P_{c}\\) and \\(\\Theta\\) to \\(\\Theta_{c})\\)\n\\(\\log p_{c}(\\theta^{(c)})\\) = \\(\\log p\\ (inv\\ \\Phi^{(c)})\\) + \\(\\log |\\nabla_{\\theta} \\Phi^{(c}|\\)\n\\(Q^{c}\\)~\\(N(mean(\\Theta_{c}), diag var(\\Theta_{c}))\\)\n\\(\\Phi^{(c^{*})} =\\ argmin_{\\Phi^{(c)}}KL(P^{(c)}||Q^{(c)})\\)"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#experimentation",
    "href": "posts/2023-07-21-wnr.html#experimentation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Experimentation",
    "text": "Experimentation\n\nInitial experiments done on Eight School ’s model.\n\n\nJ = 8\ny = jnp.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\nsigma = jnp.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n\n\ndef eight_schools(J, sigma, y=None):\n    mu = numpyro.sample('mu', dist.Normal(2, 5))                 # mu ~ N(2,5)\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))              # tau ~ HalfCauchy(5)\n    with numpyro.plate('J', J):\n        theta = numpyro.sample('theta', dist.Normal(mu, tau))    # theta_i ~ N(mu,tau)\n        numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)  # P(y|theta_i) ~ N(theta_i, sigma)\n\n\nOn post warmup samples\n\n\n# Sampling\nnuts_kernel = NUTS(eight_schools)\nwarmup = MCMC(nuts_kernel, num_warmup=1000, num_samples=0)    # Collecting all warmup samples           \nwarmup.warmup(rng_key, J, sigma, y=y, collect_warmup=True)\nsamples = warmup.get_samples()\n\nwarmup: 100%|███████████████████████████████████████████████████| 1000/1000 [00:01&lt;00:00, 664.42it/s, 31 steps of size 1.67e-01. acc. prob=0.79]"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#experimentation-1",
    "href": "posts/2023-07-21-wnr.html#experimentation-1",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Experimentation",
    "text": "Experimentation\n\nUsing the following continuous reparameterization family (from Variationally Inferred Parameterization (VIP)) :\n\nFor each \\(z_{i}\\) ~ \\(N(z_{i}|\\mu_{i}, \\sigma_{i})\\)\nDefine \\(z'_{i}\\) ~ $N({i} {i}, ^{_{i}}) $\nNow, \\(z_{i}\\) = \\(\\mu_{i} + \\sigma_{i}^{1-\\lambda_{i}}(z'_{i}-\\lambda_{i}\\mu_{i})\\)\n\n\n\n# For reparameterizing the samples and for creating the mulivariate normal distribution\ndef reparameterize_samples_dist(samples, c):\n    param_samples = samples['theta'].T\n    param_mean = samples['mu']\n    param_std = samples['tau']\n\n    new_param_samples = param_mean * c + (param_samples - param_mean) * param_std ** (c - 1)\n    \n    theta_mu = jnp.mean(new_param_samples, axis=1)\n    theta_std = jnp.std(new_param_samples, axis=1)\n    \n    mvn = dist.MultivariateNormal(loc=theta_mu, covariance_matrix=jnp.diag(theta_std**2))\n    \n    return new_param_samples, mvn, theta_mu, theta_std\n\n\n# For Computing the jacobian matrix for transformation\ndef log_jacobian(samples, c):\n    sigma = samples['tau']\n    logJ = jnp.sum(jnp.log(sigma) * (1 - c) * samples['theta'].shape[1])\n    logJ /= len(sigma)\n    return logJ\n\n\n# Function for computing KL divergence\ndef kl_value(centeredness):\n    centeredness = centeredness[0]\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, centeredness)\n    jacobian_log = log_jacobian(samples, centeredness)\n    kl = -mvn.log_prob(reparam_sample.T).mean() + jacobian_log    \n    return kl"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#experimentation-2",
    "href": "posts/2023-07-21-wnr.html#experimentation-2",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Experimentation",
    "text": "Experimentation\n\nOptimization\n\nJax based BFGS Optimization\n\n\n# jax based BFGS optimization\nc_initial = jnp.array([0.1])\nres = minimize(kl_value, c_initial, method='BFGS')\n\nbest_c_jax = res.x[0]\nprint(best_c_jax)\n\n0.039444983\n\n\n\nOptuna based optimization\n\n\ndef kl_value_optuna(centeredness):\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, centeredness)\n    jacobian_log = log_jacobian(samples, centeredness)\n    kl = -mvn.log_prob(reparam_sample.T).mean() + jacobian_log    \n    return kl\n\n\n# Defining the function that needs to be minimized\ndef objective(trial):\n    x = trial.suggest_float('x', 0, 1) \n    loss = kl_value_optuna(x)\n    return loss\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nbest_trial = study.best_trial\nbest_c_optuna = best_trial.params['x']\n\n[I 2023-07-21 10:18:04,196] A new study created in memory with name: no-name-e44544c7-441a-473a-bc18-a432f75adfa1\n[I 2023-07-21 10:18:04,206] Trial 0 finished with value: 23.828033447265625 and parameters: {'x': 0.6836423506232001}. Best is trial 0 with value: 23.828033447265625.\n[I 2023-07-21 10:18:04,213] Trial 1 finished with value: 22.330141067504883 and parameters: {'x': 0.31510480607284663}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,219] Trial 2 finished with value: 23.988616943359375 and parameters: {'x': 0.7416812802642535}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,225] Trial 3 finished with value: 22.793212890625 and parameters: {'x': 0.4021102636806101}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,235] Trial 4 finished with value: 23.1121826171875 and parameters: {'x': 0.47281628051339153}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,241] Trial 5 finished with value: 24.047700881958008 and parameters: {'x': 0.763610716203396}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,246] Trial 6 finished with value: 23.63525390625 and parameters: {'x': 0.6186275897607579}. Best is trial 1 with value: 22.330141067504883.\n[I 2023-07-21 10:18:04,250] Trial 7 finished with value: 22.01073455810547 and parameters: {'x': 0.26166971294215235}. Best is trial 7 with value: 22.01073455810547.\n[I 2023-07-21 10:18:04,255] Trial 8 finished with value: 20.962158203125 and parameters: {'x': 0.02985572213240162}. Best is trial 8 with value: 20.962158203125.\n[I 2023-07-21 10:18:04,261] Trial 9 finished with value: 24.657468795776367 and parameters: {'x': 0.9820145838977358}. Best is trial 8 with value: 20.962158203125.\n[I 2023-07-21 10:18:04,270] Trial 10 finished with value: 20.976028442382812 and parameters: {'x': 0.06068083002352253}. Best is trial 8 with value: 20.962158203125.\n[I 2023-07-21 10:18:04,279] Trial 11 finished with value: 21.0074405670166 and parameters: {'x': 0.006349822398729228}. Best is trial 8 with value: 20.962158203125.\n[I 2023-07-21 10:18:04,289] Trial 12 finished with value: 20.95828628540039 and parameters: {'x': 0.03837928344880258}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,296] Trial 13 finished with value: 21.380779266357422 and parameters: {'x': 0.1591324708414881}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,304] Trial 14 finished with value: 21.249404907226562 and parameters: {'x': 0.13516324406752406}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,311] Trial 15 finished with value: 21.51775550842285 and parameters: {'x': 0.1823033770408416}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,318] Trial 16 finished with value: 20.9879093170166 and parameters: {'x': 0.013546568169784973}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,326] Trial 17 finished with value: 22.08734703063965 and parameters: {'x': 0.2741767294681202}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,333] Trial 18 finished with value: 21.084999084472656 and parameters: {'x': 0.09928297501202724}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,342] Trial 19 finished with value: 21.66363525390625 and parameters: {'x': 0.2060073710034258}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,349] Trial 20 finished with value: 21.030845642089844 and parameters: {'x': 0.08372884654010455}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,356] Trial 21 finished with value: 20.97620964050293 and parameters: {'x': 0.019163537063202913}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,364] Trial 22 finished with value: 21.066829681396484 and parameters: {'x': 0.09443922855709355}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,373] Trial 23 finished with value: 21.014368057250977 and parameters: {'x': 0.004176641844005589}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,382] Trial 24 finished with value: 21.727500915527344 and parameters: {'x': 0.21623911997454487}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,391] Trial 25 finished with value: 22.513742446899414 and parameters: {'x': 0.34788855285413245}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,401] Trial 26 finished with value: 21.168197631835938 and parameters: {'x': 0.11871261692692425}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,411] Trial 27 finished with value: 21.795785903930664 and parameters: {'x': 0.22714616272200738}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,420] Trial 28 finished with value: 21.02497100830078 and parameters: {'x': 0.08178260702592446}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,428] Trial 29 finished with value: 21.428577423095703 and parameters: {'x': 0.16736705401787944}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,435] Trial 30 finished with value: 22.204639434814453 and parameters: {'x': 0.29366488158390514}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,446] Trial 31 finished with value: 20.961380004882812 and parameters: {'x': 0.048223931932001696}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,453] Trial 32 finished with value: 20.9786319732666 and parameters: {'x': 0.062229582663207994}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,469] Trial 33 finished with value: 20.96645736694336 and parameters: {'x': 0.05374654191329599}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,479] Trial 34 finished with value: 21.31035614013672 and parameters: {'x': 0.1465825701130587}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,493] Trial 35 finished with value: 20.961551666259766 and parameters: {'x': 0.04846254887528956}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,505] Trial 36 finished with value: 22.511117935180664 and parameters: {'x': 0.3474062272235182}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,529] Trial 37 finished with value: 21.258878707885742 and parameters: {'x': 0.13698018355781486}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,539] Trial 38 finished with value: 21.836639404296875 and parameters: {'x': 0.23367165785156452}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,558] Trial 39 finished with value: 21.46848487854004 and parameters: {'x': 0.17411007326437267}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,567] Trial 40 finished with value: 20.964630126953125 and parameters: {'x': 0.05202992141529239}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,575] Trial 41 finished with value: 20.961645126342773 and parameters: {'x': 0.048591371000934074}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,586] Trial 42 finished with value: 21.120513916015625 and parameters: {'x': 0.10802606138102802}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,593] Trial 43 finished with value: 21.01250457763672 and parameters: {'x': 0.00474672532039349}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,601] Trial 44 finished with value: 20.958911895751953 and parameters: {'x': 0.043482403412563084}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,609] Trial 45 finished with value: 20.96554183959961 and parameters: {'x': 0.05291200059542722}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,619] Trial 46 finished with value: 21.173307418823242 and parameters: {'x': 0.11980380956679877}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,627] Trial 47 finished with value: 23.02162742614746 and parameters: {'x': 0.4515929645231723}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,636] Trial 48 finished with value: 21.510578155517578 and parameters: {'x': 0.18111786301082644}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,644] Trial 49 finished with value: 20.95948600769043 and parameters: {'x': 0.04495479049105896}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,653] Trial 50 finished with value: 22.025508880615234 and parameters: {'x': 0.26407019378104224}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,663] Trial 51 finished with value: 20.960033416748047 and parameters: {'x': 0.0460635420372181}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,673] Trial 52 finished with value: 21.039709091186523 and parameters: {'x': 0.08654198156352484}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,681] Trial 53 finished with value: 21.015544891357422 and parameters: {'x': 0.0038215679879344475}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,694] Trial 54 finished with value: 21.246990203857422 and parameters: {'x': 0.1346967947930119}. Best is trial 12 with value: 20.95828628540039.\n[I 2023-07-21 10:18:04,706] Trial 55 finished with value: 20.9582576751709 and parameters: {'x': 0.040092522778978915}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,717] Trial 56 finished with value: 21.04631805419922 and parameters: {'x': 0.08855630713949014}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,729] Trial 57 finished with value: 20.969972610473633 and parameters: {'x': 0.022985995639844478}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,741] Trial 58 finished with value: 21.616783142089844 and parameters: {'x': 0.19846014189389016}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,752] Trial 59 finished with value: 21.32686996459961 and parameters: {'x': 0.14957843808205074}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,764] Trial 60 finished with value: 21.14473533630371 and parameters: {'x': 0.11357810170329409}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,775] Trial 61 finished with value: 20.958988189697266 and parameters: {'x': 0.03523243714958368}. Best is trial 55 with value: 20.9582576751709.\n[I 2023-07-21 10:18:04,786] Trial 62 finished with value: 20.95824432373047 and parameters: {'x': 0.03980408031718225}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,797] Trial 63 finished with value: 20.96192741394043 and parameters: {'x': 0.03014216381338495}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,813] Trial 64 finished with value: 21.021495819091797 and parameters: {'x': 0.08059661545748803}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,821] Trial 65 finished with value: 21.025482177734375 and parameters: {'x': 0.0009691038185872733}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,833] Trial 66 finished with value: 21.006858825683594 and parameters: {'x': 0.07525511089436765}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,840] Trial 67 finished with value: 21.14578628540039 and parameters: {'x': 0.11381304741962886}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,847] Trial 68 finished with value: 20.962078094482422 and parameters: {'x': 0.02995665198392279}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,863] Trial 69 finished with value: 21.322097778320312 and parameters: {'x': 0.14871659387062408}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,870] Trial 70 finished with value: 20.994400024414062 and parameters: {'x': 0.07011025385814619}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,878] Trial 71 finished with value: 20.958637237548828 and parameters: {'x': 0.0363739431908644}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,886] Trial 72 finished with value: 20.959197998046875 and parameters: {'x': 0.03467646738105701}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,898] Trial 73 finished with value: 21.048114776611328 and parameters: {'x': 0.08909241133254184}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,909] Trial 74 finished with value: 20.96074104309082 and parameters: {'x': 0.03176699515811235}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,920] Trial 75 finished with value: 21.021148681640625 and parameters: {'x': 0.002182253952155766}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,933] Trial 76 finished with value: 21.12643051147461 and parameters: {'x': 0.10940902514645028}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,945] Trial 77 finished with value: 20.960681915283203 and parameters: {'x': 0.03185762972201704}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,955] Trial 78 finished with value: 20.994333267211914 and parameters: {'x': 0.07008075304300285}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,964] Trial 79 finished with value: 21.428905487060547 and parameters: {'x': 0.16742308532333353}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,972] Trial 80 finished with value: 21.197486877441406 and parameters: {'x': 0.12485225812659262}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,981] Trial 81 finished with value: 20.96072769165039 and parameters: {'x': 0.0472450186828239}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,989] Trial 82 finished with value: 20.95913314819336 and parameters: {'x': 0.03484208925496168}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:04,998] Trial 83 finished with value: 20.96847152709961 and parameters: {'x': 0.024057097791501503}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,006] Trial 84 finished with value: 20.979900360107422 and parameters: {'x': 0.06294947941382306}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,016] Trial 85 finished with value: 21.07381248474121 and parameters: {'x': 0.09633630736584171}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,024] Trial 86 finished with value: 20.963844299316406 and parameters: {'x': 0.028003696384222197}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,032] Trial 87 finished with value: 20.9892578125 and parameters: {'x': 0.06775539420105053}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,040] Trial 88 finished with value: 21.25747299194336 and parameters: {'x': 0.13671166311556443}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,048] Trial 89 finished with value: 21.06121253967285 and parameters: {'x': 0.09287712546228753}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,056] Trial 90 finished with value: 21.546085357666016 and parameters: {'x': 0.18696192580912566}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,063] Trial 91 finished with value: 20.960697174072266 and parameters: {'x': 0.04720043772663794}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,076] Trial 92 finished with value: 20.976818084716797 and parameters: {'x': 0.01883262442405295}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,088] Trial 93 finished with value: 21.0242977142334 and parameters: {'x': 0.0012964148062991898}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,097] Trial 94 finished with value: 20.95972442626953 and parameters: {'x': 0.0454616951355606}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,104] Trial 95 finished with value: 21.119361877441406 and parameters: {'x': 0.10775472692646301}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,115] Trial 96 finished with value: 20.982824325561523 and parameters: {'x': 0.06453943294273504}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,128] Trial 97 finished with value: 20.958269119262695 and parameters: {'x': 0.03862885747244034}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,138] Trial 98 finished with value: 21.014057159423828 and parameters: {'x': 0.07796030040550225}. Best is trial 62 with value: 20.95824432373047.\n[I 2023-07-21 10:18:05,158] Trial 99 finished with value: 21.34369659423828 and parameters: {'x': 0.15259583517020947}. Best is trial 62 with value: 20.95824432373047.\n\n\n\ndf_study = study.trials_dataframe()\nplt.plot(df_study['params_x'], df_study['value'], 'o', alpha=0.5)\nplt.show()\n\n\n\n\n\nEvident from above that Optuna keeps decreasing the search space after each iteration to get close to the best centeredness value.\n\n\n# Function for creating the scatter plot\ndef scatterplt_2(samples, c):\n    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    axes = axes.flatten()\n    reparam_sample, mvn, mu_theta, std_theta = reparameterize_samples_dist(samples, c)\n    reparam_sample = reparam_sample if reparam else samples['theta'].T\n    axes[0].scatter(reparam_sample[0],jnp.log(samples['tau']))\n    axes[0].set_xlabel('theta0_reparam')\n    axes[0].set_ylabel('log_std')\n\n    axes[1].scatter(samples['theta'].T[0],jnp.log(samples['tau']))\n    axes[1].set_xlabel('theta0')\n\n\nResults obtained from initial experiments\n\n\nscatterplt_2(samples, c = best_c_jax)     ## Using the best centeredness computed"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#implementation",
    "href": "posts/2023-07-21-wnr.html#implementation",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Implementation",
    "text": "Implementation\n\nModified BlackJax sampler code to incorporate the proposed algorithm.\nInference time up from 2 seconds to 20 seconds.\nTo evaluate the results used a model whose true centeredness already known:\n\nWrite the model"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#results",
    "href": "posts/2023-07-21-wnr.html#results",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Results",
    "text": "Results\n\n\n&lt;img width=\"700\" alt=\"image\" src=\"images/wnr/true_calculated.png\"&gt;"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#results-1",
    "href": "posts/2023-07-21-wnr.html#results-1",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#results-2",
    "href": "posts/2023-07-21-wnr.html#results-2",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Results",
    "text": "Results\n\nLets’ compare the sampling efficieny\n\n$\n\\[\\begin{array}{c} \\hline\nParameter & ESS/ \\nabla (vip) & ESS/ \\nabla (our) \\\\ \\hline\n\\mu & 9.27*10^{-4} & 5.54*10^{-4} \\\\\n\\tau & 4.22*10^{-5} & \\textbf{$5.07*10^{-4}$} \\\\\n\\theta & 9.73*10^{-4} & \\textbf{$1.66*10^{-3}$} \\\\ \\hline\n\\end{array}\\]\n$\n\nOur ESS/ \\(\\nabla\\) results better then the ones produced by existing methods."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#future-work",
    "href": "posts/2023-07-21-wnr.html#future-work",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "Future Work",
    "text": "Future Work\n\nChanging the default window sizes.\nTrying different parameterization families."
  },
  {
    "objectID": "posts/2023-07-21-wnr.html#references",
    "href": "posts/2023-07-21-wnr.html#references",
    "title": "Windowed Non-Linear Reparamaterization",
    "section": "References",
    "text": "References\n[1] M.I.Gorinova,D.Moore,andM.D.Hoffman,“Automaticreparameterisation of probabilistic programs,” 2019.\n[2] M.Hoffman,P.Sountsov,etal.,“Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport,” 2019."
  },
  {
    "objectID": "posts/2023-04-12-la.html",
    "href": "posts/2023-04-12-la.html",
    "title": "Laplace Approximation",
    "section": "",
    "text": "# Importing modules\ntry:\n  import jax                      # JAX is a library for differentiable programming\nexcept ModuleNotFoundError:\n  %pip install jaxlib jax\n  import jax\nimport jax.numpy as jnp           # JAX's numpy implementation\ntry:\n  import tensorflow_probability.substrates.jax as tfp     # TFP is a library for probabilistic programming\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom tqdm import trange\nimport logging\nlogger = logging.getLogger()\nclass CheckTypesFilter(logging.Filter):                   \n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\n\nSampling from the Bernouli distribution with \\(\\theta\\) = 0.7\n\nbernoulli_samples = tfp.distributions.Bernoulli(\n    probs=0.7\n)  # Create a Bernoulli distribution with p=0.7\nsamples = bernoulli_samples.sample(\n    sample_shape=100, seed=jax.random.PRNGKey(0)\n)  # Sample 100 samples from the distribution\nprint(samples)\n\nalpha = 3  # Set the parameter (alpha) of the Beta distribution\nbeta = 5  # Set the parameter (beta) of the Beta distribution\nsamples.sum()\n\n[1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1]\n\n\nDeviceArray(69, dtype=int32)\n\n\n\n\nNegative log joint\n\ndef neg_logjoint(theta):  # Define the negative log-joint distribution\n    alpha = 3\n    beta = 5\n    dist_prior = tfp.distributions.Beta(alpha, beta)\n    dist_likelihood = tfp.distributions.Bernoulli(probs=theta)\n    return -(dist_prior.log_prob(theta) + dist_likelihood.log_prob(samples).sum())\n\n\n\nCalculating \\(\\theta_{map}\\) by minimising the negative log joint using gradient descent\n\ngradient = jax.value_and_grad(\n    jax.jit(neg_logjoint)\n)  # Define the gradient of the negative log-joint distribution\nlr = 0.001  # Set the learning rate\nepochs = 200  # Set the number of epochs\ntheta_map = 0.5  # Set the initial value of theta\nlosses = []\nfor i in trange(epochs):  # Run the optimization loop\n    val, grad = gradient(theta_map)\n    theta_map -= lr * grad\n    losses.append(val)\nplt.plot(losses)\nsns.despine()\ntheta_map\n\n100%|██████████| 200/200 [00:02&lt;00:00, 71.83it/s] \n\n\nDeviceArray(0.6698113, dtype=float32, weak_type=True)\n\n\n\n\n\n\n\nVerification of obtained \\(\\theta_{map}\\) value using the formula:\n\n\n\\(\\theta_{map} = \\frac{n_h+\\alpha-1}{n_h+n_t+\\alpha+\\beta-2}\\)\n\nnH = samples.sum().astype(\"float32\")  # Compute the number of heads\nnT = (samples.size - nH).astype(\"float32\")  # Compute the number of tails\ntheta_check = (nH + alpha - 1) / (\n    nH + nT + alpha + beta - 2\n)  # Compute the posterior mean\ntheta_check\n\nDeviceArray(0.6698113, dtype=float32)\n\n\n\n\nComputing Hessian and Covariance\n\nhessian = jax.hessian(neg_logjoint)(\n    theta_map\n)  # Compute the Hessian of the negative log-joint distribution\nhessian = jnp.reshape(hessian, (1, 1))  # Reshape the Hessian to a 1x1 matrix\ncov = jnp.linalg.inv(hessian)  # Compute the covariance matrix\ncov\n\nDeviceArray([[0.00208645]], dtype=float32)\n\n\n\n\nPlots Comparing the distribution obtained using Laplace approximation with actual Beta Bernoulli posterior\n\n# Compute the Laplace approximation\nx = jnp.linspace(0, 1, 100)  # Create a grid of 100 points between 0 and 1\nx = x.reshape(-1, 1)  # Reshape the grid to a 100x1 matrix\nLaplace_Approx = tfp.distributions.MultivariateNormalFullCovariance(  # Create a multivariate normal distribution\n    loc=theta_map, covariance_matrix=cov\n)\nLaplace_Approx_pdf = Laplace_Approx.prob(\n    x\n)  # Compute the probability density function of the Laplace approximation\nplt.plot(x, Laplace_Approx_pdf, label=\"Laplace Approximation\")\n\n\n# Compute the true posterior distribution\nalpha = 3\nbeta = 5\ntrue_posterior = tfp.distributions.Beta(\n    alpha + nH, beta + nT\n)  # Create a Beta distribution\ntrue_posterior_pdf = true_posterior.prob(\n    x\n)  # Compute the probability density function of the true posterior\nplt.plot(x, true_posterior_pdf, label=\"True Posterior\")\nplt.xlim(0, 1)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2181895a10&gt;\n\n\n\n\n\n\n# Compute the log-probability density function of the Laplace approximation\ntrue_posterior_pdf_log = true_posterior.log_prob(x)\nLaplace_Approx_pdf_log = Laplace_Approx.log_prob(x)\nplt.plot(x, Laplace_Approx_pdf_log, label=\"Laplace Approximation\")\nplt.plot(x, true_posterior_pdf_log, label=\"True Posterior\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2180719250&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Windowed Non-Linear Reparamaterization\n\n\n\n\n\n\n\nMCMC\n\n\nBayesian\n\n\n\n\nWindowed Non-Linear Reparamaterization\n\n\n\n\n\n\nJul 21, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nConformal Prediction\n\n\n\n\n\n\n\nConformal Prediction\n\n\nML\n\n\n\n\nExplaining Conformal Prediction\n\n\n\n\n\n\nApr 25, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\n  \n\n\n\n\nLaplace Approximation\n\n\n\n\n\n\n\nLaplace Approximation\n\n\nML\n\n\n\n\nImplementing Laplace Approximation in JAX\n\n\n\n\n\n\nApr 15, 2023\n\n\nMadhav Kanda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-24-cp.html",
    "href": "posts/2023-04-24-cp.html",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Guess the following images:\n\n\n\nimage.png\n\n\nPrediction set generated by conformal prediction for the images:\n\n\n\nimage.png\n\n\n\n\n\nUsing conformal prediction we aim to generate rigorous, finite sample confidence intervals for any model and any dataset. Unlike a point prediction from neural network, here we will get a confidence interval in which desired output is guaranteed to be.\n\n\n\n\nBegin with a fitted predicted model which we call \\(\\hat{f}\\).\nCreate a predicted set (set of possible labels) for this model using a small amount of calibration data."
  },
  {
    "objectID": "posts/2023-04-24-cp.html#why-use-conformal-prediction",
    "href": "posts/2023-04-24-cp.html#why-use-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Guess the following images:\n\n\n\nimage.png\n\n\nPrediction set generated by conformal prediction for the images:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#aim",
    "href": "posts/2023-04-24-cp.html#aim",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Using conformal prediction we aim to generate rigorous, finite sample confidence intervals for any model and any dataset. Unlike a point prediction from neural network, here we will get a confidence interval in which desired output is guaranteed to be."
  },
  {
    "objectID": "posts/2023-04-24-cp.html#outline",
    "href": "posts/2023-04-24-cp.html#outline",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Begin with a fitted predicted model which we call \\(\\hat{f}\\).\nCreate a predicted set (set of possible labels) for this model using a small amount of calibration data."
  },
  {
    "objectID": "posts/2023-04-24-cp.html#given",
    "href": "posts/2023-04-24-cp.html#given",
    "title": "Conformal Prediction",
    "section": "Given",
    "text": "Given\n\nA calibration dataset \\(\\{(x_i,y_i)\\}_{i=1}^n\\) (This is the dataset that the model does not see during training).\nA \\(model\\) \\(\\hat{\\pi}(x) = P[Y=y|X=x]\\)\nA \\(new\\) \\(data\\) \\(point\\) \\(x_{n+1}\\) to test the model"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#goal",
    "href": "posts/2023-04-24-cp.html#goal",
    "title": "Conformal Prediction",
    "section": "Goal",
    "text": "Goal\nPredict a set \\(\\tau(X_{test})\\) for the data point \\(X_{test}\\) that is a subset of the label space \\(i.e.\\) predict a set, \\(\\tau(X_{test}) \\subseteq y\\). This set should contain the true class \\(Y_{test}\\) and should be valid in the following sense:\n$ 1 - P[Y_{test} (X_{test})] - + $\nhere \\(\\alpha\\) is a user chosen rate in \\(\\in [0,1]\\), \\(y\\) is the set of all labels & \\(n\\) is the number of points in calibration dataset. The above mentioned property is called Marginal Coverage."
  },
  {
    "objectID": "posts/2023-04-24-cp.html#objective-for-the-sets",
    "href": "posts/2023-04-24-cp.html#objective-for-the-sets",
    "title": "Conformal Prediction",
    "section": "Objective for the sets",
    "text": "Objective for the sets\n\nExact coverage\nSmall size\nSize of the set should indicate the difficulty of the examples \\(i.e.\\) Adaptivity"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#general-method-for-conformal-prediction",
    "href": "posts/2023-04-24-cp.html#general-method-for-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "General Method for Conformal Prediction",
    "text": "General Method for Conformal Prediction\n\nIdentify a heuristic notion of uncertainity\nDefine a score function \\(S(x,y)\\) based on the values in step 1. In general large values of \\(S\\) corresponds to a bad fit between \\(x\\) \\(\\&\\) \\(y\\)\nCompute \\(\\hat{q} : \\frac{\\lceil{(n+1)(1-\\alpha)}\\rceil}{n}\\) quantile of \\(S(x,y)\\) on calibration dataset\nTo obtain the prediction set: \\(\\tau(x) = \\{y:S(x,y) \\le \\hat{q} \\}\\)"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#now-lets-implement-conformal-prediction",
    "href": "posts/2023-04-24-cp.html#now-lets-implement-conformal-prediction",
    "title": "Conformal Prediction",
    "section": "Now Lets’ implement conformal Prediction",
    "text": "Now Lets’ implement conformal Prediction\nA simpler version of the conformal prediction\n\nCompute \\(\\hat{q} : \\alpha\\) quantile of \\(S(x,y)\\) on calibration dataset where \\(S(x,y)\\) is the score function correspoding to the true label\nTo obtain the prediction set: \\(\\tau(x) = \\{y:S(x,y) \\ge \\hat{q} \\}\\)\n\n\ncalib_target_results = np.array(calib_target_results)\ncalib_prediction_results = np.array(calib_prediction_results)\ntest_prediction_results = np.array(test_prediction_results)\ntest_target_results = np.array(test_target_results)\n\ncalib_prediction_results.shape, calib_target_results.shape, test_prediction_results.shape, test_target_results.shape\n\n((5000, 10), (5000,), (5000, 10), (5000,))\n\n\n\ncalib_df = pd.DataFrame(calib_prediction_results)\ncalib_df[\"Max\"] = calib_df.max(axis=1)\ncalib_df[\"Max_idx\"] = calib_df.idxmax(axis=1)\ncalib_df[\"True_idx\"] = calib_target_results\ncalib_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n0\n7.672552e-08\n5.311417e-07\n1.562821e-04\n3.211690e-09\n4.822823e-04\n2.262360e-06\n9.993560e-01\n3.220139e-11\n1.609646e-06\n9.680530e-07\n0.999356\n6\n6\n\n\n1\n7.041307e-08\n1.779545e-07\n6.658971e-06\n4.084817e-07\n6.008969e-08\n1.122653e-08\n1.417746e-04\n1.310595e-09\n9.998498e-01\n9.002325e-07\n0.999850\n8\n8\n\n\n2\n1.891271e-06\n7.539936e-07\n8.252732e-06\n7.875642e-05\n6.665982e-01\n2.489255e-06\n7.079141e-04\n4.619782e-05\n3.568919e-02\n2.968663e-01\n0.666598\n4\n5\n\n\n3\n3.600329e-08\n4.196039e-09\n4.928238e-06\n3.145105e-09\n9.909703e-01\n3.594812e-08\n2.783901e-05\n7.394720e-06\n8.526304e-07\n8.988610e-03\n0.990970\n4\n4\n\n\n4\n8.007726e-06\n8.686094e-05\n1.262506e-04\n9.905047e-01\n2.181237e-07\n3.404921e-07\n8.625836e-06\n2.125504e-06\n9.200612e-03\n6.224329e-05\n0.990505\n3\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n3.017699e-06\n8.860756e-05\n3.417503e-04\n9.827271e-01\n5.802370e-05\n1.595727e-06\n2.064353e-04\n2.161075e-06\n1.435820e-02\n2.213240e-03\n0.982727\n3\n3\n\n\n4996\n5.440750e-13\n9.999996e-01\n7.675386e-08\n5.033297e-11\n2.031701e-11\n4.493560e-11\n7.547337e-10\n3.736634e-08\n2.838881e-07\n3.246134e-12\n1.000000\n1\n1\n\n\n4997\n5.372684e-09\n3.440873e-09\n1.484947e-07\n9.999758e-01\n2.131953e-12\n4.374669e-11\n2.115596e-12\n2.734103e-06\n2.123476e-05\n9.234377e-08\n0.999976\n3\n3\n\n\n4998\n1.966202e-07\n6.421658e-07\n5.014269e-05\n2.109797e-09\n2.172950e-05\n5.110368e-07\n9.999149e-01\n3.196222e-10\n3.617991e-06\n8.205562e-06\n0.999915\n6\n4\n\n\n4999\n2.777072e-14\n1.000000e+00\n1.977435e-08\n3.481474e-11\n2.748897e-12\n9.301366e-12\n5.802548e-11\n3.565957e-08\n2.786610e-08\n1.873916e-12\n1.000000\n1\n1\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\ntest_df = pd.DataFrame(test_prediction_results)\ntest_df[\"Max\"] = test_df.max(axis=1)\ntest_df[\"Max_idx\"] = test_df.idxmax(axis=1)\ntest_df[\"True_idx\"] = test_target_results\ntest_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n0\n1.507169e-09\n6.793355e-10\n9.392306e-10\n3.272889e-09\n4.279739e-09\n1.000421e-10\n7.060916e-13\n9.973305e-01\n3.673427e-07\n2.669133e-03\n0.997331\n7\n7\n\n\n1\n1.528662e-08\n4.265159e-12\n3.008223e-05\n9.999698e-01\n1.717640e-15\n2.861857e-14\n1.901743e-12\n4.749006e-13\n8.433129e-08\n6.817440e-14\n0.999970\n3\n3\n\n\n2\n1.440134e-10\n3.157558e-06\n2.407116e-05\n5.108404e-11\n5.610714e-07\n1.101829e-08\n9.999588e-01\n4.209422e-15\n1.334558e-05\n3.933881e-10\n0.999959\n6\n6\n\n\n3\n6.468001e-12\n9.999986e-01\n3.110969e-07\n3.365797e-09\n3.275069e-11\n3.477734e-10\n1.458348e-08\n3.999051e-08\n1.144535e-06\n1.596769e-10\n0.999999\n1\n1\n\n\n4\n8.815193e-05\n6.341890e-03\n8.312734e-04\n1.951014e-04\n1.125594e-02\n2.372148e-04\n7.425601e-03\n1.311864e-03\n1.279543e-02\n9.595175e-01\n0.959518\n9\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n2.418953e-08\n7.203251e-08\n1.530548e-07\n2.897828e-06\n5.851566e-07\n6.657368e-09\n3.703725e-08\n6.012499e-05\n9.920814e-01\n7.854681e-03\n0.992081\n8\n8\n\n\n4996\n4.415281e-09\n1.504091e-08\n2.217878e-06\n9.999076e-01\n1.170547e-09\n1.767450e-10\n1.133458e-08\n1.103373e-09\n8.993938e-05\n2.646218e-07\n0.999908\n3\n3\n\n\n4997\n1.863301e-18\n3.640452e-17\n5.301770e-13\n5.253428e-15\n9.999909e-01\n7.949940e-15\n1.362663e-11\n6.954227e-13\n6.574446e-12\n9.051804e-06\n0.999991\n4\n4\n\n\n4998\n6.859559e-17\n7.214033e-16\n3.279532e-11\n1.938939e-14\n9.999964e-01\n1.438514e-13\n1.612270e-10\n2.956339e-12\n7.959585e-12\n3.579523e-06\n0.999996\n4\n4\n\n\n4999\n2.643393e-01\n7.046102e-07\n1.252546e-02\n5.733218e-01\n1.109647e-07\n3.844558e-07\n2.375773e-02\n1.729390e-07\n1.260542e-01\n1.263923e-07\n0.573322\n3\n5\n\n\n\n\n5000 rows × 13 columns"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#intuitive-understanding",
    "href": "posts/2023-04-24-cp.html#intuitive-understanding",
    "title": "Conformal Prediction",
    "section": "Intuitive understanding",
    "text": "Intuitive understanding\n\nstds_cal = np.std(calib_prediction_results, axis=1)\nmin_std_indices_cal = np.argsort(stds_cal)\n\nSorting based on std deviation in the rows\n\ncalib_df_std = calib_df.loc[min_std_indices_cal]\ncalib_df_std\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n4792\n2.204627e-01\n1.263805e-02\n1.064651e-01\n2.497595e-02\n3.831481e-02\n1.915999e-02\n2.428759e-01\n4.884591e-02\n2.138946e-01\n7.236703e-02\n0.242876\n6\n5\n\n\n3635\n3.720535e-03\n3.984575e-02\n6.415164e-02\n8.764555e-02\n3.574266e-01\n1.044143e-02\n2.420991e-02\n2.146130e-01\n4.631169e-02\n1.516339e-01\n0.357427\n4\n6\n\n\n1115\n1.880663e-02\n2.226060e-01\n8.889224e-02\n6.196426e-02\n1.060972e-02\n5.382521e-03\n2.736638e-01\n7.305532e-03\n2.858033e-01\n2.496605e-02\n0.285803\n8\n5\n\n\n1031\n1.033852e-01\n5.177050e-02\n7.211524e-02\n1.307259e-01\n8.214290e-03\n5.626875e-03\n2.568235e-01\n5.283331e-03\n3.498454e-01\n1.620973e-02\n0.349845\n8\n5\n\n\n1778\n7.243351e-02\n5.959544e-05\n1.995622e-01\n1.425000e-05\n2.422987e-01\n4.558027e-04\n1.450675e-01\n8.497129e-03\n4.330557e-04\n3.311783e-01\n0.331178\n9\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2909\n1.735541e-15\n2.001958e-12\n8.669379e-16\n1.821178e-15\n2.716592e-15\n2.300831e-16\n2.615356e-20\n1.000000e+00\n8.539057e-13\n3.396687e-09\n1.000000\n7\n7\n\n\n2914\n1.000000e+00\n5.918221e-19\n1.986233e-10\n1.237661e-14\n4.863426e-19\n3.770102e-16\n1.504733e-11\n7.523226e-09\n1.627343e-10\n1.057123e-13\n1.000000\n0\n0\n\n\n2920\n1.139901e-11\n6.290391e-12\n4.389559e-08\n1.000000e+00\n4.736745e-17\n5.172185e-15\n1.662083e-15\n2.492923e-11\n5.069354e-10\n1.366577e-12\n1.000000\n3\n3\n\n\n2847\n2.374335e-11\n5.410563e-10\n1.000000e+00\n6.474658e-10\n3.318972e-12\n1.285993e-12\n1.194038e-08\n1.259493e-15\n1.686373e-10\n9.267352e-15\n1.000000\n2\n2\n\n\n4999\n2.777072e-14\n1.000000e+00\n1.977435e-08\n3.481474e-11\n2.748897e-12\n9.301366e-12\n5.802548e-11\n3.565957e-08\n2.786610e-08\n1.873916e-12\n1.000000\n1\n1\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\ny1 = calib_prediction_results[min_std_indices_cal[0]]\ny2 = calib_prediction_results[min_std_indices_cal[1]]\ny3 = calib_prediction_results[min_std_indices_cal[2]]\ny4 = calib_prediction_results[4500]\n\nx = np.arange(10)\n\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\n\n# Add bar plots to the subplots\nbars1 = axs[0, 0].bar(x, y1, color=\"#ff7f0e\", width=0.6)\naxs[0, 0].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[0]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars2 = axs[0, 1].bar(x, y2, color=\"#2ca02c\", width=0.6)\naxs[0, 1].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[1]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars3 = axs[1, 0].bar(x, y3, color=\"#1f77b4\", width=0.6)\naxs[1, 0].set_title(\n    f\"True Label: {calib_target_results[min_std_indices_cal[2]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars4 = axs[1, 1].bar(x, y4, color=\"#d62728\", width=0.6)\naxs[1, 1].set_title(\n    f\"True Label: {calib_target_results[4500]}\", fontsize=12, fontweight=\"bold\"\n)\n\n# Add labels and title to the figure\nfig.suptitle(\"Model's output on calibration dataset\", fontsize=14, fontweight=\"bold\")\n\nfor ax in axs.flat:\n    ax.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\n# Fine-tune the subplot layout\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nbars1[calib_target_results[min_std_indices_cal[0]]].set_color(\"#9467bd\")\nbars2[calib_target_results[min_std_indices_cal[1]]].set_color(\"#9467bd\")\nbars3[calib_target_results[min_std_indices_cal[2]]].set_color(\"#9467bd\")\nbars4[calib_target_results[4500]].set_color(\"#9467bd\")\n\n\n\n\n\ncalib_true = calib_prediction_results[\n    np.arange(calib_prediction_results.shape[0]), calib_target_results\n]\n\nFrom the above plot it is evident that most of the softmax outputs corresponding to the true label are either close to 1 or close to 0. Thus, once we find out the value corresponding to the threshold of the 0 peak in the plot. Any quantile value just above this will quickly go near the next peak as there is no distribution mass for the rest of the softmax outputs.\n\n## Quantile value that we use to predict the prediction set\nqhat_intuit = np.quantile(calib_true, 0.15)  ## taking 15% quantile\nqhat_intuit\n\n0.17026243805885327\n\n\n_This leads to the fact that 85% of examples have their true class softmax score above \\(\\hat{q}_{intuit}\\)_\nSorting the test dataset according to std\n\nstds_test = np.std(test_prediction_results, axis=1)\nmin_std_indices_test = np.argsort(stds_test)\n\n\ntest_df_std = test_df.loc[min_std_indices_test]\ntest_df_std\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMax\nMax_idx\nTrue_idx\n\n\n\n\n1646\n9.070086e-06\n7.102170e-02\n1.074573e-01\n2.019677e-01\n2.011996e-01\n1.028181e-03\n1.132987e-03\n1.039726e-01\n1.606203e-01\n1.515905e-01\n0.201968\n3\n8\n\n\n1184\n4.843292e-08\n1.841913e-01\n5.504493e-02\n1.019774e-01\n1.562013e-01\n2.759257e-04\n9.354739e-04\n1.538243e-01\n1.096576e-02\n3.365835e-01\n0.336583\n9\n7\n\n\n3654\n4.213768e-04\n7.209036e-02\n1.887893e-02\n1.189776e-02\n3.334404e-01\n2.851049e-03\n1.309165e-01\n8.140079e-04\n2.165484e-01\n2.121412e-01\n0.333440\n4\n5\n\n\n3019\n2.256842e-01\n2.321515e-07\n2.911193e-01\n2.409384e-01\n1.869016e-11\n2.536230e-09\n5.548395e-02\n2.447040e-10\n1.867739e-01\n2.661302e-11\n0.291119\n2\n5\n\n\n4896\n1.610583e-01\n4.679085e-05\n8.821968e-04\n1.386619e-01\n1.308930e-03\n8.392150e-05\n3.836229e-02\n5.723350e-02\n2.713310e-01\n3.310311e-01\n0.331031\n9\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3128\n1.000000e+00\n4.840718e-18\n1.905646e-08\n2.940498e-11\n1.603996e-18\n5.580763e-16\n4.068423e-11\n1.949603e-11\n2.676126e-09\n3.716259e-15\n1.000000\n0\n0\n\n\n4507\n1.798757e-18\n1.045387e-13\n5.538239e-17\n2.234547e-16\n7.610493e-18\n6.503045e-19\n4.574509e-24\n1.000000e+00\n1.466630e-14\n1.054316e-12\n1.000000\n7\n7\n\n\n3130\n5.435448e-18\n5.444775e-11\n1.383103e-15\n2.973889e-15\n2.213195e-15\n1.832753e-16\n7.384352e-22\n1.000000e+00\n3.005146e-12\n3.994342e-10\n1.000000\n7\n7\n\n\n3142\n1.000000e+00\n2.979347e-17\n1.458575e-08\n1.863164e-11\n1.070295e-18\n1.046285e-15\n7.635514e-11\n2.087787e-12\n5.457018e-09\n3.388109e-15\n1.000000\n0\n0\n\n\n3060\n1.986817e-17\n5.229983e-14\n1.823637e-17\n3.066719e-17\n1.413327e-16\n2.220211e-18\n7.849721e-23\n1.000000e+00\n4.188006e-14\n2.416875e-10\n1.000000\n7\n7\n\n\n\n\n5000 rows × 13 columns\n\n\n\n\n## Forming prediction sets\n\ny1 = test_prediction_results[min_std_indices_test[1]]\ny2 = test_prediction_results[min_std_indices_test[2]]\ny3 = test_prediction_results[min_std_indices_test[3]]\ny4 = test_prediction_results[4500]\n\ntest_array_indices = [\n    min_std_indices_test[1],\n    min_std_indices_test[2],\n    min_std_indices_test[3],\n    4500,\n]\n\nx = np.arange(10)\n\n# Create a new figure with 3 subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Add bar plots to the subplots\nbars1 = axs[0, 0].bar(x, y1, color=\"#ff7f0e\", width=0.6)\naxs[0, 0].set_title(\n    f\"True Label: {test_target_results[test_array_indices[0]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars2 = axs[0, 1].bar(x, y2, color=\"#2ca02c\", width=0.6)\naxs[0, 1].set_title(\n    f\"True Label: {test_target_results[test_array_indices[1]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars3 = axs[1, 0].bar(x, y3, color=\"#1f77b4\", width=0.6)\naxs[1, 0].set_title(\n    f\"True Label: {test_target_results[test_array_indices[2]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\nbars4 = axs[1, 1].bar(x, y4, color=\"#d62728\", width=0.6)\naxs[1, 1].set_title(\n    f\"True Label: {test_target_results[test_array_indices[3]]}\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\n\n# Add labels and title to the figure\nfig.suptitle(\"Model's output on test dataset\", fontsize=14, fontweight=\"bold\")\n\nfor ax in axs.flat:\n    ax.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axs.flatten():\n    ax.axhline(y=qhat_intuit, color=\"black\", linewidth=2)\n\n# Fine-tune the subplot layout\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n\nbars1_idx = y1 &gt; qhat_intuit\nbars2_idx = y2 &gt; qhat_intuit\nbars3_idx = y3 &gt; qhat_intuit\nbars4_idx = y4 &gt; qhat_intuit\n\nfor i in range(10):\n    if bars1_idx[i]:\n        bars1[i].set_color(\"#8c564b\")\n    if bars2_idx[i]:\n        bars2[i].set_color(\"#8c564b\")\n    if bars3_idx[i]:\n        bars3[i].set_color(\"#8c564b\")\n    if bars4_idx[i]:\n        bars4[i].set_color(\"#8c564b\")\n\nbars1[test_target_results[test_array_indices[0]]].set_color(\"#9467bd\")\nbars2[test_target_results[test_array_indices[1]]].set_color(\"#9467bd\")\nbars3[test_target_results[test_array_indices[2]]].set_color(\"#9467bd\")\nbars4[test_target_results[test_array_indices[3]]].set_color(\"#9467bd\")\n\n\n\n\n_All the bars above the \\(\\hat{q}_{intuit}\\) are the part of the prediction set for the corresponding test dataset_\n\n# Use numpy indexing to get the softmax scores for each image corresponding to their true labels\ntest_true = test_prediction_results[\n    np.arange(test_prediction_results.shape[0]), test_target_results\n]"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#implementing-conformal-prediction-using-the-general-method",
    "href": "posts/2023-04-24-cp.html#implementing-conformal-prediction-using-the-general-method",
    "title": "Conformal Prediction",
    "section": "Implementing conformal prediction using the General Method",
    "text": "Implementing conformal prediction using the General Method\n1. Here the heuristic notion of uncertainity is softmax output\n\n# Problem setup\nn = 5000  # number of calibration points\nalpha = 0.15  # 1-alpha is the desired coverage\n\n\ncal_scores = 1 - calib_true\n\n2. Defining the score function as: \\(S(x,y)\\) = \\(1- softmax(x_i,y_i)\\)\n\nq_level = np.ceil((n + 1) * (1 - alpha)) / n  ## alpha = 0.1, n = 1000, q_level = 0.901\nqhat = np.quantile(\n    cal_scores, q_level\n)  ## value for which 90% of the scores are less than it\nqhat\n\n0.8337927011847496\n\n\n3. Calculating \\(\\hat{q}\\)\n4. Creating the prediction set\n$ C(x) = {y:S(x,y) }$\n\nfig = plt.figure()\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(test_images[test_array_indices[i]][0], cmap=\"gray\", interpolation=\"none\")\n    test_scores = 1 - test_prediction_results[test_array_indices[i]]\n    prediction_set = test_scores &lt; qhat\n    indices = []\n    for index, val in enumerate(prediction_set):\n        if val:\n            indices.append(index)\n\n    true_label = test_target_results[test_array_indices]\n\n    plt.title(f\"Set: {indices} True:{true_label[i]}\")\n    plt.xticks([])\n    plt.yticks([])"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#insights-and-summary",
    "href": "posts/2023-04-24-cp.html#insights-and-summary",
    "title": "Conformal Prediction",
    "section": "Insights and Summary:",
    "text": "Insights and Summary:\n\nGiven an image \\(x\\) and label \\(j\\). Softmax measures \\(P(Y = j | X = x)\\). However, we have no guarantee that the softmax outputs are any good; they maybe arbitrarily overfit or otherwise untrustworthy. Thus, we use the holdout set to adjust for their deficiencies.\nIn the above example the holdout set contained 5000 examples that the model never saw during training which gives us an honest appraisal of its performance.\nHere, the conformal score was 1 - softmax output of the true class. Then we took $ = 1 - $ quantile of the scores.\nUsing Step 3 at the test time, we got the softmax outputs of a new image \\(X_{test}\\) and collected all classes with outputs above $ 1 − $ into a prediction set \\(C(X_{test})\\)\n\n\nImplementation using Imagenet\n\nif not os.path.exists(\"../data\"):\n    os.system(\"gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz\")\n    os.system(\"tar -xf ../data.tar.gz -C ../\")\n    os.system(\"rm ../data.tar.gz\")\nif not os.path.exists(\"../data/imagenet/human_readable_labels.json\"):\n    !wget -nv -O ../data/imagenet/human_readable_labels.json -L https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\ndata = np.load(\"../data/imagenet/imagenet-resnet152.npz\")\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nsmx = data[\"smx\"]\nlabels = data[\"labels\"].astype(int)\n\n\n# Problem setup\nn = 1000  # number of calibration points\nalpha = 0.1  # 1-alpha is the desired coverage\n\n\nidx = np.array([1] * n + [0] * (smx.shape[0] - n)) &gt; 0\nnp.random.shuffle(idx)\ncal_smx, val_smx = smx[idx, :], smx[~idx, :]\ncal_labels, val_labels = labels[idx], labels[~idx]\n\n\nimagenet_calib_df = pd.DataFrame(cal_smx)\nimagenet_calib_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n\n\n\n\n0\n9.646587e-01\n1.350361e-05\n2.151330e-07\n1.699551e-06\n2.384544e-06\n1.646308e-06\n1.394906e-07\n2.117511e-08\n3.057390e-09\n4.086660e-10\n...\n5.279725e-09\n9.462966e-08\n1.185813e-08\n5.307772e-10\n2.161666e-07\n1.007043e-08\n9.514928e-08\n8.144019e-07\n1.339111e-07\n6.878381e-09\n\n\n1\n9.992527e-01\n1.005275e-06\n5.030975e-08\n2.312540e-08\n6.919812e-07\n5.068674e-08\n5.945228e-08\n2.580266e-09\n1.059923e-09\n5.929557e-11\n...\n3.178681e-10\n3.120479e-09\n2.160190e-09\n6.229624e-10\n3.004631e-08\n2.982520e-10\n3.827619e-08\n2.310420e-07\n9.114003e-08\n6.513726e-10\n\n\n2\n9.998410e-01\n2.081634e-08\n2.163244e-09\n1.033369e-08\n9.947884e-09\n4.689700e-09\n4.500399e-09\n4.603104e-11\n2.665861e-11\n4.032333e-12\n...\n1.170430e-10\n1.740400e-10\n1.001514e-10\n2.484425e-11\n6.860166e-10\n5.098253e-11\n9.393597e-10\n3.404014e-08\n1.460277e-09\n8.657306e-13\n\n\n3\n9.996231e-01\n6.980400e-06\n7.547856e-08\n1.445374e-07\n5.570853e-07\n1.413495e-06\n1.172659e-07\n4.219434e-09\n9.644072e-10\n4.150972e-11\n...\n1.467458e-09\n1.727905e-08\n4.188708e-08\n8.764998e-10\n3.017675e-08\n1.152834e-09\n2.212167e-08\n5.312061e-07\n7.742039e-09\n6.035842e-10\n\n\n4\n3.740840e-07\n9.997242e-01\n6.791072e-10\n4.707819e-09\n3.942747e-09\n3.235905e-07\n1.922253e-08\n7.563041e-09\n4.848560e-09\n1.836324e-11\n...\n1.428742e-09\n2.168828e-09\n7.591582e-10\n7.432400e-11\n8.145293e-10\n6.436701e-10\n6.601004e-10\n2.608228e-10\n1.372821e-09\n1.686885e-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n1.338609e-13\n2.481204e-13\n2.981873e-12\n2.992094e-12\n8.025680e-13\n1.196738e-12\n1.058158e-12\n1.031028e-14\n8.366420e-14\n1.834110e-16\n...\n2.835526e-10\n1.185883e-09\n5.707088e-11\n1.053065e-10\n6.115803e-09\n1.000000e+00\n6.104551e-11\n3.662891e-10\n2.519912e-13\n1.617798e-11\n\n\n996\n1.461727e-05\n2.962031e-05\n2.201413e-07\n3.813796e-08\n1.196295e-07\n3.246882e-07\n2.723306e-06\n2.824044e-06\n9.639041e-06\n2.289236e-05\n...\n2.566843e-02\n1.156482e-01\n1.163806e-03\n6.741311e-03\n1.174134e-03\n1.015575e-03\n4.271199e-01\n3.027194e-01\n3.415058e-04\n8.422194e-07\n\n\n997\n3.484188e-06\n1.047919e-07\n7.475886e-08\n3.465406e-07\n1.347377e-06\n4.767327e-06\n4.182576e-08\n4.709841e-08\n1.508235e-08\n1.065061e-08\n...\n2.632773e-06\n2.174731e-05\n3.774206e-04\n1.449335e-04\n8.616778e-01\n8.140442e-05\n1.180090e-04\n1.335993e-01\n7.427732e-06\n3.561391e-08\n\n\n998\n7.082336e-04\n2.196637e-05\n1.516250e-05\n7.714512e-04\n1.190577e-01\n5.289520e-02\n3.330159e-04\n1.690781e-07\n1.402206e-06\n4.881958e-08\n...\n1.218608e-05\n2.880947e-03\n5.116140e-04\n1.090989e-01\n8.638866e-03\n3.532250e-02\n1.301925e-02\n5.380661e-01\n1.777594e-05\n4.036425e-07\n\n\n999\n4.129702e-14\n2.889617e-12\n2.798768e-13\n4.931771e-13\n2.598153e-12\n9.916586e-14\n3.006582e-13\n8.608723e-12\n2.060572e-12\n5.938183e-13\n...\n1.010096e-11\n1.221625e-11\n2.120370e-11\n2.549839e-13\n5.645862e-10\n5.007978e-11\n3.019627e-10\n1.544346e-11\n1.280130e-11\n9.915479e-01\n\n\n\n\n1000 rows × 1000 columns"
  },
  {
    "objectID": "posts/2023-04-24-cp.html#adaptive-prediction-sets",
    "href": "posts/2023-04-24-cp.html#adaptive-prediction-sets",
    "title": "Conformal Prediction",
    "section": "Adaptive Prediction Sets",
    "text": "Adaptive Prediction Sets\nIn comparison to the earlier method:\n\nThis method will have a larger predictive set size.\nMuch more adaptive (\\(i.e.\\) Larger set size for hard examples and small set size for easy examples).\nThe earlier method used the softmax value corresponding to only the true class of the output.\n\n\nImplementation\n\nif not os.path.exists(\"../data\"):\n    os.system(\"gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz\")\n    os.system(\"tar -xf ../data.tar.gz -C ../\")\n    os.system(\"rm ../data.tar.gz\")\nif not os.path.exists(\"../data/imagenet/human_readable_labels.json\"):\n    !wget -nv -O ../data/imagenet/human_readable_labels.json -L https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\ndata = np.load(\"../data/imagenet/imagenet-resnet152.npz\")\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nsmx = data[\"smx\"]\nlabels = data[\"labels\"].astype(int)\n\n\n# Problem setup\nn = 1000  # number of calibration points\nalpha = 0.1  # 1-alpha is the desired coverage\n\n\nidx = np.array([1] * n + [0] * (smx.shape[0] - n)) &gt; 0\nnp.random.shuffle(idx)\ncal_smx, val_smx = smx[idx, :], smx[~idx, :]\ncal_labels, val_labels = labels[idx], labels[~idx]\n\n\ncal_pi = cal_smx.argsort(1)[\n    :, ::-1\n]  ## sorting the cal_smx in descending order and storing the indices in cal_pi\ncal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(\n    axis=1\n)  ##  take the elements of 'cal_smx' corresponding to the indices in 'cal_pi' and take the cumulative sum along each row\ncal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n    range(n), cal_labels\n]  ##  take the elements of 'cal_srt' corresponding to the indices of the sorted 'cal_pi' and select the score corresponding to the 'cal_labels'\n\n\nexample = np.array(\n    [\n        [3, 4, 2, 1, 6],\n        [5, 4, 6, 7, 3],\n        [9, 5, 4, 3, 7],\n        [5, 4, 3, 7, 8],\n        [0, 3, 2, 1, 6],\n    ]\n)\nexample_labels = np.array([2, 3, 1, 4, 2])\n\nexample_pi = example.argsort(1)[:, ::-1]\nprint(example_pi)\n\nexample_srt = np.take_along_axis(example, example_pi, axis=1).cumsum(axis=1)\nprint(example_srt)\n\nexample_scores = np.take_along_axis(example_srt, example_pi.argsort(axis=1), axis=1)\nprint(example_scores)\n\nexample_scores[range(5), example_labels]\n\n[[4 1 0 2 3]\n [3 2 0 1 4]\n [0 4 1 2 3]\n [4 3 0 1 2]\n [4 1 2 3 0]]\n[[ 6 10 13 15 16]\n [ 7 13 18 22 25]\n [ 9 16 21 25 28]\n [ 8 15 20 24 27]\n [ 6  9 11 12 12]]\n[[13 10 15 16  6]\n [18 22 13  7 25]\n [ 9 21 25 28 16]\n [20 24 27 15  8]\n [12  9 11 12  6]]\n\n\narray([15,  7, 21,  8, 11])\n\n\n\n# example = np.array([[3,4,2,1,6],[5,4,6,7,3],[9,5,4,3,7],[5,4,3,7,8],[0,3,2,1,6]])\n# example_pi = example.sort()\n# print(example_pi)\n\n1. The softmax output corresponding to an image is sorted in decreasing order. Then we consider the \\(E_i\\) as the total mass of the softmax function for a particular label until we reach the true label.\n\n# Get the score quantile\nqhat = np.quantile(cal_scores, np.ceil((n + 1) * (1 - alpha)) / n)\nqhat\n\n0.9998794758319854\n\n\n2. Calculating the quantile value \\(\\hat{q}\\)\n\n# Deploy (output=list of length n, each element is tensor of classes)\nval_pi = val_smx.argsort(1)[:, ::-1]\nval_srt = np.take_along_axis(val_smx, val_pi, axis=1).cumsum(axis=1)\nprediction_sets = np.take_along_axis(val_srt &lt;= qhat, val_pi.argsort(axis=1), axis=1)\n\n\neg = np.array([7, 5, 4, 3, 2, 1, 4, 3, 6, 7])\neg_pi = eg.argsort()[::-1]\neg_srt = np.take_along_axis(eg, eg_pi, axis=0).cumsum()\np_set = np.take_along_axis(eg_srt &lt;= 0.8, eg_pi.argsort(), axis=0)\n\n\nimg = smx[1][0:10]\nimg_pi = img.argsort()[::-1]\nimg_srt = np.take_along_axis(img, img_pi, axis=0).cumsum()\nprediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\nprint(img_pi, img, img_srt, prediction_set)\n\n[0 1 4 3 5 2 6 7 8 9] [9.64658678e-01 1.35036071e-05 2.15132957e-07 1.69955081e-06\n 2.38454436e-06 1.64630808e-06 1.39490609e-07 2.11751061e-08\n 3.05739034e-09 4.08666018e-10] [0.96465868 0.96467218 0.96467457 0.96467627 0.96467791 0.96467813\n 0.96467827 0.96467829 0.96467829 0.96467829] [ True  True  True  True  True  True  True  True  True  True]\n\n\n\nwith open(\"../data/imagenet/human_readable_labels.json\") as f:\n    label_strings = np.array(json.load(f))\n\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nfor i in range(10):\n    rand_path = np.random.choice(example_paths)\n    img = imread(\"../data/imagenet/examples/\" + rand_path)\n    img_index = int(rand_path.split(\".\")[0])\n    img_pi = smx[img_index].argsort()[::-1]\n    img_srt = np.take_along_axis(smx[img_index], img_pi, axis=0).cumsum()\n    prediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\n    plt.figure()\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.show()\n    print(f\"The prediction set is: {list(label_strings[prediction_set])}\")\n\n\n\n\nThe prediction set is: ['prairie grouse', 'partridge', 'Afghan Hound', 'Otterhound', 'Bedlington Terrier', 'Kerry Blue Terrier', 'Giant Schnauzer', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'Irish Setter', 'Gordon Setter', 'Brittany', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Australian Kelpie', 'Komondor', 'Newfoundland', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'hyena', 'leopard', 'cheetah', 'brown bear', 'American black bear', 'mongoose', 'wild boar', 'bison', 'ram', 'llama', 'weasel', 'mink', 'guenon', 'baboon', 'honeycomb', 'jeep', 'wig', 'acorn', 'gyromitra']\nThe prediction set is: ['Otterhound', 'Border Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Miniature Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Australian Silky Terrier', 'German Shepherd Dog']\nThe prediction set is: ['goldfish', 'tiger shark', 'cock', 'house finch', 'agama', 'triceratops', 'ring-necked snake', 'sea snake', 'southern black widow', 'centipede', 'black grouse', 'prairie grouse', 'grey parrot', 'macaw', 'lorikeet', 'hornbill', 'hummingbird', 'toucan', 'flatworm', 'nematode', 'sea slug', 'fiddler crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'crane (bird)', 'oystercatcher', 'Bloodhound', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Labrador Retriever', 'English Setter', 'Gordon Setter', 'Brittany', 'Cocker Spaniels', 'Sussex Spaniel', 'Rottweiler', 'Greater Swiss Mountain Dog', 'Dalmatian', 'cougar', 'tiger', 'polar bear', 'mongoose', 'ladybug', 'stick insect', 'cockroach', 'leafhopper', 'damselfly', 'red admiral', 'gossamer-winged butterfly', 'sea cucumber', 'hamster', 'beaver', 'guinea pig', 'pig', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'skunk', 'badger', 'macaque', 'marmoset', 'red panda', 'eel', 'coho salmon', 'rock beauty', 'clownfish', 'sturgeon', 'garfish', 'lionfish', 'aircraft carrier', 'airliner', 'airship', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'waste container', 'assault rifle', 'backpack', 'balance beam', 'balloon', 'Band-Aid', 'baluster', 'barber chair', 'barbershop', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'swimming cap', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'beer bottle', 'binoculars', 'birdhouse', 'boathouse', 'bolo tie', 'bottle cap', 'breakwater', 'broom', 'buckle', 'bulletproof vest', 'high-speed train', 'taxicab', 'cannon', 'canoe', 'can opener', 'car mirror', 'carousel', 'tool kit', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'catamaran', 'CD player', 'mobile phone', 'chain', 'chain-link fence', 'chainsaw', 'movie theater', 'coffeemaker', 'computer keyboard', 'container ship', 'convertible', 'cowboy hat', 'crane (machine)', 'crash helmet', 'crate', 'crutch', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'digital clock', 'digital watch', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'drilling rig', 'dumbbell', 'Dutch oven', 'electric fan', 'electric locomotive', 'envelope', 'feather boa', 'fireboat', 'fire engine', 'football helmet', 'forklift', 'freight car', 'frying pan', 'garbage truck', 'gas mask', 'gas pump', 'go-kart', 'golf ball', 'golf cart', 'grille', 'grocery store', 'guillotine', 'barrette', 'half-track', 'hammer', 'hand-held computer', 'hard disk drive', 'harvester', 'hatchet', 'holster', 'honeycomb', 'hook', 'horizontal bar', 'horse-drawn vehicle', \"jack-o'-lantern\", 'jeep', 'jigsaw puzzle', 'pulled rickshaw', 'joystick', 'knot', 'lab coat', 'ladle', 'laptop computer', 'lawn mower', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'speaker', 'sawmill', 'magnetic compass', 'mailbox', 'manhole cover', 'match', 'maze', 'medicine chest', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'minivan', 'missile', 'mobile home', 'Model T', 'modem', 'monitor', 'moped', 'mortar', 'scooter', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'neck brace', 'odometer', 'oil filter', 'oscilloscope', 'bullock cart', 'oxygen mask', 'packet', 'paddle wheel', 'padlock', 'paintbrush', 'parachute', 'parking meter', 'passenger car', 'payphone', 'pencil case', 'pencil sharpener', 'Petri dish', 'photocopier', 'picket fence', 'pickup truck', 'pier', 'pill bottle', 'ping-pong ball', 'hand plane', 'plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'soda bottle', 'power drill', 'printer', 'projectile', 'projector', 'hockey puck', 'punching bag', 'race car', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rotisserie', 'eraser', 'ruler', 'running shoe', 'safe', 'sandal', 'weighing scale', 'school bus', 'scoreboard', 'CRT screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shopping cart', 'shovel', 'ski', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'solar thermal collector', 'space bar', 'space heater', 'space shuttle', 'motorboat', 'sports car', 'spotlight', 'steam locomotive', 'through arch bridge', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'suspension bridge', 'mop', 'swing', 'switch', 'syringe', 'tank', 'tape player', 'television', 'threshing machine', 'tile roof', 'toaster', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'tray', 'trimaran', 'trolleybus', 'tub', 'turnstile', 'typewriter keyboard', 'viaduct', 'wall clock', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water tower', 'whistle', 'wing', 'shipwreck', 'yurt', 'website', 'traffic sign', 'traffic light', 'dust jacket', 'ice pop', 'hot dog', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'hay', 'meatloaf', 'burrito', 'alp', 'bubble', 'cliff', 'coral reef', 'volcano', 'baseball player', 'scuba diver', 'rapeseed', 'corn', 'coral fungus', 'agaric', 'stinkhorn mushroom', 'ear']\nThe prediction set is: ['alligator lizard', 'trilobite', 'scorpion', 'tick', 'centipede', 'conch', 'snail', 'chiton', 'crayfish', 'hermit crab', 'isopod', 'ground beetle', 'weevil', 'cockroach', 'cicada', 'sea cucumber', 'armadillo', 'corn']\nThe prediction set is: ['tench', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'cock', 'hen', 'house finch', 'junco', 'indigo bunting', 'vulture', 'spotted salamander', 'loggerhead sea turtle', 'leatherback sea turtle', 'green iguana', 'desert grassland whiptail lizard', 'frilled-necked lizard', 'Gila monster', 'European green lizard', 'triceratops', 'eastern hog-nosed snake', 'kingsnake', 'vine snake', 'night snake', 'boa constrictor', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'tick', 'centipede', 'black grouse', 'ruffed grouse', 'prairie grouse', 'partridge', 'grey parrot', 'macaw', 'sulphur-crested cockatoo', 'coucal', 'hornbill', 'toucan', 'tusker', 'echidna', 'wombat', 'jellyfish', 'sea anemone', 'flatworm', 'conch', 'slug', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'red king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'American coot', 'dunlin', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Italian Greyhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'Gordon Setter', 'Brittany', 'Clumber Spaniel', 'English Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Bouvier des Flandres', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland', 'Pyrenean Mountain Dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Griffon Bruxellois', 'Pembroke Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog', 'Alaskan tundra wolf', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'leopard', 'American black bear', 'ground beetle', 'rhinoceros beetle', 'grasshopper', 'cricket', 'stick insect', 'cockroach', 'mantis', 'ringlet', 'monarch butterfly', 'starfish', 'sea urchin', 'cottontail rabbit', 'Angora rabbit', 'hamster', 'porcupine', 'beaver', 'common sorrel', 'zebra', 'hippopotamus', 'ox', 'water buffalo', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'armadillo', 'gibbon', 'Asian elephant', 'African bush elephant', 'snoek', 'eel', 'coho salmon', 'rock beauty', 'sturgeon', 'garfish', 'lionfish', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airship', 'altar', 'ambulance', 'analog clock', 'apron', 'waste container', 'assault rifle', 'backpack', 'bakery', 'balloon', 'Band-Aid', 'banjo', 'baluster', 'barbell', 'barber chair', 'barbershop', 'barometer', 'barrel', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military cap', 'beer bottle', 'beer glass', 'bell-cot', 'bib', 'tandem bicycle', 'bikini', 'ring binder', 'binoculars', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bow', 'bow tie', 'brass', 'bra', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'can opener', 'cardigan', 'car mirror', 'carousel', 'tool kit', 'carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'CD player', 'cello', 'mobile phone', 'chain', 'chain mail', 'chainsaw', 'chest', 'chiffonier', 'chime', 'china cabinet', 'Christmas stocking', 'church', 'movie theater', 'cleaver', 'cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'coil', 'combination lock', 'computer keyboard', 'confectionery store', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'crane (machine)', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fireboat', 'fire screen sheet', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'gas mask', 'gas pump', 'goblet', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'grille', 'grocery store', 'guillotine', 'barrette', 'hair spray', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'harmonica', 'harp', 'hatchet', 'holster', 'home theater', 'hook', 'hoop skirt', 'horizontal bar', 'hourglass', 'iPod', 'clothes iron', \"jack-o'-lantern\", 'jeans', 'T-shirt', 'pulled rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lens cap', 'library', 'limousine', 'ocean liner', 'lipstick', 'slip-on shoe', 'lotion', 'speaker', 'sawmill', 'magnetic compass', 'mail bag', 'mailbox', 'tights', 'tank suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'maze', 'medicine chest', 'megalith', 'microphone', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mobile home', 'Model T', 'modem', 'monastery', 'monitor', 'square academic cap', 'mosque', 'mosquito net', 'scooter', 'mountain bike', 'tent', 'mousetrap', 'moving van', 'muzzle', 'nail', 'neck brace', 'necklace', 'nipple', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'overskirt', 'oxygen mask', 'packet', 'paddle', 'padlock', 'paintbrush', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'passenger car', 'patio', 'payphone', 'pedestal', 'pencil case', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'picket fence', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pirate ship', 'pitcher', 'planetarium', 'plastic bag', 'plate rack', 'plunger', 'Polaroid camera', 'pole', 'poncho', 'billiard table', 'soda bottle', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'projector', 'punching bag', 'purse', 'quill', 'quilt', 'race car', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'rugby ball', 'ruler', 'running shoe', 'safe', 'safety pin', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'schooner', 'CRT screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe store', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski mask', 'sleeping bag', 'sliding door', 'slot machine', 'snorkel', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'space bar', 'space heater', 'space shuttle', 'spatula', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'through arch bridge', 'steel drum', 'stethoscope', 'scarf', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swimsuit', 'swing', 'switch', 'syringe', 'table lamp', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'front curtain', 'thimble', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'toy store', 'semi-trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'trombone', 'tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vase', 'vault', 'velvet', 'vending machine', 'vestment', 'violin', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'wing', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'yawl', 'yurt', 'website', 'comic book', 'crossword', 'traffic sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'consomme', 'hot pot', 'ice pop', 'pretzel', 'hot dog', 'cabbage', 'zucchini', 'butternut squash', 'cucumber', 'mushroom', 'orange', 'pineapple', 'banana', 'jackfruit', 'custard apple', 'chocolate syrup', 'dough', 'pizza', 'pot pie', 'burrito', 'red wine', 'espresso', 'cup', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'shoal', 'seashore', 'valley', 'volcano', 'baseball player', 'bridegroom', 'scuba diver', \"yellow lady's slipper\", 'corn', 'coral fungus', 'agaric', 'gyromitra', 'earth star', 'hen-of-the-woods', 'bolete', 'toilet paper']\nThe prediction set is: []\nThe prediction set is: ['tennis ball']\nThe prediction set is: []\nThe prediction set is: ['jay', 'tick', 'grey parrot', 'macaw', 'slug', 'common gallinule', 'Pekingese', 'Shih Tzu', 'Papillon', 'West Highland White Terrier', 'Shetland Sheepdog', 'collie', 'rhinoceros beetle', 'dragonfly', 'damselfly', 'sea urchin', 'accordion', 'analog clock', 'backpack', 'ballpoint pen', 'Band-Aid', 'bassoon', 'beaker', 'bib', 'ring binder', 'bolo tie', 'bookcase', 'bookstore', 'bow', 'bow tie', 'broom', 'bucket', 'buckle', 'cauldron', 'candle', 'can opener', 'tool kit', 'carton', 'cassette', 'cassette player', 'CD player', 'mobile phone', 'chain', 'chime', 'Christmas stocking', 'cloak', 'coffee mug', 'coil', 'computer keyboard', 'croquet ball', 'crutch', 'desk', 'digital clock', 'digital watch', 'dishcloth', 'drum', 'drumstick', 'electric guitar', 'envelope', 'face powder', 'feather boa', 'filing cabinet', 'flute', 'fountain pen', 'grand piano', 'barrette', 'hair spray', 'hammer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'hook', 'iPod', 'jeans', 'jigsaw puzzle', 'knot', 'lab coat', 'ladle', 'laptop computer', 'lens cap', 'paper knife', 'library', 'lighter', 'lipstick', 'lotion', 'loupe', 'mail bag', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'medicine chest', 'microphone', 'mitten', 'modem', 'square academic cap', 'computer mouse', 'nail', 'necklace', 'nipple', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'packet', 'paintbrush', 'pan flute', 'paper towel', 'pencil case', 'pencil sharpener', 'perfume', 'plectrum', 'pinwheel', 'plunger', 'pole', 'pot', 'printer', 'purse', 'quill', 'racket', 'radio', 'reel', 'remote control', 'revolver', 'rifle', 'eraser', 'ruler', 'safety pin', 'scabbard', 'screw', 'screwdriver', 'shovel', 'ski', 'slide rule', 'snorkel', 'sock', 'sombrero', 'soup bowl', 'spatula', 'spindle', 'steel drum', 'stethoscope', 'scarf', 'strainer', 'sunglass', 'sunglasses', 'sunscreen', 'mop', 'switch', 'syringe', 'tape player', 'tennis ball', 'thimble', 'torch', 'tray', 'tripod', 'umbrella', 'velvet', 'violin', 'wall clock', 'wallet', 'water bottle', 'whistle', 'wig', 'wooden spoon', 'wool', 'comic book', 'crossword', 'dust jacket', 'ice pop', 'toilet paper']\nThe prediction set is: ['grey parrot', 'macaw', 'lorikeet']"
  }
]